["i really like a lot of f#, but it is really difficult for those of us with limited.net experience (python, r...etc experience here). there are several f# books, but it's all a bit above me. if you are an f# evangelist, please consider spreading that idea around. maybe an updated tryfsharp.org would help. not just repl snippets either but how to put things together.i appreciate everything the open source community does btw. it is just a little frustrating that there is a really cool language and ecosystem that i'm only making glacial progress in.", "fascinating. i always thought python 2.7 support would end july 3, 2020 (exactly 5+5 years from when 2.7 was released), but for some reason they recently decided to go with jan 1st.> specifically, 2.7 will receive bugfix support until january 1, 2020. all 2.7 development work will cease in 2020.> i've updated the pep to say 2.7 is completely dead on jan 1 2020. the final release may not literally be on january 1st, but we certainly don't want to support 2.7 through all of 2020.", "its useful to have phd because of the research methodology discipline and the classes you have to take to learn statistics for the final thesis. having a phd in computer science almost means you do know how to code with one language, which is definitely helpful when you are building your model in r or python. also the fact that you have a broader knowledge in the domain (ideally), gives you the credential to create a defensible model.", "fogleman also have awesome minecraft-clone[0] in python/pyglet in just 900 lines of code.[0]", "there are actually four possible binary operations, all of which agree when both arguments are positive (e.g. f(13,5)=3 for all of them), and all of which satisfy the property that if f(n,d) = r, then r \u2261 n (mod d). namely:\u2022 the first one always returns a positive number: for this f, when f(n,d) = r, it's true that 0 \u2264 r < |d|. for example, f(13,-5)=3, and f(-13,-5)=2. some programming languages that obey this: algol, dart, maple, pascal, z3.\u2022 the second one always takes the sign of the divisor (i.e. the denominator, if you write the division as a fraction n/d): for this f, when f(n,d) = r, it's true that |r| < |d|, and that r and d both have the same sign. for example, f(13,-5)=-2, and f(-13,-5)=-3. some programming languages that follow this: apl, cobol, j, lua, mathematica, ms excel, perl, python, r, ruby, tcl.\u2022 the third one always takes the sign of the dividend (i.e. the numerator, if you write the division as a fraction n/d): for this f, when f(n, d) = r, it's true that |r| < |d|, and that r and n both have the same sign. for example f(13,-5)=3, and f(-13,-5)=-3. some programming languages that follow this: awk, bash, bc, c99, c++11, c#, d, eiffel, erlang, go, java, ocaml, php, rust, scala, swift, vb, x86 assembly.\u2022 the fourth one takes the sign of the product, i.e. it's positive if both the dividend and divisor have the same sign, and negative otherwise: for this f, when f(n, d) = r, it's true that |r| < |d|, and that r and n*d have the same sign. for example, f(13,-5)=-2, and f(-13,-5)=2. i'm not aware of a programming language that does this, though i've seen some ad-hoc hand-rolled big-integer libraries that used to do this.of course in mathematics (number theory) we usually don't care, as, for example, modulo 5 (or -5 for that matter), we would say -13 \u2261 -8 \u2261 -3 \u2261 2 \u2261 7 \u2261 12... (mod 5), and so we can pick any of them and it doesn't affect the mathematics.", "as much as i love javascript, i have always preference synchronous languages (like, say, python) for robotics programming.", "what are you finding the advantages of using javascript over another language like say python for those iot applications?", "you are absolutely right - i was misled by the page i referred to using \"n\" as the number being factored, and not the size of the problem, which is log_2(n).so to correct myself, the difficulty factor is 1400 to 1500, and here's the modified python program, using the simplest change i could:code_removed that's not the fastest or best way to compute this, but it's the smallest and cleanest change i could get away with that reflects how i think about this, which is number of bits. a reasonable alternative is in the parent comment to this one.", "i know little about the school, but i would be hard to convince they would do worse.i think just going to a school where there is a sense of positivity and opportunity is in itself useful, but without resources, it may not be. this school, as far as i know, will have greater resources.the staff, assuming they are being paid well and given access to more resources, will absolutely do a better job.i am really not a fan of the school, and fear it\u2019s largely a tax write-off and publicity stunt, but knowing how bad a lot of these marginalized populations have it in public school, i just can\u2019t imagine it being worse.i would argue dignified is necessary for effectiveness. are you suggesting that educational environments that are undignified might motivate desperation for achievement? or some similar bootstrapping theory? if so, i find that immoral even if it were an evidenced theory.maybe you\u2019re making some sort of iq-population argument. if so, i would not accept that we can make that assumption. but, if so, i would nonetheless assert equal schooling resources for every child (which is largely what i mean by \u201cdignified\u201d) is a moral imperative.", "> give me visual basic 6's ui builder and all the greatness of a 21st century machine, so that i could finally whip up an ui without yelling profanities at the css reference [...] that clearly just wasn't built for thiscan i interest you in a vb6-y design/code/deploy tool with no css or js? it's all in python, even in-browser:", "i'm thinking of moving to japan myself. really enjoyed visiting it as a tourist. i learned the language for my side-project and planning to take a language proficiency test this december. anybody need a python programmer who worked at yandex?", "there is a work-in-progress second edition of \"real world ocaml\" book [1], which has many small but vital updates corresponding to the modern state of ocaml and its infrastructure. moreover, i recommend checking the lwt [2] from the beginning for working with multithreading. ocaml ctypes [3] library for creating ffi stubs without a single line of c and cstruct [4] for working with c structures. if you want to interact with some python code or script your ocaml program with python - you can use pyml [5] library. ocaml universe also has a good library for numbers crunching and scientific computing (including primitive machine learning) - owl [6]. for writing ocaml instead of javascript, it is possible to use bucklescript [7]. and if you are starting a new project in ocaml - i recommend using the most modern buildsystem - dune [8]. it is very simple to use and with a less overhead than others. i found also angrstrom [9] is very useful for implementing binary and text format parsers.utop [10] and ocaml-jupyter [11] are the best choice for experimenting, browsing the api and testing some small things.[1]", "i think the biggest chunk of people directly benefiting from oss are companies, for whom patreon might be an inefficient intermediary. personally, i'd prefer my employer funnel a few thousand dollars to the the fsf, python software foundation, etc. than setup a patreon donation for each project we use. (maybe some big ones like redis and nginx and backbonejs should get direct donations, but now many npm packages do we use? oh god, so many.)that said, if you maintain an oss tool, please do set up an easy way to donate. when it's easy to donate a dollar or two, i do (well...occasionally). i've found very few developers have patreon's setup. some will have a paypal link, which is especially nice for random online tools that i use once or twice. most don't provide an easy opportunity to contribute financially. though i guess i should keep more of an eye out for them.(while googling for more info, i did come across -- so that's neat.)", "> outside of it which still has it goodi don't know if i'd say it has it good. it just doesn't have it bad the way the rest of workforce does.this assumes it as a technical/business role, not just \"tech\" in general that includes software devs and hardware engineers. the python gurus and embedded systems analysts are doing okay, it's the cable monkeys and tier 1-2 support folks feeling the pinch.", "not only that, but home assistant is by far my favorite example of a well run \"crowd run\" open source project.there's no big company bankrolling it (well there kinda is since ubiquiti hired the creator, but not to the same extent something like chromium is just google), there are a huge number of committers, and the structure encourages people to maintain and improve their own contributions via 3rd party packages.there's no fighting, there's not much gatekeeping, it's not overly complicated, and they are extremely welcoming to new contributions, no matter how \"unique\" the use case is (look at some of the integrations! there's integrations for local bus schedule systems!). and on top of that, there's world-class documentation! that's rare enough in paid products, but to see it from a project like this, and the fact that it's almost always up to date is simply amazing.they held my hand through creating 2 new integrations, and i haven't developed in python for almost 10 years, and they were extremely helpful, responsive, and at the end of the day the product got better for everyone.i really don't have enough good things to say about home assistant.", "1: cleaner and more concise. perhaps safer, as the less code there is going to performing a task, the fewer places i can make a mistake. more importantly, it directs me to think about my problem from a higher level of abstraction.2: i do mix both paradigms, so i\u2019m not stuck without imperative features. that being said, there are times where i feel like my attempts to move a concept into fp forces me to make some less than ideal choices to fit it into the paradigm. if those are in inner loops, i forgo functional programming to do precisely what i need. (for example, managing exceptional cases or heterogeneous actions can be awkward.)3: i would have used them more. i\u2019ve always been a fan of comprehensions and map c/o python, but my code would have been more efficient and concise had i started earlier. the additions of cppitertools, range-v3, <algorithm> and <functional> make functional programming both easy and efficient in c++ in ways it was not prior to c++1[147]. i think python made fp accessible by putting it in an imperative language which is concise and readable.", "i'm in the same boat, coming from python. i like it a lot -- it lets me write things that are hard to express in python. (sum types and exhaustive pattern matching in particular.) my python had been moving in this direction anyway -- less oo, more procedural, more pure functions, limited mutation.", "> but coming from a background with c/javascript, the syntax is definitely hard to understand.i work with haskell fairly regularly, and have experience with a few other languages (python, racket, c/c++, java, etc.), but i definitely agree with this sentiment. i am not a fan of ocaml's syntax at all.i haven't had a chance to check out reason yet though, so maybe i'll spend some time playing with that soon!", "as an aside, as someone who primarily uses imperative languages, i\u2019ve found the proliferation of support for functional paradigms in python and c++ (fold expressions, reduce, transform, map, etc) has changed how i write code immensely.i\u2019m likely to try this out.", "ocaml is a very powerful language. i'm still discovering new features/patterns everyday. but coming from a background with c/javascript, the syntax is definitely hard to understand. for this particular problem, i think reasonml has done a very good job of introducing ocaml platform to a wider audience.shamelessly plug: i'm working sketch.sh [1] as a learning project for reasonml. it's a interactive playground (like ipython/jupyter) for ocaml and reasonml. it's provide inline types and evaluated values. give it a try if you're learning ocaml/reasonml.[1]:", "you label a few photos yourself and then check if the labels still correspond in the newer versions of the program.then when you edit any parts of the code and the test suite breaks - you can see that some of the invariants had been broken.this is even more important for python, as at least having a simple test suite can already tell whether your environment is sane. and that's about 90% of the newcomer's time saved.", "learned this the hard way when implementing a ring buffer in c++ after spending a lot of time doing things in python.", "the real game-changer would be to link downloads/checkouts to user accounts. like, you can download this repo anonymously once or twice a day; more than that, and you have to pay -- on a sliding scale, so that companies with build systems and so on are forced to pay more.if github and gitlab (and maybe bitbucket) had the balls to build something like that, i think things would improve. users would be impacted, so it would be essential to get some help on the pr side from established projects and bodies like fsf, python foundation, and so on. getting the big companies onboard should be easy enough - just give them a flat-fee yearly subscription option that they can spin in a nice bloomberg release on how they support innovation and blablabla.you would likely still see cheapskates setting up their internal mirrors and so on, but with the right price structure, i bet most medium-to-big shops would accept it and start generating substantial amounts for oss projects.", "guido van rossum has written about the background of this in python here:", "the key to this is in how you do integer division. here is what happens in python2:>>> 19 % -12-5>>> 19 / -12-2that is because python2 integer division is something called \"floored division\". this can be seen as always rounding toward negative infinity. this is arguably the correct way to do integer division and in fact there has been a lot of argument about this over the years.one very nice feature of floored division is that it does not have a weird discontinuity around 0 as \"round toward 0\" does. this discontinuity can sometimes cause problems in embedded applications involving physical motion.so mod and remainder are the same. you just have to do the division right...", "if you have significant amounts of python code to reuse in another language, i would recommend:a) run it as a subprocessb) call it over http request/responsec) use a message broker like amqp", "oh dear. that's very close. rsa2048 and rsa4096 make for very long outcomes in certs and dnssec and the like and i suspect hsm upgrades are coming too if we don't walk to the ecc stuff for shorter representations of the signings at equivalent complexity in solving (so i am told)[edit] thanks for simple python too!", "rsa-230 has 230 decimal digits, or 762 bits.[0]rsa-1024 has 309 decimal digits, or 1024 bits.[1]so the relative increase in size in bits is 1024/762, roughly a 34% increase, a factor of about 1.34.currently the complexity of the nfs is thought to be:exp( c (ln n)^(1/3) (ln ln n)^(2/3) ) [2]where c is about 2. that means that rsa-1024 (1024 bits) will take about 18% to 19% longer than rsa-230 (762 bits).[3][0] program:code_removed", "fluent pythoneffective pythondive into python", "re: pulling historical data with pandas-datareader, backtesting, algorithmic trading: historical returns- [the article uses a constant 7% annual return rate]- \"the current average annual return from 1923 (the year of the s&p\u2019s inception) through 2016 is 12.25%.\" (but that doesn't account for inflation)- (300%+ over n years (from a down market))is there a jupyter notebook with this code (with a requirements.txt for (repo2docker))?", "won't this calculation overflow to +infinity in javascript after a while? on the other hand, python will try to compute the actual result using arbitrary precision integers, which is that's slower.", "is there an up-to-date/maintained comparison of python-in-the-browser systems? as well as this, there's brython and skulpt, and it would be nice to be able to compare various implemented features, quirks and speed.", "the lebron james school is great for the kids who get to attend. i am sure of that.but, i find it interesting that so many overlook it\u2019s dependence on a celebrity savior and shortcomings of addressing the charter school contentions contributing to the disparities in so many thousands of disctricts nationwide.we need to repeatable dignified education systems.", "not academia, but in finance, i remember an email interaction that was very nearly company-wide at a large hedge fund. in both our production & development environments, all generated core files on our linux servers were configured to be dumped to a shared nfs mount (ostensibly to ease debugging). each environment had around a 1 or 2 tb mount. teams were expected to clean up their cores when they were no longer needed (useful for debugging those heisenbugs). nearly all of the code was c++. think 10s to 100s of millions of lines of c++ that had to work together spread across hundreds of services, countless libs that were used to interconnect.anyways, we were working on a fairly large company-wide effort. we were migrating from 32-bit to 64-bit, had 2 os upgrades (new version of rhel, migrating from windows xp to windows 7), 2 compiler upgrades (i forget the gcc versions - it came with the upgrade, and on windows vs2003 to vs2008). because of the coupling of the libs & services, both the linux & windows sides had to be released at the same time. we could update the windows boxes at anytime and run the old software on it, but we basically couldn't develop the old software on win7 as vs2003 wasn't compatible (it could be made to work, but the ide or compiler or linker may fail randomly and hard). this is just to explain the scope of the effort we were making.back to why i mentioned the core files. to anyone that's done it, it's obvious the magnitude of effort doing this all at once is. there will be bugs. specifically a metric-shit-ton of them. those core files were needed. they stopped being generated because our shared core space filled up. the culprit? a phd quant running some model in python using some c/c++ extensions kept crashing python, each one dropping a multi gb core file. this one quant was single-handedly using over half of the shared space. when confronted/inadvertently shamed in a development-wide email (we sent out usage breakdowns per user/project when a usage threshold was exceeded), his response was golden: \"what are these core files, and how do i stop them from being generated?\" uniform response from everyone else: \"fix your damn code!\" mind, at the time, this was in front of ~600 developers.the kicker: the reason this whole effort was being made? because someone sent the ceo an xlsx spreadsheet in 2008 (the exact year may have been later, but immaterial to the story), and still being on winxp & office 2003, he couldn't open it. so...down the rabbit hole we went.", "i guess \"one way to do the thing\" motto is at work here - run python as modules every time. i guess the script part should run the file it contains as a module for graceful fallback", "python is the last language that needs a vim plugin to do folding better - `foldmethod indent` exists in standard vim, and python enforces indentation by making it syntactically relevant.", "free python creative commons book works for both professionals and beginnerslink:", "learn python the hard way and automate the boring stuff with python were both very helpful for me to get started.", "ah yes. sum(range()) in python vs for(i=1...) in js was much closer, about 20% quicker in javascript.", "can you elaborate on that? is there a blog post somewhere that illustrates the problem you're talking about? i was under the assumption that python interpreters run single-threaded.", "you should give nim a try. it's a python-like language which can be compiled to c and javascript. there is even a spa framework[1] for it already and the nimforum[2] is written in it.1 - -", "thanks for the elaborate response, now i much better understand what you're trying to say.this is different from the examples you mention: there is no external software that creates the sandbox (whereas with java and flash you had to install those as plugins). it's really just javascript as it is built into the browser. any vulnerability in this \"sandbox\" is a vulnerability in the browser, not some 3rd party plugin.put differently, this pypy emulation is no different from the javascript api that was in your browser already. this isn't a new plugin, just using javascript to emulate python through the pypy interpreter.", "the talk about boids made me think this would go in a different direction.programmers and our employers exhibit flocking behavior. see how ruby on rails brought so many people to the ruby language. how python had a similar surge earlier in the 00s, and later with the explosion of use (numpy and similar) in data science applications. additionally, things like ruby on rails helped make the web application commonplace. see javascript and ajax for that as well.perl established an effective code repository (cpan) that enabled their userbase to grow via a similar flocking mechanism. go has no central repository but has an easy to use mechanism for adding dependencies based on git repositories. that ease of access to published modules allows people to adopt the technology once a peer (someone working in a similar domain) publishes something. and it encourages people to contribute back in some fashion (publish new modules or update the existing ones). in perl's case you saw a lot of people from outside it making use of it (the same is now true for python with data science) because the heavy lifting was done and they could focus on gluing things together.exploiting this flocking behavior, then, becomes important to anyone wanting to push a new technology out. even if it's the best thing and would produce the best software ever, it won't always succeed on its own merits.(random thoughts, no real thesis here)", "it's really awesome to see how far this has come. i believe we at repl.it were the first to try something like this in production. we emscriptined cpython to javascript and contributed quite a bit to the project in the process (including the initial virtual filesystem implementation). deployed this at scale at codecademy were millions of users were using it to learn python but hit a lot of problems with it. for one, if you're in a 3rd world country then the bundle size is a non-starter (this is potentially solved with wasm being a binary format). even if you managed to download the bundle lots of old computers would run out of memory trying to parse the js (again probably solved by the binary ast format of wasm). because of this and a few other issues (the dev experience of emscripten was really hard) we had to move away to running code on remote containers.now that i'm back working on repl.it full time i'm excited to play again with the tech and see what we can do with it this time around.", "i'm sure it is a thousand times slower than i++, but it should be the same code on each platform.but you made me think: indeed, the underlying call could be slower from js whereas from pypy/python{2,3} it's fast (or vice versa). so i just tested a version where it checks how long ten million iterations take (n=3). code here: since there is no syscall to be made, pypy can do this in compiled code and is blazingly fast (i bet c/assembly isn't much faster there), and the rest seems comparable to the previous estimate: the browser version is 2.25\u00d7 slower compared to python3 (now 2.43\u00d7) and 3.33\u00d7 slower compared to python2 (now 3.70\u00d7).", "int in python2 is a more simple structure (fixed precision for non-long integers) than int of python3 (arbitrary precision)", "python 3.6.6 from debian buster in a virtualbox vm on a windows 10 host. testing with 3.7.0, the result is no different. python 3.5 is no longer available in the repositories to test whether that is indeed slower.", "you would use sum(range()) in a python though. remember that many things you do manually in js have a better, automatic and faster way in the stlib.besides, if calculations are important, you would use numpy.", "my first instinct was to scream \"just use a cms\", but there are obvious advantages to this technique, so here are my thoughts to make it less hacky if someone ever intends to run that in production.- the most obvious one i don't see mentionned is that you should use a separate database than your main one for all the generated tables. pretty sure it would save on operational headaches.- it would probably be useful to generate all the python code that could be used for making migrations, if only for debugging purposes- understand early on which fields you want to use with which options. i wonder if foreignkeyfield would be possible. that would be a great feature, but probably bring headaches.- obviously, try to submit a pr to django for the migrations loader hack, maintaining monkey-patchs is troublesomeall in all, it's a pretty cool hack, but even with those modifications, i'd still be very wary to use that in production instead of battle-tested alternatives, unless you have a use-case that really warrants having full-blown sql schema for your dynamic data.edit: on second thought, i can think of a use case where using that in a cms would have saved me troubles (at least different tables for different models). when you have a production site receiving model a data (for example, your clients orders) and you want to edit model b data (for example, your site dynamic pages), you can't do that with eav all in a single table, but with using dynamic models, you could just copy the model b data, or even go with more complicated scripts.", "if the python module was large and complex, it does something that would be incredibly difficult to replicate in nodejs either given the current ecosystem or costs of a rewrite, is not available in rust/c/c++ that could be wasm'd (not mentioning others here due to large wasm overhead), you accept the large binary sizes of these things (i.e. you're not embedding the python interpreter/runtime and putting on npm), and you're willing to put a node-friendly layer on top that makes the lower level calls...then it might be sane. but i'd consider modernizing to an updated emscripten that does wasm, see if you can avoid some emscripten runtime (i.e. wasm standalone compilation).for all other cases, stay in node or use a lib from a language more suited to wasm compilation. now, if it's an option between this and a native node module, that depends on your opinion of node ffi and native node modules.", "other comments in this thread point to the implementation likely being python 2.x.", "really amazing. i use python every day at work, and most of the crucial libraries have tight inner loops written in c or cython. i wonder what the web assembly future holds for those languages, and inter-language ffi.", "which version of python 3 did you test with? i expected 3.6+ to be faster than python 2.7", "another way of describing this is:* django has facilities for mapping python classes to database tables* author uses metaprogramming to construct classes to match table definitions decided at runtime* author uses built-in schema migration tools to synchronize backendthere's nothing about this that wows me, although it does demonstrate knowledge of django internals and python metaprogramming. it doesn't impress me because this is the bread-and-butter of what django does.", "code_removed", "would it be sane to use this to run python modules in node.js?", "i believe this feature was introduced in python 3.6. this implementation may be 3.6 <edit: just checked the site and see it uses pypy which states -- on their site -- the python version is 3.5.3", "code_removed syntaxerror: invalid syntaxso this is a subset of python or something like that?", "regarding `avoid side-effects`, the amount of side-effects that are required for a typical real problem solving code base is quite reasonably measurable. for example, the number of network calls, db calls, filesystem read/writes are are measurable. have you been able to reduce the `side-effects` significantly or have just become more aware of it?when we started using expressjs (coming away from java), there was a tremendous change. as soon as you begin to code in javascript (say from java/ruby/python world), async hits you. there is no escaping it. and the programmer faces a tax (every time when implementing async/await or generators), his/her awareness about side-effects (deliberately generalizing io as side-effects) and they tend to be more careful with it.", "i still get surprises like starting to learn google cloud functions and realizing that up until july of this year they only supported python 2.i have no idea why would a project of this caliber would start by using python 2 instead of python 3.edit: when i started reading about gcf, all docs said i could only use python 2. later i found that they seem to be on the way to change this. but still, i was very surprised that python 2 was even an option to begin with.", "well why doesn't this apply to js? the python interpreter is running in the js sandbox", "tried fibonacci, and the thing crashed for 100. i'm not sure that a wasm python library can do any heavy lifting.", "i wanted to take julia for a spin and see how implementing some simple numerical computation would go. this script does not really use the most distinguishing features of julia 1.0, the same logic could be implemented in python for instance. but those are the first steps, later i will try to implement something more sophisticated. as to my findings about julia itself:- a syntacticly pleasant language! i like the multi-line expression with begin -> end (you can see a few examples in the code)- packaging is in a bit of a flux still, but it is expected- documentation is pretty good- linear-algebra operations as a first class citizen, random number generation as well. plenty of other features like that that make numerical code in julia somewhat more compact and idiomatic", "there is a similar python interpreter here: is used by a class taught on coursera (introduction to interactive programming in python)from what i can see:- pypy.js benefit: runs faster- codeskulptor benefit: can use gui components", "this is javascript, not real python. if you're so afraid that this is dangerous, why do you post malicious code? and if you're so clever to post that code, why don't you try similar code to see if it really has access to your files?code_removed the answer, by the way, is no.", "hey hn -founder of srsoterica here.this is not another srs platform.this is a comprehensive srs flashcard deck for learning the python programming language. the deck is built for use in anki.i'm here to answer any questions or challenges that you may have.thanks for your time and consideration!mike", "the speed is surprisingly reasonable. testing with a simple loop, i get about 1.3-1.4 million loops per second in python3, 2 million loops per second in python2, 14.5 million loops in pypy, and 0.61 million loops in the browser.my code basically calls int(time.time()) until its value changes, doing i+=1 for every time it did not change (n=10 for every platform (python2/3/pypy/browser)). my browser is firefox 61, full code here:", "it actually works pretty nicely. most of our apps are small and focused, and we nest apps within each other heavily.it's fairly common for example to see an app introduce some models, python apis to work with them, and business logic, and then to have a sub-app for the web api and bits relating to that, and a sub-app for the admin interfaces (we don't use the django admin).example:- orderscode_removed this means that working on our click and collect integration, one only needs to hold in their head the current app, and maybe some apis provided by the apps above it. it's relatively uncommon to import across apps on different branches of the tree, and when we do that's typically only to top-level apps containing the most core models, or for very explicit public apis provided to the whole site (which might be re-exported from an __init__.py up the tree anyway).i find it a pretty nice codebase to work on. we're hiring too", "there's no python standard, but my guess is that-- since this is based on the same source code that native pypy is, with a different compiler backend -- they are making the claim based on the fact that native pypy is widely considered to be similar enough to cpython for many workloads. thus -- assuming the correctness of the rpython -> javascript transformation (which they have not proven, and would be a monumental task) -- you should expect this version to work like the native pypy.", "* >>> import sys >>> sys.version '2.7.9 (?, jul 03 2015, 17:08:29) n[pypy 2.6.0]' *why, exactly, does this have such a garbage-ancient version of python?", "this is awesome. they mention that it is a \"compliant\" implementation of python. how does one go about proving this?", "i love python but js is close to an order of magnitude faster", "wasm plans on adding gc, polymorphic inline cache, direct dom access, etc. that would allow for python without having to download the whole runtime. whenever that happens, it might become more mainstream. similar for other languages.", "this is pretty neat! i was wondering, are there any projects to transpile python to js, but targeting the server side? it would be interesting to port a flask application, for example, to run on node and benefit from the jit, or maybe use express and call flask through its wsgi interface.", "shouldn't be too surprising, given how pypy is generally faster than cpython by about x7-8 factor: expect a much larger difference in tasks that are pure python (loading a csv is done primarily through the csv module, i.e. all the hot loops are in c).", "its front page describes it as an experiment, and it has 98 open bugs. both pypy and python have released new versions since the last update of this project, so it's at least missing important updates.", "great project. my team used it for a live-coding python notebook: my surprise it was faster than jupyter / cpython for some tasks (e.g. loading and parsing large csvs).", "as far as i know, permanent eol for python 2 is still jan 1, 2020 ( python 3 was first released december of 2008. that's an awfully long tail.", "i'd love there to be a client-side webapp development system using python with feature and output parity to javascript. if it was a product, i would buy it.", "correct. it's just a view to a server running python.", "such a snarky response, but i can't deny it. i see a js package that hasn't been updated in a year and i expect that it won't even build with my environment because webpack evolution. i see a python package that hasn't been updated in a year and it doesn't faze me. not a whole lot that can break in terms of building and integration with my python project.", "> what's happening to the python community?it's moved from \"growing\" to \"well established.\" i don't see many people lamenting something is in cpan and not pip anymore. it shouldn't be surprising that you find more stale projects and packages as time goes on. most of the interesting stuff is in the development with specific packages (pandas, salt, openstack, etc.).", "> this is pretty old, and hasn't seen an update in over a yearso pretty much like every python project then. what's happening to the python community?", "it's a pretty old side-project, based on an interpreter that itself took quite a while to support python 3 well due to lack of funding, so it's not that weird it hasn't been updated.", "i think jupyter works with a backend server to execute python (right?).", "i think we're roughly already there. i think the pivotal moment where python 2 could've died out rapidly passed and they kept officially supporting it for too long.", "", "yeah weird. i would expect python2 to be dying off more by now. wonder if we're headed to an environment where there's essentially 2 separate languages as python 3 continues to change and grow?", "will web assembly have any effect on a future web based python interpreter? is there one in the works?", "pity it's not python3.", "it's a snarl word for dynamically typed languages, to conflate strong, dynamic type systems like python's with true untyped languages like assembly, forth, and bcpl.", "what about search and cache?it's still python 2.7 only:(", "i think this (short) article had a lot of good points, though i do take minor issue with one bit:> some people learn haskell with the expectation that they will achieve some sort of programming enlightenment or nirvana. you will be disappointed if you bring these unrealistic expectations to the language.while i have not achieved any sort of nirvana, i do believe that learning haskell has \"enlightened\" me insofar as i now look at programming in a fundamentally different way. i put a much greater emphasis on writing many small functions, i use types wherever possible (and type hints too, since python is my primary language), and i avoid side-effects when i can. all of these things i do because haskell taught me just how powerful simplicity can be.i also learned about different ways to conceptualize the relationships between data, and i learned about laziness (something i'd never heard of before then). haskell has a lot to offer for people who come from an imperative background.> haskell is not an achievement to unlock or a trophy to be won because learning is a never-ending process and not a finish line.while you can never \"finish\" learning, you can set milestones to mark your achievement. comprehending some of haskell's features (both the basics and the advanced ones) are perfectly acceptable milestones by which to track progress. i don't think that's a bad thing.---that said, overall the article is very good. i agree that newcomers should write small programs, get their hands dirty, and learn to really use the basic features of the language firsthand before moving on to more advanced things. (and especially i agree that new users should avoid typeclasses at first, or even at second.)", "yes, but mid-pack among alternate layouts. qwerty doesn't even show up. the difference between them is minimal compared with sticking with qwerty.other factors to consider would be time-to-learn, hot-key placement, device compatibility and general flexibility. i put my own python code in your test and workman did much better. be sure to use several samples when making a determination.in most of my tests, colemak was a consistent winner by a small margin.", ">dynamic data structures in untyped languages are riskywhat exactly do you mean by \"untyped languages\"? python and every other high level language like it definitely has types. maybe you meant \"dynamically typed\"?", "this article de-emphasises an important piece of the puzzle: the -m switch to the python command. this runs a python file as a module rather than a script, complete with module-ish rules for imports, but with __name__==\"__main__\" so the script-ish code in it runs. the author says \"i am unaware of a clean solution to this problem [of a python file that could be run as a script or used as a module]\", and then mentions -m as a sub-bullet point without explanation. but this is the clean solution!here is a quick example of how it works. consider the following directory layout:code_removed you would expect foo.py to start with these lines:code_removed as the article explains, these will not work if you just run foo.py as a script, because sys.path will start with \"topdir/pkg/sub/\":code_removed the simple fix here is to run the python file as a module, with \"topdir/\" as your working directory, so sys.path starts with \"topdir/\":code_removed after briefly mentioning python -m, the article has quite a lot of text on how to modify sys.path instead. that variable is interesting for understanding what's going on, but you should certainly not be modifying it. the exception is that if you have multiple different root directories containing packages (multiple directories like \"topdir\"), in which case you should indirectly modify sys.path by setting the environment variable pythonpath. code should never touch sys.path because the location of packages on your computer is a property of that computer's environment, not of the code.", "that's about what the author says right at the top of the article.however, this is only more flexible in the front end. if you want to do anything with your data in the back end, a jsonfield will help you less than fully structured data. while you can do some queries on json fields, most of the richer queries will have to be done in python, slowing processing down.if you're looking for runtime-changeable, structured data that you can run complex queries on, modelmodels sound like a good alternative. most enterprise applications are in this area.", "very much this. this is the results for my personal bread and butter, python and sql. workman tested very much mid-pack.", "when i meant customization, i didn't mean the availability of plugins. i meant the ability to change everything, and do it myself simply and quickly if needed, from within the editor itself.there's only two editors i know that give you that, emacs and atom.that said, i didn't try neovim, but it just seems like vim with lua plugins. kinda similar to how sublime text can have python plugins.", "this is extremely clever! something like this needs a deep understanding of django and python internals. the bullet points are written like they put it together under 10 minutes or so:d i love it!", "yes, we have adopted go for a lot of things at work and i have reviewed code of many of our devs who are more or less go novices. it has indeed been pretty easy to understand - much easier than other languages (notably java in this regard, but python falls afoul of it a bit too) where people tend to write things in quite different styles and/or with excessive abstraction that made it much harder to understand what was going on.", "do we care how the linux compatibility is achieved? if the end result is the game is available on linux, stable and performant why should i give a damn that it uses wine internally any more than whether is uses unity or python or whatever?", "thanks for the mention -- we're big fans of ives and co!vs code has really done an incredible job and we're all benefiting from their work. one thing that we've done recently is add language servers on our containers. so when you boot up a language on repl.it, say python/java/c++, we'll start a language server and that gives you kick-ass code intelligence: autocomplete, click-to-symbol, and some refactoring features:", "because they give me a service for free? or just because we're all in this together?i'm sure it depends on the examples you pick. i'm totally happy if people use my mited code in a commerical product without reciprocating. i'm not writing it for them. i'm mostly writing it for myself. the payback is the joy i feel when others find it useful. i also feel joy by being part of the larger collection of people and companies that have given me so much free stuff. python, clang, v8, firefox, chromium, electron, zlib, libjpeg, libpng, git, sdl, react, etc etc...", "please take a look at in our team we have been using it for 3 years. it works great and serve as a very good example of a beautiful written, well tested angularjs single page app with python django powering the backend api. also look at event notification part and good quality responsive app, works great even on mobile phones. you can find a respiratory of designs where you can even see the svg source of design files for ui. only small subset of open source project have such level of work available publicly.", "in addition to what someone copied and pasted, some python scripts to make sure everything is excuted right. i do need to look into it more though:p", "as someone that hasn't been using python while the concept of \"virtual environments\" emerged i always found how many tools for managing them exist confusing. eventually i tried to just create a virtual environment by hand, which turns out to be pretty straightforward, and wrote a short article about it.", "off the top of my head, htdp, despite being entirely presented within drracket, has you work through programs that you could just as well compile. learn python the hard way, as well as every other \"hard way\" book, makes a point of having you write entire programs. the ocaml resource that was posted a week or two ago makes a point of deemphasizing the repl.", "along the same lines: (object shell) is a command-line utility for running sequences of operations connected by a pipe-like construct. objects are piped, not strings. included in the sequence of operations may be remote sequences of command, which run on all nodes of a cluster. for example, if you have configured a cluster named foo, then you can get the pid and commandline of every node of your cluster like this:code_removed - osh: invokes the interpreter- @foo [... ]: run the bracketed commands on every node of cluster foo.- ps: the osh ps command, which yields a stream of process objects.- ^: the osh pipe symbol- f: function application. the function in this case takes a process p, and returns a tuple containing the process's pid and command line.- $: print the contents of the input stream, one per line.this is a python tool, so the stuff piped between operations are python objects.", "imagine if microsoft focused on ironpython and added a high level of integration with powershell and/or python-extensible utilities. running/executing powershell from within a python repl would really be something.", "ditto, especially in combination with python/django, as used by nextdoor. ironically, they had already removed celery from their stack a few years prior.", "it's typical microsoft engineering, and i say that as a frequent defender of powershell here.it's encrypted, except when it isn't, except on tuesdays when it is. an fyi not all psremoting traffic is encrypted when using message encryption is done using the authenticated context which varies based on what auth was actually used.for example basic auth has no authenticated context in sspi so has no encryption.ntlm (fallback if kerb is not available) is next to useless as it is based on rc4 which is considered broken today.kerberos should be using aes if configured correctly but older environment may have older insecure algorithms set up if the config is mismanaged on the domain level.credssp uses the tls context that is set up in the auth so is similar to using using you not only get a more consistent and secure encryption mechanism on all auth methods. you also get encryption on the whole tcp packet that is sent to the client, whereas http message only encryption encrypts the message body.\"and\"my understanding of the issue is that winrm can do more than just powershell remoting (for example there are also python libraries that run on linux machines to interface with winrm which i'm assuming do something different to ps remoting since ps is not native/required on those platforms). so if you are doing only powershell remoting you're probably fine with but if you're doing anything else then it could be exposed via http winrm. so is still recommended.tl;dr - all powershell remoting is winrm, not all winrm traffic is ps remoting.\" think lots of people couldn't get a straight answer on this and it came up on the recent mcp insider ama.first 3 minutes of video 2 \"it's secure, use it [...] tell 'em jeffrey snover told you, go do it\".\"also, for more information on security when remoting read the powershell.org free e-book, secrets of powershell remoting.*\"", "gen1 standard used proprietary gae apis that locked you in.gen2 standard (php7, node8, java8, python3, etc) uses the standard gcp apis that you can call from anywhere, including other clouds. it should be relatively straightforward to move from gen2 standard to flex and vice versa.(i work for gcp)", "i use powershell on windows because cmd is atrocious and some things just aren't great in windows for python. c# is also not efficient for those people used to the efficiency that comes with a linux terminal and how to use it.when i'm on linux, i have tons of commands, bash, python, perl, awk, sed...etc etc. all of those tools work perfectly with text which the unix style os embraces. windows on the other hand is all objects and this can be painful in powershell at times.tldr; powershell is alright on windows as it is the only real viable option for a power user. however, linux is far better with what it already has and powershell would be a step backwards in functionality and efficiency.", "the rust library interfacing with python is this library understand things like python arrays, objects, etc. and provides nice rust interfaces to them. basically i just have to write a macro that specifies what functions i'm exposing to python, and other then that i'm writing normal rust. on the python side i'm importing them and calling them like any other python library.the build system is (which is linked at the bottom of the above readme).", "you should really consider either very detailed porting guides or compatibility libraries, at least from what i hear those required changes will keep some projects on first-gen otherwise (and thus python 2, and thus requests to dependencies to please keep python 2 supported and...)", "haskell is hard, mainly because it's normally not a first language, and the first language people learn probably has algol-like syntax (e.g. c, java). i think erlang is a bit easier to learn due to having fewer features, and for me was the gateway drug to haskell. it let me learn to program with recursion instead of loops, and pattern matching instead of `if`s. the type system is the best part of haskell, but unfortunately it makes it very easy to get stuck when starting out (\"io string\" vs \"string\", \"bytestring\" vs \"text\"). it's well worth the investment, though, to get a tool that lets you develop concise code like python/ruby but gives you strong guarantees of correctness. also worth noting the incredible ecosystem of libraries and tooling, like stack and intero:-", "random question, is it possible to have a python2 default service and a python3 service in the same app engine project?", "right, that only concerns sending data at startup, which both python (and zproc) already do.i thought you were talking about sending data to child processes in constant* time, while it was running.", "it makes sense to me.as a person with autism, i have always been scared of a misunderstanding with a police officer that would land me in jail, which is a common issue. i would calm my nerves by telling myself \u201cit will be ok. i will think of it as a lot of time to read.\u201di later learned books are very hard to come by at prisons, which adds a bit to my fear of going out and doing things.knowing i would be forced to work is terrifying because it would necessitate so many more misunderstandings.", "maybe i am misunderstanding but \u201cas a service\u201d makes me think they are selling the labor to private companies.given we pay so much in taxes for our prisons, i\u2019m not convinced this is just covering their room and board.according to a quick search, design2part is a manufacturing trade show. i didn\u2019t know that before.", "> for example, in python standard (old standard), it's extremely confusing to work out which gcloud libraries are bundled, and which need to be vendoredi agree. i spent a lot of time trying to figure this out myself with various apis, and in some instances, i gave up and looked for an alternative after i failed to figure out how to correctly vendor some google cloud libs (even following what documentation i did find, which made it look so simple).", "anyone know if cloud iap can handle at least some of the auth wall use cases in this environment, and how to do it?i suspect yes, but the last time i checked the cloud iap docs (for gae standard's recent python 3.7 launch) they didn't yet address the unique situation of second-generation app engine standard environment runtimes.", "hi, i couldn't agree more with the parent comment about the appengine docs. i built a fairly small app and it was utterly excruciating to sort out the nuances.there should be right up front, a crystal clear page detailing the three possible environments.- legacy standard (with a very clear deprecation notice)- legacy flexible (with a very clear deprecation notice)- standard second generation gvisorfor example, in python standard (old standard), it's extremely confusing to work out which gcloud libraries are bundled, and which need to be vendored and packaged into the app (yes i know there is a page that tries to explain. it's unclear).thank you for working on the documentation. it is the number one area that gcp needs to invest in. all the technology is miles ahead of aws in the core areas. the docs however, are very very far behind.happy to elaborate more if there's any specifics you need.", "python.org has an official tutorial on their website:", "i'm the author of this blog post and the original developer of this scheduler system.glad to see a lot of interesting and insightful comments in this thread!some contexts here:0. like any piece of software, this scheduler system is not perfect for every company -- legacy code, # of engineers, skillsets of existing employees, engineering culture, tech stack, business,...1. every engineer in nextdoor needs to do on-call rotation -- 50+ engineers back in 2014 (100+ now?), when the scheduler was built. it's important to run a system that all 50+ engineers have confidence to debug on a friday night if things go south. there are some blackbox-type alternatives of cron, which may be great for a small team of backend experts to operate, but may not be a good fit for 50~100 engineers with very diverse skillsets & backend experience. but why every engineer needs to be on call? well, that may deserve a new thread of discussions:)2. in 2014, there were ~200 production jobs with very different computing characteristics (450+ now). jobs are owned by different teams and different people with different expertise. jobs are frequently (every few weeks?) added, removed, and updated. outages most likely happen during code deployment. it\u2019s important to run a system that works well during deployment (and rollback), e.g., always run the right version of code for hundreds of jobs.3. the core scheduler system was pretty tiny, which could be easily understood by any engineer in the company. i remembered it took me a few days (probably less than a week) to finish most of the code. the hard part was productionization, e.g., carefully migrating 200 production jobs to the new system, logging, monitoring/alerting at job level, deployment & rollback process, oncall-related things (documentation/training)\u2026 with this simple system, we could easily enumerate various failure situations, so oncall engineers have confidence how to respond.4. i\u2019m not with nextdoor any more. but turns out this scheduler system is still working well now: i think the roi is pretty good -- a 3-man-month project (from design to run all 200+ jobs on the new system) => running 4 years and counting & easy oncall situation for 50~100+ engineers.5. why not jenkins or other open source alternatives? we investigated a bunch of alternatives. jenkins is great. back in 2014, nextdoor used jenkins for ci (not sure if it's still true now). we ruled out jenkins (and rundeck or the like) for operational reasons, e.g., challenges to integrate with existing code deployment process, operational complexity for 50+ engineers with different backgrounds/expertise... open source alternatives? well, in 2014, we couldn't find a good python-based project. it's important to limit the number of languages/external technologies in the tech stack.", "i've used app engine for side projects, and small personal projects that only i use. it's great. i've wrote mostly everything in go, and a little python.so far i've been billed $0.00. the free tier is quite generous, and probably plenty for most low traffic endeavors.", "i have a question about web design from a complete idiot: how do you do it? i mean, really, from scratch, how is it done?i have heard that people used to design in a graphics program (photoshop, gimp) what the website should look like and then tried to translate that to css, html, and js, or maybe just to react these days. is that how it's done?right now when i want something to look a particular way, i go into a css file, guess at what a good value might be for a colour or a size or a font, reload the page in the browser, see how it looks, and iterate a few times. sometimes i use the web browser's webdev tools to try to see the changes in these numbers a bit more live.what is it that people actually do? how do you even begin to do something as crazy as css painting? would love to see an intro to web design that starts like those intros to programming that begin with, this is how you install a text editor, this is how you use git, a programming language has variables and functions, and so forth, e.g.", "a bit of a side-topic but, has anyone tried aps (advanced python scheduler - in production?i've been evaluating it as it seems to provide fault-tolerance, but imo the documentation could be much better with more examples (e.g. mixing different triggers, configs,.etc)can anyone comment on it?", "> the flexible environment also continues to confuse me - it talks about being a managed docker container, but it doesn't seem like you actually get access to anything like a dockerfile and that's more of an implementation detail on their end.you're right that you don't get access to the dockerfile. docker isn't a 100%-isolated environment for multitenant use-cases, so google needs to control the stack themselves, using specific trusted versions of base-image layers and app layers. there's maybe extra \"docker-image compile time\" security restrictions made on top of that as well.however, it's not just an implementation detail\u2014it has pretty visible effects.1. a flexible environment app slug gets baked into a regular docker image and placed into your google container registry alongside container-images you've built yourself with gcr. (and your gcr registry, in turn, is backed by a google cloud storage bucket in your own account\u2014meaning that you're paying for storing the resulting images just like any other uploaded objects. you should prune old app engine flexible environment image versions if you don't want to pay!)2. you can do whatever you want with the filesystem of your container, because it is just a docker container. you can mount volumes to the container in the container spec (app.yaml), just like if you were writing a k8s container spec. (in fact, sharing ipc sockets through a docker volume-mount is how the google cloud sql proxy works.) you can internally auto-update your container by downloading new python/node/etc. code into the container and getting the interpreter to re-exec(2) itself. pretty much anything that can run in a \"python docker container environment\" can run in google's \"python flexible environment\", because they're pretty much the same (save for those container-build-time security restrictions.)", "based on some comments from python 3 on gae standard [1], i think cloud task supersedes queue. dsl authentication sounds cool though.[1]:", "on our ci server, i setup an apt-cache container, and have a 'base' image derived from bitnami/minideb that sets the container's apt proxy to apt-cache. there's also squid for http caching, archiva for java artifacts, and devpi for python ones. so, sure, changes will require re-downloads, but they're pretty fast since everything's local. for getting multi-stage builds to work nicely, i just use a makefile, defining separate targets for 'builder' images and 'runtime ones', and copy artifacts from the former into the latter to form the final application images.", "of course, prototyping only + test runs; then moving the code to proper source files (or writing them separately and including them in the notebook). usually i write everything in the notebook, make sure i catch all the bugs, do some initial test runs to see if i am getting anywhere, then separating code out to individual python files. the good thing is i can immediately showcase principles and progress to clients by interleaving markdown with the code, throw in some visualization and stats to the mix, add links to arxiv papers and github repos, showcase multiple approaches/models with preliminary accuracy results or heatmap visualizations etc. then (reasonable) clients are properly informed and can decide which direction they want to move forward etc.", "> i think it's a symptom of times changing. people don't want to code in vi/nano anymore.not sure if that is true. many people are still using vim for many newer languages. i think it depends on the language.some languages are simply more ide-reliant than others. statically typed languages with a large vocabulary (java, c#, scala) tend to benefit more from ides than simple, dynamic languages like python and ruby.and there are newer languages like go. go was designed to not require an ide while not being opposed to one.i think the language server idea is a brilliant one and probably something we'll see more and more of, because it makes a language completely editor/ide agnostic. it would allow folks using vim or vs code to debug or refactor as cleanly as full-blown ides. i suspect this will lead to a decline in the specialized ide market (jetbrains).", "consider something like pypi and the way in which it will host python packages. there's times with python that you will not know what packages are dependencies until you actually download it (this you may notice is why pipenv will take so long to create a lock file, it has to download a ton of packages, get our your network traffic tools and have a look sometime). so i'm fairly sure there are times where the reason you don't know the dependencies exactly is because they are in some form of being unknowable. now you might argue that such systems should be re-implemented and as a generalized goal taken in isolation i would tend to agree (improving existing code including legacy systems is something we consult on in my company). but alas in existing systems you sometimes have to deal with non-ideal circumstances that were created with non-ideal package management tools (legacy python and c++ being prime examples of difficult areas to get 100% right).", "my initial impression of the product was that it was a way to see which packages you depend on are looking for support in terms of funding/support. on a non-technical level this would be really good information for us to know because it would really help us make better open source investments.on a technical level we do the best we can to get to know all the package dependencies and the various other things that we have to be aware of such as security and license issues (we are aware that not everyone does this). but honestly even when you decide to take this seriously it is still hard. because of transitive dependencies and long chains of dependencies in the libraries you use, it is hard to be able to have a high confidence that you know everything you depend on. essentially this requires tooling of some form or another to have any chance. with our in-house code we know what our direct dependencies are and we usually can track down most of what we need fairly quickly because we control our build environments. however when we go consult for a clients who have large codebases that we have never seen before it takes a while to track everything down. sometimes if people have customized build systems this can be really hard to do. we have our own static analysis tools to help with this too.even without all the online package management services it's still a hard problem, consider the case of the humble makefile ( add in dependencies on remote computers and this gets harder. take for example python with it's huge number of different ways of installing a package, we have some tooling to check things but it's probably not 100% accurate because of the various ways in which python packaging is broken exacerbated by the various ways in which people have worked around these shortcomings in the past. pipenv has helped with the lock files but not everything is using those. the power of good tools for package analysis is clear and we use whatever we can. i hope you will see that this is actually a hard problem in a business sense as it costs substantial amounts of time for a business to create tooling for these things and the customer is likely unable to assess the benefits of this directly. we have an obligation and a desire to bring value to our customers and this means we will sometimes have to prioritize a 100% coverage of package information less than other objectives if the client demands it (for example fixing mission critical bugs may be a higher priority). in an ideal world we would love to know every aspect of the stack that we run on but as time goes on the increasing complexity of the systems we use makes this harder.", "the state of app engine environments seems super confusing, but i _think_ this uses the \"second generation standard environment\" like the python 3.7 and node 8 beta environments, which means that unlike app engine of yore, you have access to the full package ecosystem and full network access. i'm not actually 100% sure on this because it's not listed on the runtime page ( but the docs show that you can use composer, unlike the php 5.5 runtime.the documentation for all these second generation runtimes is shockingly bad. i consider google to be _generally_ pretty good at documentation, but this is one of those cases where their propensity for confusing naming and classification of their products makes it hard as hell to figure out what is going on. it doesn't help that the top-level \"product overview\" page is useless marketing junk (microservices! serverless!) instead of a practical description, which is buried several links deep.having never used app engine but occasionally wandering into its docs when a new product gets announced like this, am i right in assuming that app engine's \"second generation standard environment\" is more or less equivalent to the feature set you'd get from heroku? the flexible environment also continues to confuse me - it talks about being a managed docker container, but it doesn't seem like you actually get access to anything like a dockerfile and that's more of an implementation detail on their end.", "this is on the heels of the python 3.7 appengine release from 2 weeks ago[0]. gvisor[1] (user-space kernel) seems to be ready for prime time, which is allowing rolling out new appengine runtimes waaay faster.[0]", "i didn't find the winter trail video but stumbled upon some interesting links:2018 3d scanning: a comprehensive survey bigsfm: reconstructing the world from internet photos j. crandall owens check out his on/offscreen audio source separation work. it's pretty neat. (2018) audio-visual scene analysis with self-supervised multisensory features snavely megadepth: learning single-view depth prediction from internet photos live demo page:)", "you've never had docker do anything weird in production?it didn't happen often, but we definitely had problems. we're running kubernetes clusters with several hundred nodes.caching is very crude. when building its based on lines in the dockerfile which means adding a dependency means redownloading everything. you also can't mount a directory for builds.multi stage builds are very limited in what they can do and often aren't powerful enough to implement efficient builds. you end up either having 20 minute builds or complex make files to work around the inefficient default workflow.fwiw an intelligent caching mechanism should not require manual cleanup. thankfully kubernetes does this for us in production... but the crazy 1 gb images you end up with for a moderately complex python app make it hard. (especially when people use:latest and then there are 12 versions of the app laying around)", "if your jobs are written in python, there is nothing wrong with a python-based scheduler. it can actually be quite convenient.", "as someone that hasn't been using python while the concept of \"virtual environments\" emerged i've always found how many tools for managing them exist confusing. eventually i tried to just create a virtual environment without using any of those tools, which turns out to be pretty straightforward, and wrote a short article about it.", "> this works for big objects, but not for small objectsvery true; i went into some more detail about my typical use case above. using mp for lots of small objects that you've already extracted from raw data/io/whatever is a game of diminishing returns. it's in situations like that where traditional shared-memory starts looking more and more attractive. when i get to that point, while multiprocessing and some other packages provide a few nice abstractions over shmem, i start looking for other platforms than python.> it's not quite that simple; sharing n pages can take very little time or a bit more timedefinitely; i was simplifying in order to compare the overhead of fork with the overhead of pickling/shipping/unpickling data. sharing large pieces of data with even very slow fork()ing is, in my experience, so much faster than the [de]serialize approach that it is effectively constant in comparison, but i didn't mean to discount the complexities of what make certain forking situations faster/slower than others.", "the idea of a type / class / object here is equivalent in that they provide locality of interpretation. there is one place that may contain the code that manipulates these values; in other words, there is one place that contains your understanding - your interpretation - of the integer.you (i.e., all commenters in this thread somehow disputing the accountid object version of the story) are completely right, it is difficult to decide up front what you need. of course, depending on your domain, it may be perfeclty valid to have -5 years or 1800, but i am not asking you to never implement years like this, i am merely suggesting that it is useful to have one place where such decisions, invariants, roles - your understanding of a year, perhaps specific to your domain - are located.how your understanding is structuring is a different concern, because of course, a class with 10000 loc is horrible to work with. but you can always use means of abstraction, and compose independent modules. an int does everything, maybe in one context a year shoudln't be 1800, maybe formatted as 4 digits. a year class / type / object is the place to store these decisions. as i outlined, such decisions are both technical (how many bits?) but may also be domain specific (year must be > 1800). maybe you have an int internally, but abstract it with a non-zero int.it boils down to interpretation. an int allows for many operations, but they may be invalid (in the sense of \"implausible\", or \"undefined\"). for example, an account id of 1 can be multiplied by 2 and then by 2 again, and so on. integers form a relation this way, but not all extensions of this relation might be \"meaningful.integers as ids are a great example for this. technically, they are great, because they are simple numbers, can be typed on a keyboard, and be readily interpreted, but not like integers.6 people two times as many than 3 people. in germany, grades are marked 1 - 6, (1 = very good, 6 = very poor). but the relationship is merely ordinal. a grade of 4 is not \"twice as bad\" as a 2, although the numbers are relatable that way. an account id of 6 is not twice as \"good\" as an id of 3. for ids, you want them to identify, in that they are exclusive and exhaustive - but integer ids are not supposed to be ordinal (in that their sizes are comparable in relation to each other, 6 > 5 is an invalid statement). of course, you can retain this interpretation, e.g. an auto increment id would indicate that id 6451 was created way later than id 6, but this is difficult to interpret; because it doesn't really tell you how much later, and also deletion of integers in between a range may be reissued (id 1, 2, 3, 4, delete 3, 3 is a missing rank, 3 could be reissued). so ids aren't ints, because they are exhaustive, but they aren't ordinal, or at least interpreting them ordinally is dangerous, and some operations aren't allowed even if you interpret them to be ordnial; for example, it would not make sense to calculate an arithmetic mean from integer ids, although mathematically this operation is allowed. in statistics, this concern is discussed as the skalenniveau (german, meaning level of scale), in english it's called level of measurement [1].in sum, it does not matter what your usecase is, an int is an int, but depending on your use case, it might be worthwhile not to pass an uninterpreted integer around, but actually wrap it in some kind of object, where you localize all your decisions how to interpret the integer.none of should be derogated as some form modern hipster javascript, where none of us highlevel kids don't know how to bit-bang a set difference from some account ids; but rather, these ideas are really old. even in c, data abstraction is useful, in that you don't fiddle with integers but define a set of methods, possibly in a module that interact with a hidden internal representation. deciding on where to cut these modules appears to be a difficult task, but we all have known this for a while now [2].as i said, nothing wrong with bitbanging, but encapsulating interpretation in types, classes, objects, modules or functions is a useful strategy to reduce the complexity, and you lose all of it when you just pass integers around and cross your fingers and hope the next developer won't calculate a mean from your integer ids.and finally: the performance argument. tell me how many requests/ops per second you need and let's find out why your program can't do them. make it work, make it fast, not the other way around.i have had too many arguments where people talked about \"performance\" without stating numbers. i had a colleague who argued that joins were bad, because they were slow (which conceptually, they are), but then your database is a highly optimized processor to do exactly these operations. their fear of table-joining yielded a database with few tables, each of which had very long columns, each of which contained values separated by two semicolons, which they would then manipulate using string manipulations. also the table grew, because the lack of normalization caused a lot of redundancy. i have seen many sins in the name of performance, and i will kindly ask about some numbers. performance without numbers is not a good argument against data abstraction.also it can usually be handeled. if your high level python program becomes too slow for some reason, feel free to implement the slow parts in a really fast language and add a clever algorithm in assembler; see for example np and scipy.these approaches aren't mutually exclusive. \"please don't do it\" and \"considered harmful\" doesn't get us anywhere.anyway, coming back to the topic of the thread: how does code become unmaintainable? by following rules of thumb without thinking on their contextual requirements, and most often by passing around integers in the name of performance, while ignoring locality of decision, locality of code, and ease of comprehension. an int is memory, and account id is an intentional interpretation of this memory (an int also being an interpretation of memory already, i get it, this is about abstraction - and finally, true comprehension comes from world-reference. the int doesn't care whether it is 5. you do, see above.)ram and cpu power are less expensive than two developers wasting their time trying to understand some low-level code and identifying which functions expect an int64 and which need a size_t, and whether they are equivalent, and so on, while they could just be passing around some thing with a stable interface, and a localized world-reference (you know, a name).i wouldn't argue that all of this object/type stuff it is the way to go, but these treatments were all invented specifically to solve the issues of raw-integers. and certainly i am quite ok with the fact that not everything uses java; however the ideas we're talking about here are not specific to java, and java in particular often provides a very poor version of the story.i would, however argue, that it is important to be congurent to the unit of expression of your programming language. treating c as if it were object-oriented will give you a bad time, and not using objects in c# and java will also give you a bad time. if your language natively provides an optimized iterator pattern, as does python, coding in the c-for-loop-index idiom will give you a bad time. most unmaintainable java comes from not understanding java, because you mistake it for c without & and *, but with objects.unmaintainable code is about people. code doesn't maintain itself.</rant rel=\"sorry\">[1]", "no, upython is a toy.in embedded s/w, the cost of the h/w outweighs everything else, ok so your 'easily developed' python app will cost (say) 10k less to develop... but the resource requirements mean you need to go up $0.5 on the processor....oops, on your 500'000 devices you've suddenly wasted $250,000. all because you couldn't be bothered to save a few kb of ram.python is a toy for embedded s/w.... dont get me going on catching bugs at runtime in an embedded system because you're using a dynamically typed language! now you have to upgrade 500,000 device over-the air (cellular data costs) which will take a week, meanwhile your customers a fuming because their data is being lost!seriously!", "an honest question: what are really the benefits of having macros in an interpreted language like python?", "'object model' in lisp has two traditional meanings. originally data with identity is called an object. a cons cell, a vector, a string, some numbers, characters, symbols,... all these are objects. one can reference any of them via variables, can return them from functions, give them as arguments to functions. arguments are not copied. function results are not copied. etc. the jvm for example has a similar 'object model' like lisp - thus the gls quote: java developers were half-way dragged to lisp.the second meaning is the object model in the sense of 'object oriented programming'. one of the similarities might be the use of meta-classes. generally though clos isn't that near to the class-based oop object-model of python. clos uses a mixed model of classes and independent generic functions (with multiple dispatch)...", "yeah exactly. i don't understand why they wanted the scheduler to be written in python, since the scheduler should be decoupled from the jobs they are running anyway.", "not micropython.if you're interested in iot (or embedded s/w in general), get away from micropython.the primary characteristic of most embedded products is to be low-cost. when you're selling millions of products, cost counts. you can't waste cycles or resources on python.micropython is a toy for the 'makers'. similarly the js equivalents. no real high volume product would use those technologies.", "...and yet people give me a hard time for using python dict/array literals for config.", "i should've been better with the tone. my view still holds.i know programmers who've written large swaths of haskell and ocaml, who however reach for the eminently dynamic racket for its metaprogramming. they know the relative strengths of each paradigm better than most of us and have fine judgement. however junior programmers (<10 years) who haven't spent the effort required to learn other paradigms and are religious about whatever language they chose to learn as an accident of history are missing out tremendously.most of the static vs dynamic type research has always been about c++ based static types (which combines simula-67's object system with pascal's types - more history here: contrasted first against lisp and later with languages like python, ruby, and javascript. statically typed fp is totally different, and there are papers on either side of the bias to point to. i will recommend the elm language as a good starting point for anyone interested in seeing what typed fp truly can be.", "i've personally moved all my python scripts i run as cronjobs (for my company, not personal scripts) into jenkins, and that's never been a problem.everything is fully dockerized, and jenkins is set to just run the docker image with a specified entry point.", "nms engineer at an enterprise telecom here. at my company, we've been switching over to jenkins for job scheduling. most of what used to be cronjobs have been fully dockerized, and now we have jenkins run periodic \"builds\" via pipelines. the pipelines themselves just run a docker image.the single biggest advantage this has gotten us is centralized logging. i can check on the console output of any cronjob just by going to jenkins and clicking on the job.moving to jenkins to cron wasn't my idea, but the implementation is mine. i've built a few base docker images as bases. one is just the standard python 3.6 docker image. another is the centos image equipped with python 3.6 and oracle rpms for jobs that need database access. another is the aforementioned image plus a number of perl dependencies for jobs that need to call into our legacy perl scripts.for many scripts, i can use identical dockerfiles. i just copy the directory containing the script, requirements.txt, dockerfile, and jenkinsfile, then i change out the script, edit the jenkinsfile to reference the new script's name, and make any needed changes to the requirements.txt.", "i'm familiar with the various attempts to remove the gil over the years.the thing is, in the 90s the choices the produced the gil as it exists were not bad ones; that's why i went to the trouble of explaining how it affects threaded code and why those effects can be considered reasonable tradeoffs for what was known at the time, in implementing threading (without completely breaking the ecosystem of python + python extensions, which was already significant even back then).of course, knowing what's known today about the directions computing and the use of python went in, different decisions might end up being made, but at this point it's very difficult (more difficult than people typically expect) to undo them or make different choices.", "ok. if python2 is deprecated, why write a tutorial in python2 and say \"python3 people can work it out\"", "that was a difficult read for me, the blog post starts out with the four main problems with cron (their use wasn't scalable, editing the text file was hard, running their jobs are complex, and they didn't have any telemetry.)that's great, what does that have to do with cron?as a result what i read was:\"we don't understand what cron does, nor do we understand how job scheduling is supposed to work, and we don't understand how to write 'service' based applications, so somebody said 'just use cron' and we did some stuff and it didn't work how we liked, and we still haven't figured out really what is going on with schedulers so we wrote our own thing which works for us but we don't have any idea why something as broken as cron has persisted as the way to do something for longer than any of us has been alive.\"i'm not sure that is the message they wanted to send. so lets look at their problems and their solution for a minute and figure out what is really going on here.first problem was 'scalability'. which is described in the blog post as \"cron jobs pushed the machine to its limit\" and their scalability solution was to write a program that put a layer between the starting of the jobs and the jobs themselves (sends messages to sqs) and they used a new scheduler (ap scheduler to implement the core scheduler).so what was the real win here? (since they have recreated cron:-)) the win is that instead of forking and execing as cron does allowing things like standard in and what not to be connected to the process, their version of cron sends a message to another system to actually start jobs. guess what, if they wrote a bit of python code that all it did was send a message to sqs and exit that would run pretty simply. if they did it in c or c++ so they weren't loading an entire interpreter and its run time everytime it would be lighting fast and add no load to the \"cron server\", this is basically being unaware of how cron works so not knowing what would be the best way to use it.their second beef was that cron tabs are hard to edit reliably. their solution was to write a giant cgi script in a web server that would read in the data structure used by their scheduler for jobs, show it as a web page, let people make changes to it, and then re-write the updated data structure to the scheduler. guess what, the cron tab is just the data structure for cron in a text form so you can edit it by hand if necessary. or you can use crontab -e which does syntax checking, or you could even write a giant cgi script that would read in the cron file, display it nicely on a web page, and then re-write as syntactically correct cron tab when it was done.problem three was that their jobs were complex and failed often. this forced their poor opsen to log in, cut and paste a complex command line and restart the job. the real problem there is jobs are failing which is going to require someone to figure out why they failed. if you don't give a crap about why they failed the standard idiom is a program that forks the child to do the thing you want done and if you catch a signal from it that it has died you fork it again[1]. but really what is important here is that you have a configuration file under source code control that contains the default parameters for the jobs you are running so that starting them is just typing in the jobname if you need to restart or maybe overriding a parameter like a disk that is too full if that is why it failed. again, nothing to do with cron and everything to do with writing code that runs on servers.and finally their is no telemetry, no way to tell what is going on. except that unix and linux have like a zillion ways to get telemetry out, the original one is syslog, where jobs can send messages that get collected over the network even, of what they are up to, how they are feeling and what, if anything, is going wrong. there are even different levels like info, or fatal which tell you which ones are important. another tried and true technique is to dump a death rattle into /tmp for later collection by a mortician process (also scheduled by cron).at the end of the day, i can't see how cron had anything to do with their problems, their inability to understand the problem they were trying to solve in a general way which would have allowed them to see many solutions, both with cron or with other tools that solve similar problems, would have saved them from re-inventing the wheel yet again, giving them the opportunity to experience the bugs that those other systems have fixed given their millions (if not billions) of hours of collective run time.[1] yes this is lame and you get jobs that just loop forever restarting again and again which is why the real problem is the failing not the forking.", "> porting to 1.0 was a pretty big headache, there was a lot of stuff in full.net that wasn't in 1.0yeah, this.for example, originally they weren't going to include sqldbadapter or datatables or any related classes. they really just thought everybody was going to be okay with using entity framework for everything and not having a generic, non-orm database interface.most of my coding involves either extremely simple tables or where i want to manipulate tables for third-party applications that have over a thousand tables where i don't have access to the source code. most of it is etl or etl-like. it's also usually in powershell or python, but some of it is in c#. making an ef would take ages for some of these applications, especially when the schema isn't always relationally sound (but it's third party so i can't change it). it still just blows my mind they they thought it'd be okay to make everybody box and unbox their database into classed objects instead of just letting you manipulate it as a datarow. as far as i know, they still consider datatable and the like to be \"legacy\".", "why? you can run python or anything else in jenkins just fine.", "the blog said they are a python shop. jenkins being java based probably won't sit well with them.", "one of my favorite parts of python is the idempotent type casting functions.say you have a function that takes a list as input, but sometimes a tuple is passed. wrapping the input like lvalues = list(values), ensures that lvalues will always be a list (or it throws a type error), so you won't get annoying attribute errors, when trying to access the list's methods. in the case of a list type(list([])).__name__ == 'list', as does type(list(list([]))).__name__, etc.this can get a bit tricky with situations where type casting may be undesirable, say casting a string to a list. so judicious use is necessary.", "i use apache airflow with bashoperator for tons of stuff like this, simple web ui for logs/retries, supports dependencies between jobs and when tasks get more complex it\u2019s python and it supports extensions.", "that's kinda funny. once you add comments and function calls to json, you've basically just made a python module.", "this isn't a critique of yaml itself (which, only of people using yaml for things, where they should have started with lua/python/scheme.someone already mentioned ansible. i find it particularly terrible. and bunch of other half-assed dsls, terrible templating languages and such.", "i am a naive coder - i started coding in 1992 with logo, basic and then c/c++ until 2002. i started with python last year and have enjoyed solving mathematical problems on project euler.now, i am building a minor element of credit risk assessment of any borrower by analysing their bank statements. i aim to predict default behaviour of any borrower so that fis could spend more time on decision-making by improving their turn-around-time in reading hundreds of lines of financial statements. for this, i started learning from berkeley's data 8.xnow i plan to switch to fast.ai and hence want to know if there is anything that i need to take care of. i won't have any problem in picking up linear algebra because i did enjoy maths in high-school and engineering.", "however wage arbitrage is specifically against the intent and the letter of visa programs. it\u2019s to fill actual shortages. if you can\u2019t find a python dev at $70k per year but you could find one at $100k per year, that isn\u2019t a \u201cshortage\u201d \u2014 yet such \u201cshortages\u201d are what gets used to rationalize h1-b expansion.", "i see this in a monty python-esque tone:\"we are happy to inform you that we are very sorry to inform you of our previous information that you were accepted to starupschool, when that was sadly, an error on the part of our rejection system falsely telling you that you were accepted, so i am here to tell you that sadly, you were not accepted by our rejection system and your rejection has summarily been rejected by our acceptance department of rejected rejections and sent to the appropriate signatories for sign-off on your sign-up down-below.see you next tuesday should this actually be a rejection of your rejection and an ackknowledgement of your acceptance, whichever may come first\"", "multi-core parallelism isn't so interesting for serious computation. you want to be able to use large distributed hpc systems, but python doesn't seem to have the equivalent of for r.", "because we want first class python multithreading, like many other languages have. if we have to drop down into c, might as well use another language with first class multi-threading like java, kotlin, golang or swift and avoid all the other issues that come with slow gil languages.", "which languages you need, totally depends on the position.i've left the field for three years now, and went on to ios development. however, my observation was that the embedded part was usually rather simple c++ and as soon as possible, python is used.never seen or heard about java used, but that could be a personal thing.not sure about the salaries in the us market. but here in europe, it all depends on the particulars. there's plenty embedded work in semi-governmental and more research oriented sectors. not too much money there. but i've heard that in the semi-conductor industry, there's also embedded work and is much better paying. stress levels seem to be higher though.", "yeah, properly written python is at extreme worst o(1000) times slower than speeding it up code with a numpy/numba/c/fortran/etc. implementation. brute-force loopy code in python i've seen is 100x slower than the compiled alternatives. so i agree, these extreme numbers are the sign of writing the worst possible python implementation of a thing and saying python sucks.", "solidworks and aftereffects are probably the heavy hitters as far as workload goes. everything else you mention is (or should be) very lightweight for a modern computer.for reference, my typical workload for my 2018 mbp is:* at least one windows vm running * at least two web browsers running, sometimes three, depending on what i'm doing. * python as needed * at least one large java application running * the usual office-y applications - outlook, adium, teams, onenote, word, excel * opening large text files in sublime text is a regular occurrence.even though there's a lot of stuff going on, there's nothing that routinely makes the machine spin up the fans, let alone overheat and crash.", ">for clarification, things i do include: * solidworks design, * aftereffects compositing of things made in cinema4d, * python scripting, * watching movies on vlc, * having a shitload of tabs open in chrome at any given time, * opening very large text files in notepad++you realize that that's a workload that 99.99% of users do not share, right?", "> i agree on the top level that a desktop computer in 2018 is actually probably more niche and pro focused than laptops, which have become the default computers for people.really? i wish this were the case. i even invested some serious dollar into getting a decent laptop (carbon x1, 6th generation), but generally in my experience the smaller it is the more unreliable it is.it gets hot, it freezes up, all sorts of annoying problems. it is really frustrating, and now at this point i prefer to do all of my serious stuff on a desktop.for clarification, things i do include: * solidworks design, * aftereffects compositing of things made in cinema4d, * python scripting, * watching movies on vlc, * having a shitload of tabs open in chrome at any given time, * opening very large text files in notepad++", "common ones, i suppose! i have projects involving java, python, and clojure. of those, i would expect the first two to be covered by a generic dependency analyzer.(it's fine if it doesn't, but it would be nice to know up front.)", "they should have renamed.net to something else. now we have to deal with the same issues as the python 2/3 but maybe even worst as it's harder to tell which apis are available looking at sample code.", "> considering how much harder and more error-prone multi-core is, maybe first try a fast sequential solution.most of the python programs referenced on that benchmarks game webpage are in-fact using multi-core?", "i successfully do concurrent+parallel computing with python using asyncio combined with processpoolexecutor. i can see why perhaps that doesn't scratch your itch, but it sure scratches my web-crawling itch.", "unchecked type \"annotation\" was one of the worst features ever added to a reasonably popular programming language. python did ok without type declarations. optional type declarations could help with checking, documentation, and optimization. what went in is the worst of both worlds. type annotations don't do much, and they might be wrong.now this. ugh. this is abusing an internal feature of one implementation.", "> i do agree macros could reduce boilerplate though, but add the extra complexity that someone has to understand ast transformations instead of just regular old python type dynamic code.not only do you have to understand ast transformations, but you also have mysterious stack traces pointing to non-existent code.the amount of tooling to make it debuggable and understandable is significant; the javascript folks have done that lift but that's a large community that was heavily dependent on it.all that said, i think ast transforms have their uses. i wrote a library[1] for pyrsistent to allow normal mutable usage of pyrsistent's immutable data structures. but i think that works because the interface it presents and the behavior it's modeling is ordinary python.[1]:", "this post:", "imo ray[1] is the greatest thing to happen in python parallelism since the invention of sliced bread.also includes best currently available hyperparameter tuning framework![1]", "thanks, i'm not quite that up to date on my 3.x python. cheers.", "care to share a bit more detail on how you did this? was there some interfacing library that you used analogous to cython/swig/etc.? presumably you didn't code directly against the c api (in python.h)?", "it's matrix multiplication.has been in python since version 3.5", "i've done it once, converting about 15 lines of python to rust. it was completely painless and resulted in a large speedup (changed a hotspot that was taking approximately 90% of execution time in a scientific simulation to approximately 0%).type system and expressive macros seems like a big win over c to me.", "could you explain the return statement in your example? i only know the '@' as a decorator in python. this looks like invalid syntax to me what am i missing?", "this is one area where julia truly excels and python falls short", "yes, i am... ( and c/c++ )but i use the right tool for the job. python is a great tool, but not for performance (applies to all dynamic, interpreted languages tbh).", "i would suggest you don't make dramatic claims for a subject that has decades of theory behind it with a huge amount of nuance depending on the exact workload and characteristics of the machines in question.don't get me wrong, message-passing has some advantages, but they certainly aren't that it 'solves' parallelism. if you wish to know more, investigate:- smalltalk and erlang (for message passing languages).- qnx (for a message-passing os)- mpipy (for a message-passing python library, mpi is the grandfather of message passing libraries that runs everywhere).- occam & the transputer for an example of a hardware-mp implementation (actually its communicating sequential processes, but for your purposes it would be enlightening).- golang for a modern-day implementation of csp.- python implementation of csp ( discussion about mp ( for more just google it)basically, its great that you want to learn about concurrency & parallelism, but you've come to a gun fight with a blunt butter knife.", "wild guess: you're not really a python programmer, are you?just curious...", "concurrency in python always ends up the reason to drop it and reimplement in go. also, the code ends up littered with type checks....", "performance doesn't equal better software.in fact, i think performance centric development is a lesser known evil.> have all your data before creating your processes/poolzproc exposes the required api for this (nothing new, just the python api):) (args and kwargs)> a massive datasetwouldn't you be better off using a database for that kind of work?> because of copy-on-write fork magic, many multiprocessing configurations (including the default) can \"send\" that data to child processes in constant timeany resources on how to implement that?", "relevant link: \"pep 563: postponed evaluation of annotations\"", "> the gil has considerable benefits: i don\u2019t have to worry about whether python functions are thread-safe.hold on, the gil doesn't make python automatically thread-safe!you can still have classic data races as the vm can pause and resume two threads writing to the same variable.", "fyi, the python version after 3.9 will be 3.10.", "reading this morning, it appears that my conjecture is correct: norvig uses 'object model' to mean something like 'everything is an object,' 'objects have type, variables don't' & 'introspection of objects & classes is strong.'in that sense, python & lisp are similar. but i don't think that's a good use of the phrase 'object model'; i think it's more usefully applied to the model objects follow.the really radical difference between the lisp & python object models is that lisp objects don't have methods. where in python one adds methods to a class, in lisp one specialises generic functions on classes \u2014 and one isn't limited to specialising on a single argument. there's no clean way in python to implement a method that works with a foo and a bar: one can either implement a method on a foo that takes an argument of any type, or a method on a bar which takes an argument of any type (although istr some rumours of typed python; dunno how that would change things, but the methods would still live inside one class or another).in lisp, though, generic functions are their own things: they don't live inside a class at all. one would just specialise a generic function taking two arguments such that one is a foo & one a bar.the difference is between this:code_removed or this:code_removed and this:code_removed this is so unlike python as to be unrecognisable.there's another difference, which used to drive me crazy in python: when i redefine a class in python, existing instances don't get updated, so if i had a repl with a lot of state i'd have to manually convert the old instances to new ones. but with lisp this happens automatically, and if one needs to run extra code (say, because the old & new class differ in an important way) then there's update-instance-for-redefined-class. that, btw, is a generic function.that's something i really like about lisp: like smalltalk, it's built for programming in the large, on stable systems which have to run for a long time. 'just reboot it' is not a very lispy idiom.", "one more epic discussion on python, where we have the unique opportunity to learn that using c libraries from python is \"cheating\".i could not agree moreit's definitely cheating to use c code with the exception of most python libraries that already are to a large extent nothing more than thin wrappers over existing c libraries or the tiny fact that the most popular by far implementation of python, cpython, is almost 50% implemented in the c language, including the standard library.the author even dared include \"c\" in the name of the implementation.those cheaters, becoming bolder and bolder every day.damn them!!!", "> \"high performance\"i never claimed it to be performant!\"above all, zproc is written for safety and the ease of use.\"(read here - it's not a revolutioni totally agree. it's just a better way of doing things zmq already perfected. like, tell me if you've ever seen a python object that has a `dict` api, but does message passing in the background.> central (pubsub?) server.central server, yes. it uses pub-sub for state watching and req-rep for everything else.> you've just \"discovered\" message-passingguess you're right? 2 years is a peanut on the time scale...p.s. thanks for all the feedback, i've been dying to hear something for a while now.", "why? i can run my python plotting script with multiprocessing in one of the blade servers at work and get the job done quickly. all without translating a big bunch of code to c.", "these are all quite a lot harder to use than go, and often they don't play well together. for example, there are lots of sync libraries (the docker api, the aws sdk, etc) that can't be turned async, so other folks have had to go through the trouble of forking and porting to async and since those other folks are often not affiliated with the original dev teams, who knows what the quality level of those libraries may be? we've also had a lot of problems with asyncio alone--often developers forgetting to await an async call or doing something (i'm not sure what exactly) that causes processes to hang indefinitely. it's all quite a lot more complex than go's concurrency model.and all of that is really just i/o parallelization; there's also cpu parallelization, and i don't believe python has anything that's quite as easy as \"do these two things in parallel\". pretty much everything requires a lot of marshalling and process management which can easily slow a program down instead of improving it.python is great for a lot of things, and the community has found many creative workarounds for its shortcomings, but go beats python in i/o and cpu parallelism handily.", "these are all quite a lot harder to use than go, and often they don't play well together. for example, there are lots of sync libraries (the docker api, the aws sdk, etc) that can't be turned async, so other folks have had to go through the trouble of forking and porting to async and since those other folks are often not affiliated with the original dev teams, who knows what the quality level of those libraries may be? we've also had a lot of problems with asyncio alone--often developers forgetting to await an async call or doing something (i'm not sure what exactly) that causes processes to hang indefinitely. it's all quite a lot more complex than go's concurrency model.and all of that is really just i/o parallelization; there's also cpu parallelization, and i don't believe python has anything that's quite as easy as \"do these two things in parallel\". pretty much everything requires a lot of marshalling and process management which can easily slow a program down instead of improving it.python is great for a lot of things, and the community has found many creative workarounds for its shortcomings, but go beats python in i/o and cpu parallelism handily.", "op probably is overstating a bit, but it is hard to efficiently parallelize computation in python. for example, if you have a large python object graph that you need to compute over, you can't easily parallize the computation without paying some significant serialization cost. you can probably alleviate that by carefully choosing algorithms that minimize the amount of serialization per worker process, but at the end of the day, all of this is still quite a lot harder than using shared memory and goroutines. and not to mention go is 1-2 orders of magnitude faster than python in single-threaded execution... python is great for lots of things, but efficient parallel programming in python is _hard_, even if there are a handful of cases where it's not so hard.", "i agree that not quite all situations will be covered by coarse locks like this. but many will, and my original comment was meant to draw attention to those situations. your previous comment seemed to be saying that everyone already knew about those, but i still believe that some people commenting or reading here weren't aware you could release the gil with just a numpy call.i did also concede that if you do have to write your algorithm completely from scratch, with no scope for using existing c extensions (be they general purpose like numpy or more specialist that implements the whole algorithm) then yes you'll be caught be the gil, so i agree with you on that. but i also made the point that you'll be caught even more (orders of magintude more!) by the slowness of python, so any discussion about parallelism or the gil is a red herring. it's like worrying that you car's windscreen will start to melt if you travel at 500mph; even if that's technically true, it's not the problem you should be focusing on.it's interesting you mention graphs because the most popular liberally licensed graph library is networkx, which is indeed pure python and so presumably isn't particularly fast. there are graph libraries written as c extension modules but i believe they are less popular and less librally licensed (gpl-style rather than bsd-style). so i definitely agree that this is a big weakness of the python ecosystem.", "the gil has considerable benefits: i don\u2019t have to worry about whether python functions are thread-safe. thread-based parallelism is hard to get right, and given the number of workarounds, python\u2019s gil is a total non-issue.", "this is the approach i've taken, albeit at the \"top level\" of the program. since i know i don't have to deal with windows i much prefer simply piping to parallel instead of xargs, or calling make -j8, or similarly letting some shell wrapper handle it over dealing with the overhead inside of python, especially multiprocessing.however, where i think having this stuff available inside of python is useful is that it's cross platform and consumable from \"higher levels\" of python. a library can do some mucky stuff internally to speed computation but still present a simple sync interface, all without external dependencies.", "this is a good point. fine grained parallelism works well with c extensions, but coarse grained does not. which is why data science and matlab like tasks are so often done in python without worrying too much about the python penalty. but if you have a lot of small dense matrix operations, even vba in excel will be faster by more than 10x because you keep popping back into python.", "while i agree that python isn't ideal beyond a certain scope, i think you're overstating how bad it is. my team and i have built a number of non-trivial machine learning products with pipelines that use both the threadpool and processpool components successfully. the headaches we have are related more to the fact that python is dynamic than its concurrency story.", "this only gives you parallelism in one specific situation though - where operations like '+' and '@' take a long time. if they were fine-grained operations, then this doesn't help you.if instead of operating on a numerical matrix, you were instead operating on something like a graph of python objects, something like a graph traversal would be hard to parallelise as you could not stay out of the gil long enough to get anything done.", "speedups of that magnitude suggest the original python approach was particularly inefficient...", "let's say i write this:code_removed if a, b and c are numpy arrays then this function releases the gil and so will run in multiple threads with no further work and with little overhead (if a, b and c are large). i would describe this as a function \"written in python\", even though numpy uses c under the hood. it seems you describe this snippet as being \"written in c instead of python\"; i find that odd, but ok.but, if i understand you right, you are also suggesting that the other commenters here that talk about the gil would also describe this as \"written in c\". they realise that this releases the gil and will run on multiple threads, but the point of their comments is that proper pure python function wouldn't. i disagree. i think that most others would describe this function as \"written in python\", and when they say that functions written in python can't be parallelised they do so because they don't realise that functions like this can be.", "oh i'm not a professional, so this wasn't a super serious project. it's a 1801-series cpu (an old soviet single-chip lsi-11 clone, chosen because of the beautiful pdp-11 isa) with 64kb of sram, an lcd from aliexpress, an rs-232 controller, some leds in key points to illustrate the inner workings, a pulse-modulated speaker, and some discrete logic to glue all this together. it has no flash/rom so you have to load the program from a pc each time. it took me ~three months to build in my spare time, including the pcb manufacturer round trip time. the biggest issue was making a modern c compiler work for this cpu, but in the end i've even got a legacy forth implementation working (more or less). forth is great on this isa because one word translates precisely to one machine instruction.we used it to drive our christmas lights the last time, then to make some simple platform games. i was trying to make micropython run on this board without much success, so we'll probably have to drop it in favor of full-featured python.there was a lot of bugs in this thing, which made it even more fun to use.", "a whole lot depends on what exactly it is that someone wants to get out of using threading.the gil means that a single python interpreter process can execute at most one python thread at a time, regardless of the number of cpus or cpu cores available on the host machine. the gil also introduces overhead which affects the performance of code using python threads; how much you're affected by it will vary depending on what your code is doing. i/o-bound code tends to be much less affected, while cpu-bound code is much more affected.all of this dates back to design decisions made in the 1990s which presumably seemed reasonable for the time: most people using python were running it on machines with one cpu which had one core, so being able to take advantage of multiple cpus/cores to schedule multiple threads to execute simultaneously was not necessarily a high priority. and most people who wanted threading wanted it to use in things like network daemons, which are primarily i/o-bound. hence, the gil and the set of tradeoffs it makes. now, of course, we carry multi-core computers in our pockets and people routinely use python for cpu-bound data science tasks. hindsight is great at spotting that, but hindsight doesn't give us a time machine to go back and change the decisions.anyway. this is not the same thing as \"multithreading is impossible\". this is the same thing as \"multithreading has some limitations, and for some cases the easiest way to work around them will be to use python's c extension api\". which is what the parent comment seemed to be saying.", "obligatory \"concurrency!= parallelism\" statement; concurrency is fine on both platforms with python threading in a single process with a gil; parallelism is less of a done deal.while it's a very big hammer, consider experimenting with celery for your parallelism needs on windows. i've had good results using per-script celery \"clusters\" with either a filesystem (on a ramdisk for extra speed) or an embedded redis backend to accomplish pretty nice bidirectional rpc-ish parallelism. the initial setup is much more complicated than something like goroutines, but once you get it working you can boilerplate it onto other tasks without much trouble.it still won't save you from memory constraints imposed by the lack of good fork() emulation, though. hopefully the wsl stuff will either bring better fork() emulation, or allow support for shared memory objects (e.g. multiprocessing.value) in order to ease some of that pain.", "you're absolutely right (but you're probably gonna get some downvotes for saying that).the ratio between the most-performant parallel framework and the least on python will be a factor of (guessing) 1.5.the ratio between a cpu-bound algorithm written in c and one in python will be of the order of 10000 (again guessing as it's application-dependent).where is your time most profitably spent?", "i came to say this as well. the flexibility of python\u2019s data model generally means that in a lot of cases where you\u2019d want a macro in another language or costly reflection, you can just dynamically create a function or class (with type()), dynamically change things to be properties (by calling the property decorator as a function), mess with how isinstance and issubclass work if you want interface enforcement, etc.i do agree macros could reduce boilerplate though, but add the extra complexity that someone has to understand ast transformations instead of just regular old python type dynamic code.stepping back though, whether with macros or metaprogramming in the dynamic data model, the hugest rule of thumb is: don\u2019t do this stuff. you aren\u2019t going to need it. don\u2019t solve your problem (like arg parsing in the rust example) with code gen.always just write a bit of unmysterious extra boilerplate code that your colleagues of mixed skill will thank you for. that verbose thing that is easier for a wide audience to quickly grok despite uglier code with more boilerplate, that is great software, with mature design decision making. it will live a lot longer and be way easier to incrementally modify than the esoteric code gen approach.", "> complaining that multithreading is impossible in python without using multiple processes, because of the gil... this is not truei think some people's opinions is that if you're writing in c then you're not really writing a python program, so they think it is impossible in python. which seems a reasonable point to make to me.your argument is that python is fine for multithreading... as long as you actually write c instead of python.", "and the article is about \"parallel programming with python\", in order to \"...take advantage of the processing power of multicore processors\".", "> talk [about cython]that was interesting, thanks!i really wish he had shown his numpy code. he said at 13:46 \"numpy actually doesn't help you at all because the calculation is still getting done at the python level\". but his function could be vectorised with numpy using functions like numpy.maximum or numpy.where, in which case the main loop will be in c not python. i can't figure out from what he said whether his numpy code did that or not.but either way, it's interesting that in this case the numpy version is arguably harder to write than the cython version: rather than just adding a few bits of metadata (the types), you have to permute the whole control flow. if there's only a small amount of code you want to convert, i would still say it's better to use numpy though (if it actually is fast enough), because getting the build tools onto your computer for cython can be a pain. and for some matrix computation there are speed inprovements above the fact that it's implemented in c e.g. matrix multiplication is faster than the naive o(n^3) version.", "threadpoolexecutor and processpoolexecutor were exactly what i was waiting for someone to mention. i was doing some python as a systems architect at my previous position and now as a full time data scientist, my life has pretty much been consumed by python. unsurprisingly, a lot of my initial work is retrieving and cleaning very large volumes of data, the later usually being i/o bound and the former being cpu bound and frankly myself and a lot of my team immediately default to using both threadpoolexecutor and processpoolexecutor respectively, because of how simple and performant they are. perhaps asyncio is more familiar terminology to people coming from web dev, so that's why they're gravitating towards it, but there are few times when i find myself needing that particular tooling outside of web dev anyways.", "i had lunch with peter around the time he updated this article. i had written two lisp books for springer-verlag about 5-6 years before. i was still using lisp but i think he had more or less switched to python although he did write a nice scheme system in java around that time.in 2018 i still use common lisp for a major personal project (hybrid ai, wrapping deep learning models so they can be used as functional components (just blogged about this yesterday the end of the day, language matters but libraries, tooling, and platform support matter more.", "because python2 is a deprecated language that will eol?", "> java has the vector type, which allows for sequences of dynamically varying lengththe vector type in java has been not best practice since java 1.5 when the collections framework came out. in particular, the java 1.4 days and before all of the collection classes were synchronized leading to some not insignificant performance hits (even in single threaded applications). arraylist and linkedlist are the classes that came with the rework. in particular, while vector implements list, its backed by an array and so various list like operations (like adding a large number of elements without presizing the array) can take a performance hit.> automatic storage management. java and python support this. lisp implementations tend to be more mature and perform better.there have been numerous changes to the jvm for garbage collection since 1.4. while i can't speak to lisp's garbage collection implementations, java has come a long ways in the past 16 years.> dynamic typing. java attaches run-time type information to instances of the class object, but not to primitive data elements. however, java requires a type declaration for every variable. this has some advantages for production code, but has disadvantages for rapid prototyping and evolution of programs. java does not have a generic/template system that would allow types life vector<string>, and suffers greatly because of it.specifically, vector<string> has been available since java 1.5. java 10 brings local type inference ( ) so instead ofcode_removed in java 1.7 one can write:code_removed and in java 10:code_removed > first-class functions. java has anonymous classes, which serve some of the purposes of closures, although in a less versatile way with a more clumsy syntax. in lisp, we can say (lambda (x) (f (g x))) where in java we would have to say new unaryfunction() { public object execute(object x) { return (cast x).g().f(); } }lambdas are available in java 8. a note, the bit about python closures being read only (\"the only drawback is that closed-over variables are read-only.\") gets to a very interesting debate on first class environments that is described at interactive environment. some java environments allow lisp-like features such as an intgeractive command loop and stop-and-fix debugging. lisp environments are still ahead, but that probably won't last long. bluej in particular has most of the major features of a good lisp environment: you can recompile a method into a running program, and you can type in an expression and have it evaluated immediately. it is intended for teaching purposes, and i can't tell if it is suitable for production use. python has the same interactive approach as lisp, but the environment is less mature than lisp's.eclipse, netbeans and intellij all have rather advanced debuggers. eclipse has a 'display' view as shown in that allows compilation and execution of arbitrary java code.> extensibility. this may prove to be java's weak point. java works well as long as you are willing to make everything a class. if something new like, say, aspect-oriented programming takes off, lisp would be able to incorporate it with macros, but java would not.aspect oriented coding is well supported in java with libraries such as and in the java 1.4 world, what was said was true. that is a very, very long time ago for java and many of the criticisms that were provided are no longer valid.the higher order functions of java are not as lispy as perl's, but they exist as streams. in java 8, one can write:code_removed note that filter and maptoint are both taking lambdas are arguments.i would encourage you to glance at groovy which in java is my favorite perl.code_removed or for a perl style map:code_removed and a perl style grep:code_removed it also has nice support of closures, lambdas currying and other functional programing concepts ( ). there's also some neat things that one can do with jsr 223 and groovy -", "in python 4.0 (the one after 3.9), and in python 3.7 now with \"from __future__ import annotations\", annotations are parsed but their interpolation is delayed. so you can literally have \"var: 0 < var < 3\" (annotation is not a string!) in your code and make it work as you wish.", "i find it strange that nobody ever seems to mention python's concurrent.futures module [0] which is new in python 3.2. i think asyncio got a lot of attention when it came out in python 3.4 and concurrent.futures took a back seat. this article also doesn't mention the module in it's python 2 and 3 differences link.asyncio is a good library for asyncronous i/o but concurrent.futures gives us some pretty nifty tooling which makes concurrent programming (with threadpoolexecutor) and parallel programming (with processpoolexecutor) pretty easy to get right. the future class is a pretty elegant solution for continuing execution while a background task is being executed.[0]", "i hope some of the (new?) concepts from trio find their way into the standard lib.trio: compared to asyncio, goroutines, etc.: on structured concurrency, or: go statement considered harmful\":", "i've yet to play with beyond just experimenting a little bit, but it seems it works very well.i've mainly been looking at these resources: i have not done rust <-> python in real practice", "we effectively don't support python yet but it should not be far away. it's 100% open source and we're looking for contributions.", "does anyone know how well python and rust team up compared to python and c in practice?", "yup, tested it out on a few python projects i have/use and it has 0 dependencies listed which is very, very wrong.", "as a developer working mostly with python this comment makes no sense to me.there are hundreds of libraries to deal with concurrency and/or parallelism in python, asyncio, celery and pyspark being the common ones.all of them provide different approaches to concurrency because the language itself is not tight to one in particular.", "concurrent or parallel? for concurrency, python has asyncio, which many people consider a success.for parallel execution, there's the gil, but in practice it rarely matters, because once you want to do parallel execution, you have most likely a computationally intensive task to do, at which point you call down to c or something, and then gil doesn't matter.", "in response to the multiple comments here complaining that multithreading is impossible in python without using multiple processes, because of the gil (global interpreter lock):this is just not true, because c extension modules (i.e. libraries written to be used from python but whose implementations are written in c) can release the global interpreter lock while inside a function call. examples of these include numpy, scipy, pandas and tensorflow, and there are many others. most python processes that are doing cpu-intensive computation spend relatively little time actually executing python, and are really just coordinating the c libraries (e.g. \"mutiply these two matrices together\").the gil is also released during io operations like writing to a file or waiting for a subprocess to finish or send data down its pipe. so in most practical situations where you have a performance-critical application written in python (or more precisely, the top layer is written in python), multithreading works fine.if you are doing cpu intensive work in pure python and you find things are unacceptably slow, then the simplest way to boost performance (and probably simplify your code) is to rewrite chunks of your code in terms of these c extension modules. if you can't do this for some reason then you will have to throw in the python towel and re-write some or all of your code in a natively compiled language (if it's just a small fraction of your code then cython is a good option). but this is the best course of action regardless of the threads situation, because pure python code runs orders of magnitude slower than native code.", "i'm currently using a headless chrome for my latest project www.blockedby.com (still in alpha stage, looking for feedback)i've been looking at a non-local solution. i'm using python and this article hints that puppeteer is not the only way to invoke this. but i don't see any documentation on the devtools protocol.anyone knows if it's supported? or any providers that do?", "i love python. but its seriously, incapable for doing non trivial concurrent tasks. multiprocessing module doesnt count. i hope the python core-devs take some inspiration from golang for developing the right abstractions for concurrency.", "i believe that since the advent of zeromq, parallelism is possible in almost any language, including python.my library lets you do parallelism in a unique way, where you do message passing parallelism without being explicit about it.", "i think a lot of this complexity can be avoided by just writing single threaded python and using gnu parallel for running it on multiple cores. you can even trivially distribute the work across a cluster that way.", "concurrency and parallelism are two different things. python is fine for concurrency.", "yeah. recently switched some blender python algorithms i wrote to swift/metal, and the speedup was somewhere between 1000 and 1000000 depending on the algorithm.", "i have to work with python on windows and believe me, concurrency is not just fine in python when you can't use fork().", "i think use python3 multiprocess and async is better for product.", "> did they ever fix the global interpreter lock? sort of a show stopper with doing stuff concurrently in python.it means threas-based parallelism of pure-python code is unavailable; concurrency is just fine on python.", "why not re-write the workshop for python3 and require python2 users to wear the pain downgrade brings?", "they did not, which is why this \"course\" illustrates taking advantage of multiple cores via multiprocessing without mentioning the gil at all. which is a little misleading if you think about it.also, by having the introductory chapter be about \"functional programming\" (which incidentally python does not do well), he completely bypasses the serious issue of shared state.which goes to show that parallelism in python is more like a gimmick than a real-world solution since it doesn't let you do in-process shared-memory processing via threads in parallel which is so important for many applications. in my case, the vast majority of the time i do not want to farm workers out to different operating system processes and deal with serialization and communication, but this is the only way for python code to take advantage of multiple cores [1].[1] another way is to write a module in c and have python code call into it on a new thread and release the gil while doing so, but of course this is even worse pain-wise than doing it with multiprocessing and you end up writing/compiling c.", "> if each lisp team is using their own dsllisp already comes with several different 'dsl' built-in:code_removed if one programs in a language with flexible syntax, then a programmer has to learn how to deal with that:code_removed language extension suddenly is a developer activity and not a language designer activity.much of the stuff in the java-world, which has used dsls with different syntax (like being based on xml), can be done directly in lisp.a lisp team will need to learn how to deal with that. there are lisp code-bases which have been maintained for decades. even the sbcl compiler, which is written in lisp, has its roots in code from the 80s (the python compiler of cmucl - unrelated to the later language python).macros make things more complex, but the fear that macros make team programming or development of long-living code impossible is just wrong.", "\"...take advantage of the processing power of multicore processors\"step 1: stop using python.\"you can have a second core when you know how to use one\"now don't get me wrong, python is a perfectly fine language for lots of things, but not for taking optimal advantage of the cpu. performance compared to c is somewhere between an order of magnitude or two slower. considering how much harder and more error-prone multi-core is, maybe first try a fast sequential solution.", "did they ever fix the global interpreter lock? sort of a show stopper with doing stuff concurrently in python. i've done a bit of batch processing using the multi process module; which uses processes instead of threads. this works but it is a bit of a kludge if you are used to languages that support concurrency properly.", "well we've already had the satoshi nakamoto institute [0] (a copy/paste collection of every satoshi nakamoto forum post) become so fanatically strict in their interpretation of the \"gospel\" that a founder split off to set up the nakamoto studies institute [1], just like the people's front of judea vs judean popular people's front split in the life of brian [2].[0]", "erlang has lists and symbols. i do miss symbols when programming python..", "> (note that you must be using python 2 for this workshop and not using python 3. complete this workshop using python 2, then read about the small changes if you are interested in using python 3)why using legacy python for this?", "the author mentions that another blogger, jim randell, found the same error in 2015 when translating the tables to python: also debated whether these were bugs from ada or transcription errors. but, yes, it does seem like an interesting milestone", "do simple hello-world html pages render ok? i found that rendering was slow-ish but totally acceptable for reasonably heavy html pages, so long as we weren't flooding page re-renders (eg by using react without any optimisation of render calls)", "also, he writes 'python's object model is the same as lisp's' which makes me think either he doesn't understand object models (really, really unlikely) or that he & i understand the phrase 'object model' to indicate very different things (more likely).i think 'object model' refers to e.g. how classes, instances & methods work together; lisp & python have radically different object models.maybe he uses it to mean something like 'type model'?", "i'm really curious about the benchmarks used for java (and c++). list processing being 20x slower than c++? array access being 7x slower than c++?so yep. a 16 year old benchmark when java 1.4 was current (java 5 was 2004 which brought with it a collections rework and a lot of improvements with not everything needing to be synchronized) based on a 20 year old update on something that was written in '91.> built-in support for lists. java has the vector type, which allows for sequences of dynamically varying lengththat's ancient history.> dynamic typing. java attaches run-time type information to instances of the class object, but not to primitive data elements. however, java requires a type declaration for every variable. this has some advantages for production code, but has disadvantages for rapid prototyping and evolution of programs. java does not have a generic/template system that would allow types life vector<string>, and suffers greatly because of it. python's object model is the same as lisp's, but python does not allow optional type declarations as lisp does.more ancient history.the java ide being compared is bluej.some of those points were valid in '97. some were interesting in 2002. but languages have changed and java (1.4 -> java 8 and beyond), python (2.1 -> 3.7) and perl (5.7 -> 5.28.0) have all matured quite a bit since then. and then there's the ideas of functional programming that are seeping into other languages.this article aged worse than the bumperstickers from '85 that's also currently on the front page.", "quoting: \"a list-like type with fun functionality. extends the builtin list with.net's language integrated queries (linq) and more. write clean code with powerful syntax. forget about messy loops, conditions and nested filters. multiple filter/map/list-comprehensions, aggregating on one another, are not pythonic at all. i love python, but sometimes - python can be so not pythonic. some of the methods might look rediculous for a single call, comparing to the regular python syntax. the whole idea is is to use it for nested, multiple filters/modifications:).\"sounds reasonable to me.. what do you think? i wouldn't add it as a dependency but it's nice.", "i wrote a collection of dockerfiles for images running python 2.7 or python 3.6 + selenium with either chrome or firefox and using xvfb for the x display (necessary for running selenium headlessly). this, in conjunction with aws step functions, lambda, and ecs, it became merely cents a month to run a headless scraper task in the cloud.", "also, what is this \"don't deserve?\" don't deserve? i'm getting an emotional reaction to this! don't deserve, because we should keep our head down? chug along, dare not ask(all numbers are based on salary surveys in my local market).if the salary range is between $130k - $145k for a full stack developer that knows c# and react and all of the other buzz worthy front end stack and i only know.net asp.net mvc and jquery (which is true), why would they hire me at $150k when they could hire someone who already knows the required tech?on the other hand, if $135k is my minimum, and they were willing to hire me at that, why wouldn\u2019t i take it? spend the next year or two learning on the job and when they inevitably give me a 3% raise for the next two years, jump ship to a new job?a job can offer me two things - immediate compensation and experience that makes me more valuable in the future.when i did leave the job i had been at for way too long, i only had three skills that the market cared about - c/c++, database, and a little c#.i had two offers - one paying 20k more and i could have negotiated $25k more than i was making as a c++ developer and one only paying $7k more with no room for negotiation as an entry level c# developer. if i had taken the higher paying job, i would have been worth less in 3 years - hardly any jobs wanted c++ developers. instead, i took the job as a c# developer. three years later, i was making what the first job offered and i was more marketable for the next 6 years. nine years later, there are plenty of jobs paying $60-70k more than i was making back then for c# developers. for c++ developers, not so much.about a year ago i was in a similar position. i was a \u201csoftware architect\u201d over a small development shop at a large company. it was time to look for another job. i had two offers, one paying $15k more that i was perfectly qualified for -.net framework, asp.net mvc, sql server, windows platform and i would have been over a team of 10.i would have made more at the above job, but using technology that was the opposite of where the industry is going.the other job was a \u201csenior developer\u201d position that needed some exposure to aws (i had a little but not enough to be an expert),javascript, nodejs, react,.net, and python was a nice to have. it only paid $2k more than i was making, i asked for another $7k just because.guess which one i took? by taking the lesser paying job, i already have companies taking me out to lunch, talks with cxos, etc. trying to recruit me directly at thier company paying $35k - $40k more than i\u2019m making now as a full time billable consultant. it would require travel and i want to wait for my son to graduate before i entertain the offers.but since we're talking about bigger companies, soulless entities that will actually sell your 401k out from under you for a profit, i believe the \"right\" and \"moral\" thing to do is to suck every penny out of them as possible, because they'll fuck you in the end if it's profitable.companies have no control over your 401k. they can\u2019t \u201ctake money\u201d out of your 401k account.thank goodness many founders have realized they have to share the pie with equity to get the goods out of the best engineers. statistically, equity is worthless at most startups. they either fail, don\u2019t have an exit event that makes it valuable, or it gets diluted. i would rather have the guarantee of a salary.", "i think the point above was that once the binary is built you don\u2019t have to worry about maintaining the exact same libraries on the server where they might be competing/mixing with various incompatible versions of shared libraries for python or ruby or whatever.that said, yes this is a shameful hole in the golang world (caused by the project leads\u2019 inside-google myopia) but i\u2019m glad they are finally addressing it at this level of integration. and despite a few oddities it looks pretty solid.", "this is a poor characterization of the issue. first of all, \"do the packages come from git?\" is not a real concern (similarly, \"but it came from pypi!\" is not itself a feature). the real concerns are all \"what reproducibility/security guarantees does my package manager provide?\". for `go get` and vanilla `pip install`, the answer is \"mostly none, but this is more or less okay for newbies/prototyping--it's okay though because the packaging docs point them to the better-but-tedious things\". cargo and go modules do a pretty good job about providing reproducibility and usability at the same time, python's answer seems to be `pipenv` which makes some pretty great promises, but has been quite buggy/quarelsome for us so far (and it's not even \"official\" as far as i can tell).to characterize go's ecosystem (but not python's) as \"throwing live chainsaws at people and berating them...\" is pretty blatant dishonesty, doubly so once go modules makes its official debut.", "dude... come on - why does everything have to be a ridiculous tug of war?> getrandom(...,0) isn't the blocking version of getrandomas i have pointed out, and cory pointed out in that thread - getrandom(flags=0) does block in the rare occasion that blocking is needed and at no subsequent time. that's all i said.> nor is it equivalent to /dev/randomwhen did i say that is was? when did i ever even mention /dev/random?you are the only one that keeps bringing up /dev/random. instead of refactoring my statements and then taking a nugget of knowledge in an effort to disagree with me and win a point: read what i have actually written. please. just slow down a little.i really don't want to argue with you any more. i do hope that you stop hammering the \"use urandom use urandom use urandom waaaahh\" thing, but i can't control you.the right answer moving forward, for almost everyone, is getrandom, not urandom.the truth is: before i opened that stackexchange question, i was only 90% sure. but now i'm confident i have the right answer. if i'm still wrong, then by all means correct me - you have a link to the open stackexchange question.python has done the right thing. so have other toolchains. as far as i can tell, everything is good. we can all just chill.", "recognize adds type hints [0], which is what the desired library would use to verify input / select suitable ui elements etc.[0]:", ":sigh:clearly you just don't like go, which is fine. of all the languages i've picked up go was far and away the simplest and most productive, which is why i continue to use it. when i wrote python we were constantly using pip to pull from repos, so moving to go i liked their repository centric view. just because python recommends pip, doesn't mean junior devs don't totally botch dependency management with it. dep management and git are core skills developers need to learn well to produce good software. there is simply no substitute for a bit of elbow grease on the matter. i'm not berating anyone, software is hard and being good at it means taking the time to learn some core tools.", "> resolve your deps once, at build time, and you're done.i have... rarely had this experience.every go package manager has found new and frankly fascinating ways to lead me into a corner where i think i have committed and pushed working code, but which causes highly-visible ci barfing.i should add as an aside: i've done two tours of duty as a cloud foundry buildpacks maintainer. everyone's package system sucks, but some suck a lot less.* maven is basically sane if used with levelled starters. otherwise it's a preview of fighting the many-tentacled ones of the deeply nested dependency graph using nothing but a dull butter knife held upside-down.* gradle is the worst allergic reaction to xml in history.* npm used to have amazing bugs and missing features but has laboriously, slowly improved.* python is about whether you hit the sweet spot of the particular packaging system you use.* php is a mess, unless you use composer, in which case you're still stuck with the weird reality that php's package system is really a mix of in-process modules which need to be compiled and slabs of plain old php.*.net core had what seems like a deliberate policy to come up with the most confusing naming and versioning scheme humanly possible and was wildly successful at doing so. oh, and no canonical reference for versions. none. apparently this improved but it was hell on earth.* golang. a new package manager every ten minutes. a different vendoring model, different assembly model, different bugs, every damn week.only bundler is basically sane. we hit lots of corner cases but for the 20/80 cases it essentially worked.", "good? go package management is awful. it makes tools like searching through a git repo a chore because all of the source code for your module is in the repo too. the op was pointing out how far behind go is behind something like python.if you really wanted to, you could so something exactly like go is doing in python and create a virtual environment with all of your packages and version of python and ship it. there are also ways to build binaries from python.if your machine is locked down by corporate it, you use a virutal environment", "isn't this true for python as well?", "well look, the problem is solved, at least for people using python. so there's really no need on my end to discuss it further.as for community communication and understanding: i really don't think there's any good reason that we all have to lock horns all the time. i do think that you'll find more joy in these sorts of discussions if you can be less caustic and more deliberative. it's no more difficult to be nice than it is to be mean; give it a try. and if you get to the edge of your knowledge (and, it seems to me that you do, fairly frequently, which is great - you are obviously interested in a wide variety of societal topics just like i am), don't just keep marching. stop and acknowledge your short-comings and recognize that there are others who might have had some original / compelling / correct thoughts on these matters also.again, this is no skin off my nose. my current project is python, and python has wisely decided to stop using urandom in favor of getrandom(flags=0), so this is a non-issue.but won't it be nice if the next non-issue can be handled with greater ease and less toxicity?", "i was coding in python almost 24/7 when studying at university and loved teaching so i started offering private python courses for students that had problems with it (it was mandatory also for some non-it studies) in exchange for beer/dinner/cash etc... one of them was my close friend that i knew for a very long time, he got very excited to the point he applied to the python-dev job at a major international corporation. during the interview, he was very shy and the employer didn't have confidence that his python skills are sufficient for the position he applied to so he gave him a coding homework (simple stuff, convert some xml output to a nice html page). he forwarded it to me asking for help if i could do this because he really wanted to get the job. i didn't have the heart to tell him his skills are not as high as they should be so i wrote that homework for him. also as a kind of payback as he actually helped me before to get my job i was working at that time. in the homework, i put there a lot of comments with in-depth explanation of how each piece worked and some of them contained rude words like \"this sh does not handle utf-8 so we need to monkey patch it here and do some black magic below\" with instruction that he should read the comments to understand it and delete them before submitting it for next interview round. of course, he did not. so he went to the next round in the interview with the homework, opened it up. everything working perfectly, even the comments with rude words made a good laugh that he hasn't removed. he made a so good impression with that that the guy immediately put him an offer for employment discarding all previous candidates as he thought he is just a classic it guy that is very bad when it comes to social interaction but when closed in a room, is coding like a beast. when the contract was presented to him, he made a long awkward pause and disclosed that i made the homework and it was not his own work. the guy then made him another offer to give him my contact details in exchange for dinner & beer for both of us if i show up at the interview. i wasn't really looking for a job change but i thought its cool since i was a student eating cheap instant noodles to save money everywhere i could so a nice dinner (normal food, yay!) sounded good for just showing up at the interview. i was immediately presented with an offer that i took even if it was not my intention as i just had a few nasty arguments at the current job. the position was for a fixed time, just for one small project but i started poking around with their security systems and built a complete system for remote forensic acquisition while my manager was looking to free up money to offer me a full-time position. (successfully).this was how i got my previous job, which i already left to fund a startup for providing reconnaissance/osint as a service but that's another even longer story.", "\"i bet this blog post was about javascript:p\"you should bet more often;). that's quite correct. mostly javascript with a bit of python.", "i was likewise shocked at how good go compared to python in this regard.resolve your deps once, at build time, and you're done.compare that to dealing with the externalized deps that is deploying a python app on a machine that might have the correct version of python and all dependencies, or not. virtualenv? good luck. you end up bringing in docker and other tools just to deal with it. but what do you do if the machine is locked down by corporate it and they don't allow docker or the installation of other python deps?", "it must depend on niche. for web development sort of things or python sort of scripting, it is windows that is regularly ignored the most by a wide margin.", "html, css, javascript, vue, and a server-side language such as python.", "recently found a command line library written in go that was so useful to me in a python webapp i'm building, i decided to just wrap the go app and call it from a python subprocess. it is a bit slow to do it this way, but premature optimization, and all that. anyhow, the bottom line is due to this i was forced to get a handle on go this week so i could at least build the go library and make a few changes as needed. wow, was i absolutely shocked to find out current package management in this go app is just pulling libraries from master on github. it really made me feel like go is far behind python in maturity. pulling all these dependency libs from master definitely does not feel production ready. maybe this will help resolve things.", "i graduated in anthropology. in the last year of the course was doing erasmus in brno, czech republic with a friend and we both noticed that a forensic software tool used by all forensic anthropologists for biological profiling was really bad. i mean, so bad it was almost divination. because it's difficult to find a job in anthropology, we decided to create a startup and develop a new better tool. so we learned r, aggregated a lot of human skeletal data (it was so hard because there's a \"gdpr for human anatomical data\") and even published papers on it. it was a success except that nobody bought it. i mean nobody, they all used our papers and online code to do their own thing. in retrospective we were both idiots with no business sense. so of course it failed hard and had to find a job somewhere else. from there on it was always on linkedin: i worked for an environmental consulting company until they fired me and other colleagues because they couldn't pay us; then moved to a japanese zaibatsu where i lost my mind wrangling spreadsheets for the quarterly reports. it was a lot of work and since this was a nearshoring job the pay was very very low. so i kept my linkedin profile as \"actively looking for opportunities\" and a recruiter for an investment bank contacted me about a position in banking operations. i went to the interview and left in the middle of it since that job consisted in copying and pasting stuff in excel all day long. a week later she called me back, apologized for wasting my time (!?) and offered a position as project management officer for a team that implements some machine learning stuff and bots across the whole company. no experience in pm at all and with a basic understanding of python and r so impostor syndrome kicked in really hard but after 6 months here i am, doing just fine. i want this to be my last job, i'm learning more statistics and python so that i can make my own forecasts and trading bots.", "i don't think there's a universal definition (as showed by the other replies to you, saying, actually it's the oppisite...) i think you just have to learn the meaning language by language. here is someone asking about the nuance in python: go can do what it wants as long as it is clear and you can find its definitions easily.", "i wonder why nothing as good as rstudio seems to be available for python yet? i feel like python is stuck in jupyter rutt - a local minima (or maxima, depending on your perspective) where jupyter is capturing all the interest but is fundamentally not as good for data exploration (just my opinion, from using both intensively over years). i often want to use python but i keep going back to rstudio because i find jupyter painful (even after enhancing with beakerx [1] which i love for what it is, but it can't fix the underlying constraints of jupyter...).[1]", "can't help but say this, you are seriously confused. not necessarily your fault, as obviously you dont have the full code.i have mentioned earlier that it is not about precision but about index space. i don't think it's going to be productive use of my time to continue this thread.one specific reason you are getting confused is because you are looking at function calls in isolation not the entire chain of calls through the different python ecosystem libraries. the problem is the indptr and indices arrays that begin their life as int64 arrays get transformed into int32 arrays in specific code paths.by stating that malloc is not relevant i mean its not relevant in this particular instance. by the time control reaches malloc the type mismatch damage has laready taken place.getting a runtime error is far from the end of the matter even if in certain cases we do get runtime errors. what static types saves the user from is the hunting needed to find out where in the chain of functions are we losing the type invariant we need.stopping such bugs is a no-brainer with static types. you claimed at one point up-streams [0] that type systems cannot rule out such errors. if you believe that, this discussion is a waste of time. that's one of the lowest forms of errors a type-system prevents. your comments like these make me doubt your grasp over these things.btw, not sure if you believe large rows and cols imply large nnz [1]. that's not how sparse works.given your handle i would have expected you to be familiar, this is bread and butter stuff in day to day ml. on the other hand if your background was stats i would expect less of computational nitty gritties. nothing wrong with that they focus on different but important aspects.if you really care i would encourage you to track the flow of code from csr creation in scipy. sparse using memory mapped arrays of indptr, indices and nnz to the c code that will get invoked on two such objects, carefully. the key word here is carefully. there is no nonstandard compilation because there is no compilation. its about dispatch to the correct c function.you seem to believe that on a 64 bit platform such indexing error will not happen. that's patently false because it happened many times.in other words, you are saying your ill conceived and incompletely considered notion of correctness are more correct and than test cases that fail.this exactly where a static type system would have helped. those ill conceived incomplete understanding would have been replaced by a proof that proper types have been maintained over the possible flows of control. in this case it would have saved me a lot of time tracking cases where int64 is dropping down to int32.at this point i would stop engaging in this conversation because it has become an exercise in pointless dogma.if you refuse to accept that runtime errors detected or undetected dont have a cost, or that static types can mitigate such costs, -- whatever rocks your boat. what i am claiming is that several times in my python/cython use i hit instances where static types would have saved a lot of trouble and time and money.another common type related problem happens when you need to ensure things remain float32 and do not get promoted to float64. i work both on the large and in the small, so i encounter these.[0]", "the point is that the lrng isn't unknowable, and if the nsa can break it, they can break a lot of other stuff. the lrng isn't built on aes-ctr (but it just as easily could have been); it's (last i checked) essentially a sort of prf built on sha1. all the machinations in the lrng with multiple pools and the elaborate mixing function and the counters are just plumbing around what is essentially a keystream generated from sha1 \"keyed\" by \"entropy\". the \"entropy updates\" the kernel rekey that prf, not because the prf is weakening but because a vulnerability (orthogonal to the lrng) could expose kernel memory and thus the lrng's state, and the updates allow it to recover from that kind of compromise.that's a lot of words because the lrng is overly complicated. but let's make it clearer: what you're saying about every tcp sequence number \"revealing\" part of the \"hidden state\" is exactly equivalent to the argument that every byte of aes-ctr ciphertext is revealing the hidden aes key. if that were the case, aes would be grievously broken.if you don't believe that aes-ctr and the lrng are roughly equivalent, try this thought experiment: retain the initialization functionality from the current lrng --- building a pool of mixed and hashed unpredictable data from kernel events --- and scrap the rest of the lrng, replacing it with... aes-ctr, using a hash of the entropy pool as its key. reads from random/urandom (or kernel random reads, as with your sequence numbers) just tap bytes from the aes-ctr keystream. is every tcp sequence number still revealing a little bit of the rng state?the python script seems entirely silly to me; once the first read from /dev/random succeeds, you're guaranteed to have a fully initialized lrng, and every subsequent read from urandom will be unpredictable and indistinguishable from random. nothing is served by updating the lrng after that point.", "i have not much personal experience with oop either.. but have seen 'fluent python' recommended a lot for this..", "here i have to disagree (in the small) essentially the problem of 'breaking' a rng is discovering that hidden state (the entropy pool) behind the csprng - if you believe that the nsa, and their competitors, don't each have a team of phds pouring over the linux random code looking for ways to do that you're dreaming. after all if you know its state you can predict the keys it will make in the futuremy intuition is that every operation we do (start tcp connections, generate keys, backoff after a failed login,....) reveals a tiny fraction of a bit about that hidden state, web assembly calling random over and over again probably reveals much much more, over time it builds up.(anyone want to guess if there's a quantum computer attack here, it kind of looks like the sort of problem where there might be)so i think that with a sophisticated enough attacker entropy does degrade, the trick is to replace that entropy at a faster rate that the rate at which you guess the worst case attacker might be able to guess.for most people it probably doesn't matter, you're exposing so little state over time you're likely effectively as safe as the parent poster claims, it kind of depends on whether, in this age when we know that nation state actors are watching every packet that passes thru their hands, watching every thing we're doing online, are you interesting enough for them to spend the time and effort? does the us/5is think you're not a trump supporter? are the chinese interested in your trade secrets? whi knows what the russians thinkas i mentioned above using /dev/urandom doesn't have a mechanism you can use to signal this guess about how much entropy it needs over time to be safe, prior to 4.8 we used to have a mechanism that said \"give it some more entropy every n seconds, 60 by default\" but that's now broken(provided here a python applet that periodically drains a little bit of /dev/random entropy if it has plenty available to stir the input pool that /dev/urandom uses for its entropy source)", "i haven't used it, but can julia replace python as a production language for dual use by software devs and data scientists?", "as someone who finds python more difficult to work with than julia, i sincerely hope so.", "i just found this post talking about the book 'idiomatic python': points out some limitations of that book, including one specific example (the 'evil robots attacking' part) that bothered me for the same reasons it bothered the article author.", "i don't understand your objections, like, at all. every \"concession\" you think you're extracting is right there, in a blog post i wrote 4 years ago. those are the goalposts. they haven't moved since 2014.everything else you have to say just reads like word salad. i should be concerned about container rngs, but not linux containers, unless they're the linux containers that don't share the krng? or the iot devices where blocking somehow solves the cold-start entropy problem, and at the same time can't be done in the init scripts? even the python issue thread you posted --- yes, i read it --- disagrees with you. i have no idea what you're talking about and, candidly, i don't know that you do either.i've written (until now) perhaps 1 word on this thread for every 30 of yours, and i think my comments are straightforward and make obvious points. you respond to each of them with a wall of text. can i suggest that maybe instead of a grand disagreement, you've instead just done a poor job communicating whatever your concern might be?regarding that last sentence of yours (er, the one before your \"good day\"): i am extremely comfortable with who does and doesn't find me credible. thanks anyways.", "the best book i've found for learning how to get the best from python's language features is powerful python: python' is also good, but i would suggest that more for a beginner, or for someone transitioning to python from another language. if you have a colleague who loops through a collection by creating an incrementing their own counter variable, then give them a copy of this book.if you already use most of python's idioms correctly (e.g. if you've learned programming primarily from resources about python), get 'powerful python' to learn more about how things work under the hood, and learn ways to make your code more readable and/or more efficient (for cpu and/or memory).", "i remember the great joy it was rewrtot some matlab and bash scripts for neuroimaging to python, and how fast it let us iterate. thanks to that rewriting we ended up having a proper pipeline that we were able to parallelize.since then python is my go-to language, but that didn't stopped me from exploring other languages like julia, for large numerical analysis or go.i see that pythonification he talks about around me, and it's true that is becoming ubiquitous and the defacto langugae on sciences, specially on life sciences.", "no mention of jupyter notebooks \u2014 i think that's a critical part of this change, in that it provides a sharable human-readable analysis format.also for the original poster (whose psycholinguistics work i am familiar with), it's easy to move data between r and python using the rpy packages (which works in notebooks!)", "the astronomy department i worked at briefly during my undergrad underwent a similar process. the ten or so grad students separately used matlab, mathematica, c, idl, and a couple of the newer members used python. they spent some time on the side working on the astropy package and all started transitioning to python as their needed functions and data processing pipelines were implemented.i'm in nuclear engineering now, i'm seeing a similar process. it used to be matlab for everything but once the anaconda python distro came out and made installing python and the science libraries a simple process many undergrads are now using python instead. the spyder editor with the documentation window makes it really easy for undergrads to lookup functions, similar to the matlab editor.", "the code you linked actually seems to refute your claim of this precision error, at least for multiply, because it is using npy_intp for nnz, which will be int64 on a 64 bit platform, and there is even an overflow check below!can you post a gist or link some other concrete example to show how it can overflow the intp type based on large nnz? reading the code, it looks like this could not happen.(note that the entire second step function wouldn\u2019t have this problem, because it\u2019s accessing indices inside the other arrays, after nnz has already been computed, and is not looping over a variable that would overflow, apart from nnz from the first function, which i pointed out above seems not to overflow unless you\u2019re compiling things in a non-standard way that affects npy_intp).i don\u2019t know what your comments about malloc having nothing to do with it are though. that is how numpy arrays possess their post-allocation result for __len__, such as for indices, indptr and data in csr. so __len__ could not overflow an int type (since it requires the platform address space\u2019s int type to allocate underlying contiguous arrays and returns a python integer).", "> there is exactly one time in the lifecycle of an entire system when you want a blocking read from the linux krngright. i'm glad to hear you phrasing it this way.and listen: i don't mean to be flamy here, and it's possible that my sense of these things is tainted by our many... colorful discussions here on hn, but the unradom orthodoxy, to which getrandom and python's os.urandom are in part a response, is something for which you are arguably a tiny bit responsible, having actually written \"use urandom. use urandom. use urandom. use urandom. use urandom. use urandom\" on your blog. that kind of punditry has no place in security engineering.there is a need for a degree of sobriety with these sorts of security considerations. sometimes edge cases move toward the mainstream, and that's exactly what has happened with this odd little phenomenon of programs being executed much, much earlier in the os lifecycle in the case of containerization and iot devices.yes, some containers are deployed in settings where the host krng can be and is used. but not all. and many iot devices suffer from a sufficiently weak set of entropy sources as to need to block.these are real concerns, as identified by the study i linked from the university of michigan. the researchers there were able to replicate over 1% of the ssh keys that they found in the wild because the devices that created them had insufficient entropy at the time of the key generation.all i'm saying is: don't treat these sorts of security issues as subject to orthodoxy. clear thinking and solid engineering are the orders of the day.", "i promise you. if you ignore the churn and learn the fundamentals, your skills will have an order of magnitude longer shelf life.if you know js and web standards then anything on the frontend is just a new application of the same ideas. if you know a bit of networking and a bit of operating systems then web development in python/django/scala are all just interesting new ways to say basically the same thing.if you know the specs then you don't need to find out about technology by following 100 different blogs.occasionally there will be new fundamentals to learn but you'll be ready because you won't have a backlog of relearning framework x's new way of saying \"hello, world\".", "this is a meaningless deadline, given that nobody is actually paid to support python. third party vendors will support it for years to come.", "this isn't nix-shell. nix-shell is a tool for spawning ephemeral software environments composed of nix packages, something a little like python's virtualenvs.digging a little bit, nixos' tests appear to be written in perl and are executed in a context that has access to a bunch of useful primitives for asserting these sorts of things.", "i got fairly proficient in ror some years ago. in that i understood most of the magic and was thinking on what is the best ways to design the program for readability / maintainability.then i got good at java + springthen i got good at react + reduxnow i gotta get good at python + django and the damn magic it does.next i gotta get good at scala.it is frustrating. and i feel that had i spent 10 years in any of those i could really make amazing things in each one. but instead i feel like a generalist. jack of all trades.and that's just the last 5 years. before that was dozens of java frameworks, knowing differences between 4 different databases and wielding sql like a master, and more.the worst parts are:- front-end engineers are looked down upon while i love coding in react + redux + es6js. wish it was typescript.- react changes damn near daily.- i really want to deeeeeeep dive in a tech but i feel lots of fomo that if i don't study a hot new tech i'm screwed. (except vue. vue is the same paradigm as backbone / ember)- i have a kid, time is limited to learn new languages.", "depends on the app. i run some things in the browser but if there is a native app for something, it's always more responsive and faster to use.qt is very fast and very cross platform and qt 5 together with python is coming in a new form:", "see", "i had prior experience with other programming / scripting languages, and picking python up felt very natural.i started with mark lutz's learning python ( about 10 years ago. in the meanwhile i've been an active contributor to large python-based open source projects, e.g., salt ( which not only that it helped me get feedback from people with a tremendous experience, but also reading through the code taught me good practices and many good ways to solve various problems. a few months ago i started reading luciano ramalho's fluent python ( which is just beautifully written and with a wide variety of topics for a deeper look inside python internals.", "lets not weasel with 'typically'.the code that will get called for a multiply is this and important that decisions at the python level trickles down to the correct choice when it comes down to this level.on a 64 bit architecture one would expect that using 64bit int arrays for indices and indptr would ensure that. but thats not the way it works. we regularly encountered cases where it would call the code corresponding to int32. i know why and have special checks and jump hoops to prevent this.thats besides the point, with static types i wouldn't need to do this, the compiler would take care of it.i appreciate your effort to dig through the logic. you have spent time speaking at length in the comment above but unfortunately said little. malloc has nothing to do with it. your third paragraph is manifestly false. why do i say so? because i deal with this everyday and have counterexamples.i didnt mean to ask you to find out. apologies if i wasted your time. i already know why the type mismatch happens. my point was to demonstrate that a lot of manual wading is needed to ensure that it finally bottoms out by calling native code with correct type.", "> \u201cyou have to ensure that it falls back to c code that uses int64 for indexing the arrays. i am sure you are not saying that you do sparse multiplications of this size in pure python.\u201dfor csc and csr matrices at least, these operations typically iterate the underlying indices, indptr and data arrays, and csc `nonzero` uses len(indices), which both relies on (eventually) the c-level call to malloc that defined `indices` (and so uses the systems address space precision, and would never report number of elements in a lower precision int than what the platform supports for memory addressing), and returns this as an infinite precision python int. afterwards it only uses arrays of indices, not integers holding sizes.long story short is that at least for csc matrices, the issue you describe wouldn\u2019t be possible internally to scipy\u2019s c operations, as you\u2019d always be dealing with an integer type large enough for any possible contiguous array length that can be requested on that platform (and the nonzero items are stored in contiguous arrays under the hood).on my team we are not doing pure python ops on the sparse matrices, rather we needed customized weighted operations (for a sparse search engine representation that weights bigrams, trigrams, trending elements, etc., in customized ways) and some set operations to filter rows out of sparse matrices.so we basically rip the internal representation (data, indices, and indptr) out of csc matrices and pass them into a toolkit of numba functions that we have spent time optimizing.", "yeah, as you can see, i pushed back on that.but, as far as i can tell - and i do indeed write with this in mind - the movement to create applications for very fresh, naive installations such as containers or iot devices, does indeed present a problem for purely-blocking random.my sense is that you and others in the community encouraging blind trust of /dev/urandom need to adjust your language when consulting on this matter in the future.since i mostly write python, i'm happy that this has been worked out and that python now uses blocking random (via getrandom(flags=0)) in these very rare situations.", "you probably know it already, but in case not, the csv format is not fully standardized, and so there are variations. so you might have to handle those to provide better support of the feature, if you implement it. example, csv.reader in python's csv module in the stdlib, has a dialect argument, due to this.", "> since even if this flows through other systems, tracking it at the source in numpy would be obvious.you cant possibly be saying that! even if one assumes that source is numpy.regarding the rest, lets say my experience with ocaml has been more gratifying than yours with scala.> we just accumulate the count with a python integer.that wont help when you are using scipy.sparse for sparse on sparse multiplication, because the multiplications fall back to c code. you have to ensure that it falls back to c code that uses int64 for indexing the arrays. i am sure you are not saying that you do sparse multiplications of this size in pure python.our differences in tastes aside, you seem to work on interesting stuff. would love exchanging notes in case we run into each other one day. should be fun.", "our nnz is certainly far greater than 2 billion. the matrix size is around 150 million rows by around 1.7 million columns. we just accumulate the count with a python integer.i don\u2019t know what you mean by \u201cthat\u2019s just numpy\u201d though \u2014 since even if this flows through other systems, tracking it at the source in numpy would be obvious.\u201cstatic types help rule out these cases..\u201d \u2014 i just disagree. that is what\u2019s advertised, but it\u2019s just not true. years of working in scala for very heavy enterprise production systems has made me realize it\u2019s a very false promise. there are actually remarkably few classes of these errors that are removed by static type enforcement, and perfectly good patterns to deal with it in dynamic type situations.if static typing was free, then sure, why not. but instead it\u2019s hugely costly and kills a lot of productivity, rather than the promise that it improves productivity over time by accumulating compounding type safety benefits.i think a good rule of thumb is that anything that causes you to need to write more code will be worse in the long run. there\u2019s no guarantee you\u2019ll actually face fewer future bugs with static typing and visibility noise in the code, but you can guarantee it adds more to your maintenance costs, compile times, and complexity of refactoring.i guess python\u2019s gradual typing is a good compromise, since you don\u2019t have to choose between zero type safety or speculative all-in type safety where the maintenance overhead almost always outweighs the benefits (rendering it a huge and unreconcilable form of premature optimization).you can only add it in those few, rare places where there is demonstrated evidence that the static typing optimization actually has a payoff.", "keala has been hit by the worst floods in a century. 160 dead, 2,23,000 displaced. rescue teams are still scrambling to get to the stranded.the website is an open source initiative that is coordinating the efforts. web developers (django-python), help these guys save lives.", "> we hope that the support built into the llvm ir is sufficient to support a broad class of garbage collected languages including scheme, ml, java, c#, perl, python, lua, ruby, other scripting languages, and more.[0]maybe \"and more\" means there's hope for go style garbage collection.[0]", "i will agree about the 'on the balance' in the context of speed of prototyping and interactive sessions.when rubber is about to hit the road, i.e. near deployment with money at stake, i would have love an option to freeze the types, at least in many places. cython comes in handy, but its clunky and its syntax and semantics is not super obvious to a beginner (i am no longer one, but i remember my days of confusion regarding cyimporting std headers, python headers, how do you use python arrays (not numpy arrays) etc etc).i am curious, have you put money at stake supported only by dynamic types?regarding int32 vs int64, its not a precision issue its about sparse matrices with more than 1<<31 nonzeros. i am equally surprised that you have not run into this given your practical experience with matrices.my case involves more than just numpy. there's hdf5, scipy.sparse, some memory mapped arrays and of course numpy.given the amount of time i spent to debug this, i would have killed for static type checks.", "i used the automate... book. i must say i never found a really good resource to move from that to oop, unit testing etc. i put a lot of scraps together for that. 'python 3 object oriented programming' by dusty phillips was a help, but not the answer. lot's of googling did that.", "i've been playing with writing a python-based awk-inspired library over the last couple weeks. this isn't an implementation of awk like here, but a python interpretation of \"if i wanted to do the sort of things that awk is good at, in python, what would that look like?\"for example, to extract and add line numbers to sql table definitions:code_removed used awk for close to 30 years, but i've never achieved or maintained any level of proficiency at it. i pretty much just use it for \"{ print $1, $3 }\" in a filter or the like. every time i try to do something more complicated i spend an hour or more futzing around with it and more often than not getting to: almost but not quite\" where i want to be. this is, of course, a me failing not an awk failing.but it's left me wanting something that would make doing awk-like processes easy in python, which i'm very proficient at.i ended up using the name \"gawk\" because it's an english word and nods to the awk inspiration, but then i remembered gnuawk so i'll probably rename it.", "while i certainly concede that dynamic typing will have painpoints like this, i just think on balance they create far fewer problems than the maintenance and inflexibility of type system enforcement patterns.that said, i find your particular example with int64 extremely hard to believe. i assume you\u2019re using numpy or ctypes to get a fixed precision integer, in which case it should be extremely easy to guarantee no precision changes, and e.g. almost all operations between np.int64 and np.int32 or a python infinite precision int will preserve the most restrictive type (highest fixed precision) in the operation.i work in numerical linear algebra and data analytics and have used python and cython for years, often caring about precision issues\u2014 and have literally never encountered a situation where it was hard to verify what happens with precision.unless you\u2019re using some non-numpy custom int64 type that has bizarre lossy semantics, it is quite hard to trigger loss of precision. and even then, a solution using numpy precision-maintaining conventions will be better and easier than some heavy type enforcement.", "as a long time python/cython user i can say that i have sorely missed static types in many occasions, especially for long running tasks. in fact i would sometimes use cython not for performance but as a type checker.i can describe a recent example. i had to ensure that an integer is always an int64 as the logic passes through different python modules and libraries. it was an absolute hell to track down all the places where things were dropping down to int32. with static types this would have been a no-brainer. this is not to say that i do not enjoy its dynamic typing where it is appropriate.hopefully python 3 will make things better with optional types. but its still not statically typed, just a pass through a powerful linter.", "i would love to see support for calling out into go functions. the go stdlib so often has good implementations of functionality in places where things like the python stdlib doesn't.there are some obvious questions around calling conventions and error handling, method invocation, etc. but nothing there seems totally insurmountable. having a compliant implementation as a jumping off point is a great start.looking at the interp internals, the representation of function call expressions might need a little bit more structure to pull this off (rather than just a big switch for the awk builtins and user calls as just more awk instructions plopped inline). furthermore there are questions about how exactly to represent go objects but i suspect with some boxing it could be made relatively ergonomic.", "the great thing and the awful thing about open source is that anyone can change what it means to be the \"zen of python.\"> there should be one\u2014and preferably only one\u2014obvious way to do iti feel this can be interpreted many ways. sure, async means there is more than one way you can do requests to get web data now, and more than one way you should structure your app - but not every app needs to do api calls async, and sometimes the complexity is not worth it.other times, yes, of course you need to do requests async. they're taking three minutes each and you have to do twenty of them.", "there are people right now writing production code in nim, crystal, julia, and joy (well maybe not joy except for jonathan blow himself). someone taking the plunge/risk on a new language is exactly how all three of those issues you allude to get resolved (as a dialogue between language users and language developers). i don't the google go point? which big company is behind python? ruby? php? are those all also risky languages to build in?", "you must be quite fortunate to have such excellent connections amongst the privileged elite, most people will never get past \"the algorithm\" if they don't stuff their resumes full of buzzwords to even get past some python script.many companies these days install tracking software on workstations, as well as for programming challenges during the interview process.ibm and defense contractors are known for the good old boy game, so your mindset is not surprising. i used to work at ball and was pushed out for complaining about every single face in every single meeting being white, so i'm something of an authority on this subject.", "you might want to ask this question over at they have been porting most of their codebase from threads and such to python3 async over the past months/years. i'm sure the've hit almost all corners of async doing that.i did some small contributions for ha components and didn't have to get deep into async just yet, but my general feeling is that it is less 'natural' that async programming in a language like go. but go has the bigger advantage is was born with async in mind, so it doesn't lag behind that much with 3rd party libraries support like python does.", "i wrote a fair bit of go at my old job and i just finished a small internal rust utility at my current job. i liked go at first but then got completely fed up with the error handling, the module/folder nonsense, having to define the same function over and over for different primitive types when it came up, and nil. so i picked up rust. it's miles better. the syntax is not as bad you imagine - my utility is about 1000 lines and there are no lifetime parameters anywhere. the functional stuff is clean (closures, iterators, match). there are real enums (none of that iota nonsense) which as a \"mature\" developer have become integral to the design of any system for me. writing macros is easy (basically just like code gen in go). the borrow checker isn't that hard to satisfy - only one thing was a head scratcher and #rust-beginner helped me out there. cargo is great and so is rls. there are a surprisingly large number of libraries given how young the language is (fewer than go for sure but enough that i didn't have to implement anything tedious myself). i mean it's just an all around pleasant experience and you're guaranteed to be writing performant (zero cost abstractions) safe code. how can you beat that? the only thing i miss from go are goroutines and the lightning fast compiler (but the rust compiler is plenty fast for how much it does). so i will probably use rust for everything other than what's fit for a python script from now on.", "you already realized that env vars are an example of dynamic scoping, and someone mentioned exceptions downthread.but pretty much any serious programming language has some (often crappy ad-hoc) implementation of dynamic scoping somewhere, because it's very useful. take decimal contexts in python -- would you really like to pass number of significant digits and rounding mode to every operation? sadly, it's also an example of a crappy ad-hoc implementation (broken with async).", "aside from the special case of \"1\" always being present, i would say that in general you should just use an optimizer to solve knapsack problems. whether you should do so for an interview question is up for debate i guess; using libraries shows you can get something done quickly and efficiently, but implementing your own solver might show you understand the underlying complexity.as far as comparing code complexity of julia to python is concerned, i would say that when you use jump in julia, you should use pyomo/casadi/pulp/... in python. that is not to say that i don't find jump to be a more appealing framework overall. it has wide support for all kinds of solvers, and some julia/jump authors even wrote fairly good miqcp/micp solvers on top of commercial/open-source milp/socp solvers.", "i\u2019m a beginner with python as my first language.i used it to write an arbitrage trading bot. i learned python and asyncio just for this, and it is working as intended.asyncio with aiohttp is used to capture price data from 4 websockets and handle requests to exchanges via their apis.python was easy to learn with \u201clearning python the hard way\u201d, asyncio was also easy to learn thanks to lots of available material online, and it looked less confusing than threading.again, speaking as a beginner, i quite like asyncio because to me it looks just like synchronous code. to me it\u2019s easier to think about and keep track of the control flow and shared data than with threading.asyncio looked like the easiest and fastest way to accomplish my goal, and it worked.", "here's a general purpose decision tree for deciding what to use for graphical applications.code_removed what-part-of-qt-to-usecode_removed", "i always wondered how far human laziness can go? this is an absolutely terrible library. every command line is a human interface for interactions with your program, should not exposed random methods for command line usage...also, developing a python library and not keeping the most basic standard (pep8)? you should not be allowed near python. i really have no better words for this, just fucking lazy. i hate lazy developers, because you are the reason most software is shit.", "yes, and it has been a good experience so far. mostly we've been doing applications involving several rabbitmq producers/consumers, a http server and database access here and there. it relies on the fact that there already are asyncio-enabled libraries for the particular technologies we routinely use.note that our use case might be different to many in this thread - in contrast to we write mainly event-driven message-oriented applications. it is often practical to have e.g. multiple different consumers within one process - without async, we'd need to resort to threads which brings a plethora of new problems. async python does offer a good solution for this.that said, there are a lot of places where async could be improved.language-wise it's pretty okay, but the asyncio library contains too many concepts (some perhaps introduced historically and now deprecated) and the docs aren't doing very good job explaining those. io loop management could be simplified and some of the related functions should have different, better names. error messages and stack traces could also be better when async is involved. as far as testing goes, pytest-asyncio could also enjoy some love.but overall, no major complaints. few catches aside, it works pretty much the same as in c# or javascript syntax-wise, which is what you would expect from this approach to asynchronous programming.", "yes, in all the backend services, api endpoints at the edge and parsing services take 'advantage' of async in python >3.my opinion is -like somebody said- it breaks the zen of python. when i started using it i had doubts about moving to a more async 'friendly' language. but i decided to stick with my old friend python... even when i don't recognize him very much after some changes after version 3.", "a macro processing language is pretty much a language purpose-built for describing how to copy text from here to there while performing certain changes in the process.a templating system is a set of language-specific constructs (a library, a module, whatever they happen to call in that language) that essentially deals only with the latter (i.e. describing how to perform the changes). it's generally up to you to deal with the \"copy text from here to there\" part, although most templating engines give you a specific interface that you have to adhere to.i suppose the difference is better illustrated by pyexpander ( ) which is a macro processing language based on python vs. jinja ( which is a templating engine for python.i don't think any general statement about which on is \"better\" can be meaningful. i suppose that, if you have a full project already written in one language, with all work performed by a single program, it's easier to get what you need via templating engine. if your project is already a collection of tools, whose outputs you need to tie together, it's often less effort to bring in a macro language than write your processing logic from scratch in a non-macro language just to leverage a templating engine. assuming, of course, that you have someone who knows the macro language in your team;-). if all your team knows is jinja2, you're gonna get jinja2.fwiw, i also do a little m4 from time to time (and a long time ago i also worked with gpp) and find both of them fairly easy to use.", "isn't python 3 exactly a way to unify the async approaches? (not saying a good/bad way, just a way) we've had a few projects trying to do async in many ways over the years: stackless, gevent, twisted, asyncio, curio,...async in python 3 goes from \"there's lots of solutions, which should i choose?\" to \"there's an obvious solution in standard library (unless you really want something else for a very specific reason)\"", "async programming in python? yes, as part of distributing parameter finding for genetic algorithms and backtesting.does it use 'asyncio'? nope.", "i really enjoy coding in c (it's probably the language i know the best) and i think c++ is an over-engineered and bloated language, however i'd still pick the latter for something like that. or rather, like most devs, i'd pick a decently small subset of c++. the main reason for that is that in c if you need a data structure that's more involved that a simple contiguous array then you're on your own. a linked list is sort of not too bad to implement (the kernel's list.h is easy to use and reuse for instance) however as soon as you want things like hash tables, trees or anything that benefits from raii imo it becomes a pain to manage everything. it's perfectly possible to do all of that in c (or hell, in assembly if you want to) but in my experience it will take a lot of time to implement correctly (and since you don't have generics it's harder to have general purpose structures that you can reuse easily) and it'll have worse ergonomics because of a lack of destructors and other raii patterns.in my experience c is great for low level things (dealing with hardware, writing small low level services etc...). you don't have to deal with too many abstraction layers, memory management is generally straight forward (your average driver won't usually deal with anything more complex than a buffer queue or ring). for the type of application the author is talking about he'll probably have to deal with rather abstract data structure holding the state of the game, \"a fluid flow of information coming into the app and going out of the app\", concurrency issues etc...is it possible to do all that in c? undoubtedly. if i was tasked to implement something like that would i choose c? hell now. i'd probably go with a subset of c++ like i mentioned before, rust if the dependencies i needed were available (or i didn't mind coding them myself) or maybe even python if i wanted to reduce development time as much as possible to get a poc implementation live as fast as possible.", "fwiw, i was at a talk by an asyncio core developer at europython and he said they are looking very closely at how to improve the asyncio api in upcoming 3.8/3.9 releases. in particular, looking at good ideas they can pull in from libraries such as trio.maybe python 4 will mark a maturing of asyncio?", "i love python but if i wanted a fast async app, i would prefer to write it in go with goroutines. i don't like the async programming style where every function involved in the logic chain has to be async. goroutines with channels are just nicer.python is awesome for a lot of things but go really shines for async style programs.", "that road (c or rust parsing core through bindings) will likely be taken, but for the initial development and jump-starting the ecosystem it was important for me to start with implementations that can be quickly experimented with and iterated and not spend a lot of extra time on dealing with segfaults, memory leaks, the different binding mechanisms on different platforms, etc. as things stand now, people are provided with multiple, fully functioning, pure implementations that already are faster than the majority of yaml/toml parsers. in the coming months and years there will be plenty of time to make things even faster.:)for me caring about performance also means caring about performance on all platforms, why not after all? you can take the tabular benchmark data i provide and paste it together, or use the raw data that is also available as eno files in the repository to compare language against language too (which i initially also did but later dropped because same-language comparison for libraries made more sense to me), if you want the quick run down as far as i remember it: mostly javascript parsers lead the ranking, ruby parsers are a bit behind and just slightly ahead of python.", "bit of an unconstructice rant, but this always bothered me: async is the nail in the coffin of the zen of python.[1]open a python repl, type:code_removed \u2026there should be one\u2014and preferably only one\u2014obvious way to do it\u2026it has not been true for years. python 3 especially, took a sledgehammer to that ole poem.see which is about this effect. i am reminded of that blog post every single time i work with async in python:/[1]", "i don\u2019t use julia for speed but because it is so easy to use and powerful.haskell e.g. is very powerful and elegant but time consuming and hard to learn. lisp is easy to learn and powerful but has kind of clunky syntax.ruby has quite nice syntax and is quite powerful but also kind of messy. python is quite clean and easy to use but not as powerful.julia i would say has hit a sweet spot between all these languages. it is quick to learn and understand while also allowing you to write clean easy to read code. that may describe python. but with macros and multiple dispatch i would say it is a much more powerful language.i also find it much nicer than python to use as a script language as you got way more functionality out of the box.i am a c++ developer professionally, and write little julia script to help me with various boilerplate coding inn c++, processing assets etc. julia is really quick to drop into when you need it.with python i always forget which module some functionality is in. the name of a function etc. julia has much better naming most useful stuff is already invluded in the automatically loaded base module.", "i think this last paragraph is a bit misleading:> use a proper programming languagem4 is a small proper language that can be used directly. jinja2 is a dsl written in python that doesn't actually have access to much of python.m4 is harder to work with in the domain (quoting, escaping, and data modeling) since it is a general purpose language, but it is easier to do general purpose processing tasks with m4 if they violate the stereotypes of what should be done in the domain.", "fire is an interesting project that's great for testing and hacking. but for crafting python clis from scratch, i fail to understand why argh[0] doesn't receive more attention.my only frustrations are a missing default handler for --version, and that i can't use argument names colliding with builtins such as 'input'. all the flexibility of argparse without boilerplate.[0]", "i'm not sure. i get the impression with such old-school macro languages (like m4) you have to use a lot of quoting tricks, evaluation order hacks, and use low level primitives like divert and dnl to build up even the simplest of useful things. many scripts seem to start by inventing a looping construct.while modern templating systems (like jinja2 or t4) use a proper programming language (like python or c#) so using even high-level constructs and complex data models is trivial.", "i agree with what others have said, that there is no best system/framework because it depends on the use-case. but i haven't seen sphinx (which i'm currently attempting to use) here, so i'll mention it. it's often used for web-based docs but has built-in pdf output via pandoc. the hitchhiker's guide to python is one example of a book produced with it:", "i had prior experience with text processing and small scripting tasks with perl, so i found it easier to learn pythoni started with automate the boring stuff videos, it was a good refresher to programming concepts as well. i used to conduct perl basics workshop for my juniors at college, replacing it with python helped me a lot - both during preparing reference notes as well as while teachingif finding an avenue to teach (colleagues, juniors etc) is not available, start doing projects - translating something you did in another language, some project you always wanted to do, etcfor beginners to programming, i would strongly suggest think python and automate the boring stuff", "early on (as a heavy stata and python user) i tried julia and got quite discouraged by its messy treatment of missing values (and weights, etc). i've also tried r but also found lots of inconsistencies, so not enough reason to switch, besides when plotting nice graphs.but i would say julia is increasingly getting there. comparable packages are way easier to write in julia than in stata/mata, while being faster, so any gaps will keep disappearing in the next hears.", "lacks a complete feature set in some cases, though day-to-day it's much nicer. i'll trade curl's (admittedly clunky) --resolve, which is still necessary in some situations, for having native json pretty printing most of the timei also like that it's python and i can probably change something in its source more easily and quickly than i can with curl.", "well, perhaps you have a better answer to my question than forest's answer, to which i awarded the bounty. for my money, i think that's one of the finest and most balanced pieces of writing on this topic available on the internet today.> i don't think you're going to find too many cryptography engineers recommending that you use...the blocking variant of getrandom(2).it depends on what you mean by \"the blocking variant\". to quote cory benfield:> can we please try to be clear about what kind of blocking we mean? getrandom(flags=0) absolutely can block: that's what the original issue was all about. to ensure it never blocks you need to call getrandom(grnd_nonblock): that's why the flag exists.it may (or may not) surprise you to learn that pep 524, which uses this (sometimes blocking) variant of getrandom, has been accepted.here's a discussion of the matter featuring several reputable, kickass security-minded python people discussing the matter and indeed endorsing the sometimes-blocking (flags=0) variant of getrandom: full transparency: i read your post on this topic before i posted my question and found that, as far as i was able to read, it added nothing new to the public discourse on this topic. it does not seem to overcome the very real and well-sourced concerns raised by forest in his answer to my question.anyway, i'm not sure what you mean by \"the wrong stack overflow boards\" - can you tell me which are right and which are wrong?the link which you've provided, which of course i had already read thoroughly, as i pointed out in my question, is not from stack overflow, but from security stackexchange, just like my question.i haven't yet marked an answer correct, so if you actually have something substantive to add that you think is better than forest's answer, it's possible that you can grab the check.", "btw - a tiny bit of python to keep /dev/urandom fed post 4.8 - useful if you have a hardware rng (either an external one or the one in your cpu):code_removed", "> is there a reason to learn julia?jump is why i learned whatever i did of julia and that was mostly because it allowed me to express some things better (not necessarily faster or in parallel).a good example would be this random problem that came out of an interview discussion. almost sure my python implementation is wrong, but i can't quite prove it - the julia one is trivial to understand (a dot product + a minimization function).", "with your comment about iot security...i believe homekit devices are a great example of devices that can almost be perfectly secured. a lot of iot devices support multiple iot platforms, for example the philips hue supports ifttt, google home, amazon, and of course homekit, but the first three options only allow your iot devices to work in your home with permanent wide area network access. latency issues aside, this is bad for security because it simply opens more attack vectors to your devices, and relies on third parties to manage your security. what's the benefit of relying on amazon to manage your iot devices? well for the average joe, it means he won't have to buy a home \"hub\" (apple tv/ipad) for allowing remote access of some sort, and also the setup process is generally easier. problems arise because the iot device is now responsible for accessing the internet, and has to contain a much larger codebase.homekit's design is that each iot device will talk to your local devices, i.e. an iphone, an ipad, an apple tv. if and only if you set up an idevice as a home \"hub\", do you allow remote access. homekit is keeping it modular, which means that if a serious bug is found in remote access code, then you can be confident that apple will update the apple tv's firmware, as opposed to an iot device from a will-be-bankrupt company.now what if you have a rogue device on your local network that is hacking other devices? well this is where a firewall, as mickens' suggests in his talk, can help. keep in mind that this is a problem for any style of iot device, and can only really be protected using a firewall. you can actually create something called a bridging firewall that inspects each packet passing through it's network interfaces. currently, i've bought a small wifi router from mikrotik just for this purpose (only 25 usd). all of my iot devices (and my less secure devices like printers and audio receivers) are plugged in or associated with my mikrotik device, and the bridging firewall acts as follows:a) drops ethernet packets sent to my main router's mac address this stops any wan accessb) drops ethernet packets sent to my home server's mac address, except for port 67-68 allows dhcpc) drops packets sent to any other iot deviceand that's it! i can generally assume my linux desktop and my macbook are secure enough. a few reasons why this is not overkill. first, it separates my two networks without using any vlan nonsense (and avahi/bonjour nonsense), and creates a powerful firewall in between the two. second, it allows my iot wifi network to have a different password from my home wifi network. third, it doesn't slow down my main router's wifi speed, and i would hate to have a 802.11g device slowing down my wireless network. fourth, i believe the firewall can be set up to stop arp spoofing.finally, homekit devices are the few iot device standards that allow you to truly own a device. in fact, after buying the device, you can set up your own local homekit controller in python ( meaning you don't need to buy anything at all from apple.", "as somebody who appreciates julia, my opinion is - probably not. that is, unless you want the opportunity to create a killer library that shows how the language features map well to other domains.but, i think a lot of people in scientific computing are tired of the typeless mess that python/numpy/scipy code-bases evolve to be. and for those people, i think it has a lot of merit.at the end of the day, the language was designed to fill one major gap. a lot of time and effort in r&d is spent either; architecting sane c++ memory models, or reverse engineering existing python code. alternative well performing and safe languages like java simply are not fast enough - to get the features of the modern cpu, you need to be native. and a side-note, matlab cannot usually be ran in a production environment.", "i find julia to be a wonderfully expressive language to write in, without sacrificing any speed. the abstractions allowed me to write less code (especially boilerplate), and maintain cleaner code structure -- mapping to my understanding of the problem. feels much friendlier than python to me, but ymmv. (my experience basically revolves around prototyping various numerical/ml algorithms for exploration and understanding, and not the kind where you just call a library to solve a task. projects mostly in the range of 20 to 2000 lines of code. having used both python and julia for such tasks, i lean towards julia when i have the choice)a couple of my previous hn comments on julia's indexing: and i found it exciting to read community discussions for a language growing towards 1.0, to understand the different approaches that were considered, and why certain choices were made. from what i've seen, the whole development process was quite transparent, and the developers have always indulged sincere questions/suggestions from participants, dealing in concrete examples instead of sweeping generalizations and polemic. i don't know what standard to compare this to, but i've enjoyed the experience.", "lua's niche is absolutely as an embedded extension language for c/c++ programs. it's core benefits is how tiny and yet expressive it is (and it looks like c if you squint.) i've seen it show up in a variety of projects in that role, for games or embedded testing environments.i indeed don't see it having any benefit over python or tcl in more sophisticated environments.", "i generally use pyenv and/or a virtualenv, where the python and pip aliases point to the currently active version, which is almost always 3.", "author of fire here. was super happy to pass the 10000 star mark on github today thanks to this hn post:) -- that boosted my spirits on an otherwise long and rainy (and now delayed) bus ride.hope you find fire useful!what's next for python fire? we're working on improving the help screens and usage outputs so it feels less like a developer tool and more like a professional grade cli. so stay tuned as it gets better!", "had to for work. i switched from a php team to a python and perl team. i vastly preferred python. later, i worked to introduce go at work and now nearly all legacy services are go. now i only use python for little quick scripts.", "i probably depends on your experience with other languages. at the time i learned python, i had done c++ and java in school, and php professionally. i went through \"learn python the hard way.\" it was super quick. at that point, i just started slinging python at work and getting crs from more experienced folk.", "yeah, similar dreams for about 35+ years -- and they are still not fully realized as a spare-time hobby. it's a harder problem than it looks at first. i use the names pointrel and twirlip for my experiments on github, sourceforge, and elsewhere related to triple stores, shared documents, and more. my undergrad work in that area indirectly helped inspire wordnet -- started by my undergrad advisor at princeton, george miller, just as i graduated.one related post on that from me from 2005: \"python implementation of memex\" more recent experiment: love to help realize with that, say:", "interesting. i tried lua about 6 years ago and found myself unable to see an upside to the language in comparative perspective with python and tcl.this is for full featured *nix systems programming in the hpc world so ymmv.", "interesting. i tried lua about 6 years ago and found myself unable to see an upside to the language in comparative perspective with python and tcl.this is for full featured *nix systems programming in the hpc world so ymmv.", "hi, pixelblaze guy here:) i totally get where you are coming from. it would be nice if you could drop this on any old board and be up and running. hopefully i can share some of my perspective.pixelblaze is a commercial venture for me, and i have to figure out how to make some $ from it to pay for my time so that i can keep making it better. i can't afford to spend the amount of time that i do on a hobby, and i don't have a sponsor.if i figure out a way to open source it and have financial incentive to keep working on it, i'll do that. i've been carving out bits and pieces to open source that i think are useful utilities.the hardware itself solves a few problems, such as level shifting, that i haven't see embedded on other esp8266 boards. it also provides a platform for more interesting hardware such as the (fully oss+osh) sensor expansion board.on the pricing side of things, the hardware isn't a majority of the cost. otoh i'm never going to compete with the $3 boards w/ free shipping. i don't have the volume where i can drop the per unit 'software overhead' to a level where i'm competitive with the low end chinese markets.the price of the product as a whole is very competitive in this market. there are cheap no-feature (completely closed) led controllers for not much less, and high-end controllers start in the hundreds of dollars.---------if you want to replicate something like the language in pixelblaze, it's not so bad. its syntactically es6, though without many of the dynamic features of javascript.michael leibman (@michaelleibman) put together this codepen that emulates pixelblaze compatibility here: could use that as a starting point for some kind of rpi nodejs app that pushes pixel data to your esps, or perhaps port the functions (they aren't rocket science) to python.i toyed with the idea, but realized i have too many hobbies already:)", "we use this in production (flags for kube execs of python scripts)it is bounds better than argparse and click. would highly recommend.important to note here, that a lot of this is thanks to the python team's continued extension of the inspect module.", "absolutely, though i'd warn that if you naively use php and a default setup (apache with default settings for php or something like that), you're probably not going to serve as much traffic per server as a naive python or ruby setup will get you. you have to tune your way there, and it can be a bit of a process, but it's definitely possible, and there's nothing very subtle about it.one of the biggest problems with php, imo, is that most of the code you'll find online is extremely old, and probably will give you terrible advice, using old deprecated mysql libraries, etc.", "click makes python attractive for cli's. it is far better than bash and perl for simple stuff. fire looks useful for hacks but click has been excellently intuitive for me.", "hmm, i did this a while back. i realise that i should have suggested a python 3 resource.i wouldn't say that the course is geared towards devs at python. from what i remember it's pure python and there's nothing \"google\" about it.", "i would advise against google's python course, as it is geared toward python devs working at google. the course teaches python 2, and goes against widely accepted guidelines such as pep-8. i highly recommend david beazley's python cookbook ( for the most idiomatic and comprehensive introduction to python.", "who are you teaching to use python and a bleeding-edge library like the op's? you're not teaching them how to install python virtualenvs, or how to install homebrew?", "working with scientific researchers and data analysts.i was a big fan of ruby, and doing web development, so the pressure was on to learn javascript, which i really wasn't enjoying. only partly the language, mainly the framework churn.i was a math major and industrial engineering grad student and did a lot of analytical code in a previous life (numerical recipes in c kind of thing, as well as java interfacing with ilog/cplex). so i managed to get over to that side of life again.ruby isn't big in the scientific computing world, but python is pleasant to work with. the whole thing is a better match for me, kind of glad to be out of web development.that said, it does limit employment options somewhat. there's enough activity in python/analytics/data science to provide some options, but it's also very popular (a lot of people really want to do ml/ai stuff, so the competition is high). if you really have the front end/full stack javascript thing nailed, my guess is that jobs are very easy to come by", "google's python class is a pretty good start[0]. it's enough to get you started and moves at a nice pace! after that i'd hack on a project of my own. even better if you have a project in mind before learning the language. this book by jeff knup will get you familiar with the pythonic way of doing things[1].[0] [1]", "a pure python distributed bug tracker with a web interface. also it has attachments. very easily customizable:", "rust went this path too, there's something i love about having unittesting readily available. i can choose to use it or not use it. my asserts are available to me as i so desire. python too sorta has some unittesting in the standard library, but that's not a compiler level thing.", "all the good ml tools are in python", "on windows, check out keypirinha [1]. it's powerful and has fantastic python plugin support. for example, i've written a plugin that loads all your steam games [2] into the database and allows you to launch them quickly, and another to port your launchy config over [3] (for those like me who used launchy before). it has many other plugins [4] too.[1]", "oh cool, you even went for python 3 + pyqt5 and a module idea. interesting! seeing how your post was from january 2016, we may have started our projects around a similar time, even though mine started as a simple front-end for back in december 2015, starting to support other functionality in, interestingly enough, also january 2016.", "many posts here are focused on classic etl. i'm working on a small project for handling data just after etl.it\u2019s for dealing with annoyingly large data, bigger than ram but sitting on a personal pc. it basically performs sampling and munging for this data. there\u2019s no good solution for this right now (i know because i've been looking for more than a year).what might be interesting to you is that there's little abstraction in the project, but it's non-trivial to execute. to me, this makes it fun. despite its simplicity, it has high utility and could be used by others. this would be a great outcome for an initial project.i've got a working version of it, but it would benefit from the eye of a seasoned python dev.maybe it would be interesting to you to get in touch? my email is: mccain.alex@yandex.comcheers,", "i had a couple of resources i used, codeacademy and a basic python course on udemy. as far as books i used python crash course and blackhat python. blackhat was more learning to write my own tools for pentesting. i also looked at a lot of the tools other folks wrote on github to see how they structured things. and i asked a lot of questions. by no means try one thing when learning python or any language for that matter. best advise i ever got from anyone was to write code everyday.", "leo is far from simple but it's in python, easily scriptable, and built around trees.", "i highly recommend the type annotations along with mypy for helping code readability in python", "i have put together a (very short) post to build a dashboard using shopify data and pulling etl with stich. you could try to implement the stich part in python and would have a complete solution. like looking at stich/blendo would give you some ideas of simple etl workflows. keep in mind that etl changes depending on what you want to do. in theory you can do all etl just with python code. if you have an sql compliant database that can hold your data etl processes could simply be a matter of running sql queries to transform data within the database. then you basically load your data from any data source into your db and you run a bunch of sql to do the transform part.", "in my case, the lack of dropbox on arm has greatly hindered my ability to use my spare phone as a linux desktop. any suggestions? i've heard about building a client from source, and there is a python dropbox client that kind of works but just isn't as good as the real deal.", "i guess you\u2019re mostly complaining about python 2->3? that was a long time ago, and python is hardly the only language to introduce a new, breaking major version.this is a code analysis tool, quite different from your average library. it broke because python introduce new reserved keywords (async/await). this is a non-breaking change for almost all libraries/software (and if it is breaking for you, you probably just have to change variable names or something similar), but code analysis tools are different, they have to be totally up to date with new language features. lots of languages introduce new reserved keywords decently often (in fact, lots have explicitly added async and await somewhat recently), it tends to break very little, except code analysis tools.", "ruamel.yaml in python does that to a certain degree from what i've read, you might want to check it out if yaml is ok for your usecase! ( given this some thought as well, and given that the eno libraries hold their own representation of data in memory this might actually be plausible to implement in some way. still i fear this will turn out to be a hard, hard problem (as eno is not even generically serializable by design), so that's why i haven't explored it further. so for the moment i can only say - maybe in the near future sometime, check back every once in a while!:)", "try `python -m module` as in `python3 -m pip` if you want to make sure to use python3's pip, `python2 -m pip` for python2, `~/bin/python -m pip` for a personal python install, etc.", "not the parent but on ubuntu i use:$ python -m pip... # use python 2$ python2 -m pip... # if you want to be sure is using python 2$ python3 -m pip... # use python 3$ python3 -m venv... # create a virtualenv with python 3 as documented on to \"prevent any potential confusion as to which python interpreter a virtual environment will be based on\"i know it's verbose and long to type but looks explicit to me and saved me to use the wrong version of python several times.", "is an open source etl project written in python. the components are small, composable programs that you can run independently, so you should be able to walk before your run.a good beginner project is to build an adapter to a new data source (known as a \"tap\" in singer). most taps pull data out of business tools like salesforce or marketo, but people also build them to pull characters from the marvel api ( out the getting started guide ( or jump into the singer slack and ask for help (linked from the guide)", "i agree that java makes reading hard with regard to the semantics and unnecessary data structures for simple things like anonymous functions. i would point to python, which is semantically the easiest language to learn. it's still possible to write illegible code (primarily due to the lack of indicator of argument and returns types and, of course, bizarre variable names).", "1) learn to do as much in plain python as possible, focus on lazy evaluation (itertools, yielding,...), you'll be able to process gigabytes with a tiny memory footprint, deployment will be a breeze etc.2) get to know some of the basic python data processing/science packages like pandas, numpy, scipy etc.3) get used to writing short shell scripts - they probably won't be a part of your pipeline, but data engineering, especially development, involves a lot of data prep that coreutils will help you with. throw in a bit of jq and you'll handle a lot of prep work.4) only once you've gotten used to the above, look at dask, pyspark, airflow etc. it really depends on your use cases, but chances are you won't have to touch these technologies at all.bottom line - wait with the heavyweight tools, they might be needlessly powerful. also, work closely with devops, because the deployment side of things will help you understand consequences of your actions.", "afaik you usually have pip (system default python), pip2 (corresponds to python2), and pip3 (corresponds to python3). like others have mentioned, a few distributions have python 3 as the default python.in later versions you can invoke pip using python -m pip install..., which makes it more clear which python is being used.", "luigi is a great python library for building multi-stage pipelines with parameterized tasks. it easily extend to new storage types and targets.we currently use it to build a moderately complex product from dozens of data streams (files, apis and things in-between) and millions of records. at its core is a dag and topological sort, which capture the essence of pipelines and execution.a dag is easy to visualize, even its development over time[1].[1] (generated from the projects' code, in this case", "i've had this experience with a bunch of popular django-related projects recently, where they stop receiving regular pr review and releases. keeping up with django's release cycle then requires digging into the bug trackers and forks for common dependencies. it's totally understandable for that to happen, when even popular projects rely on the volunteer effort of one or two people.one model i'm curious about is \"a collaborative community to share the responsibility of maintaining python-based projects.\" not sure how well that works in practice, but it would be great for something like that to crack the maintainer-burnout problem.", "ah! that makes a lot of sense. if python 3 is the default, it does make sense 'pip' to be the python 3 pip. that way 'pip' always refers to the default version.unfortunately in the ubuntu and mac world 'python' is still python 2.7 and 'pip' refers to the python 2 pip. python 3 is available as 'python3' along with 'pip3'.", "this is an interesting story. python breaks perfectly reasonable code which is surprisingly nothing new for python. a tool stops working. users want fix. author of the tool is unavailable. the tool needs to be forked. a new name needs to be chosen. a new distribution point needs to be established.all because python chooses to break existing code.can't the software designers of this century choose to provide feature flags (use strict, use version, etc.) to enable new features that break existing code?there was a nice and popular article/blog post from someone famous about never ever breaking existing code or functionality because users depend on it but i cannot remember the title of that post to search it.", "as someone who took calculus, linear algebra and a couple cs related courses on my economics program in university 6 years ago, i wish i had someone around who could explain me (better) why and how all those courses can be helpful to me. ml and ai were not big those days (at least i didn't hear much about them as i do know). now, looking for my first python web developer job and dreaming about switching to something more ml related eventually, i am thankful i hadn't completely fail those courses, but feel extremely sorry i didn't study them properly.", "if you want to make it as a career choice, i think you should start with learning java and scala. for better or worse, this field is tied to the jvm and learning these languages will make picking up spark and hadoop (which tbh is a prereq for any data engineering position to have on their resume) a lot easier.also if you are looking to stay in the python world, pyspark is pretty intuitive for any python developer and tons of companies are using it.", "do you know that python has native tcl/tk bindigs? if you are familiar with it and it works for you, just keep using it.", "if 'python' is 3, i'd expect 'pip' to be for 3, and 'pip2' for 2.", "yes. on arch linux you would use just pip. pip2 for python 2.", "i am aware of that. so has been many other distributions like debian, ubuntu, etc.my point was: which command do you normally use to install packages in python 3? pip3 or pip?i thought it was pip3 but the article seems to imply that using the pip command to install packages in python 3 is common too. if this is true, which distribution ships python 3 with pip instead of pip3 for installing packages in python 3? does it not conflict with pip of python 2?", "arch linux has been shipping py3 as the default python for a long time\u2026", "> on some python 3 distributions, you may need to replace pip with pip3 in the command above.are there any distributions where the command pip just works with python 3?in every python 3 installation i have used, i had to invoke it as pip3. so it seems to me that pip3 is the commonly used command and one may need to replace pip3 with pip only on some distributions.", "yep, python doesn't use semver and after 2-3 i doubt they're keen on major version changes any more.from the 3.7 release notes ( incompatible syntax changes:code_removed", "> with the release of python 3.6, it became clear that pylama was in need of maintenance updates. python 3.6 warned that async was about to become a reserved keyword in python 3.7.i can imagine there must be lot of code bases with async as variable name. for example:code_removed this is an error in python 3.7.code_removed but this used to run just fine without any warning or issues until python 3.5.so now any code that uses async as a variable name would break. so is python 3.7 considered a breaking change that is not backward compatible?", "used to use matlab for all data processing. python was free and seemed to be gaining popularity. one day for analysing some data just decided let's see if i can do this in python. went from zero to a fully functioning script with many features in a few hours. i was hooked.", "spark, etc, are great, but honestly if you're just getting started i would forget all about existing tooling that is geared towards people working at 300 person companies and i would read the data warehouse etl toolkit by kimball: learned from the second edition, but i've heard even better things about the third. as you're working through it, create a project with real data and from-scratch re-implement a data warehouse as you go. it doesn't really matter what you tackle, but i personally like etling either data gather from web crawling a single site[0] or push in a weekly gathered wikipedia dump. you'll learn many of the foundational reasons for all the tools the industry uses, which will make it very easy for you to get up to speed on them and to make the right choices about when to introduce them. i personally tend to favour tools that have an api or cli so i can coordinate tasks without needing to click around, but many others like a giant gui so they can see data flows graphically. most good tools have at least some measure of both.[0] use something like scrapy for python (or mechanize for ruby) with css selectors and use the extension inspector gadget to quickly generate css selectors.", "i have been delving into fstar [1] which has an smt backend and provides a means of program verification. fstar programs can then be code extracted to ocaml or fsharp.i think the combination of sat solving, and a formal language like f* that can create executable ocaml or fsharp code will be very useful in creating a pragmatic path to verified programs.code_removed edit: i wanted to add that i prefer the lisp syntax of smt to the python, but that's based upon personal bias. however, a lisp-like syntax was chosen due to the ease of parsing lisp s-expressions.", "i would recommend looking into python-based workflow managers: luigi[0] then airflow[1], to get a hang of scheduling, dags, etc. both are fairly simple to get started with (especially for a seasoned python developer) and are used in production environments as well.[0]", "the team hated perl in an almost religious fashion (although they never worked with nor bothered to learn it) so we settled with python. i must say the ones pushing for it weren't exactly our best programmers.note that i didn't have to \"learn\" python. it brought nothing to the table other than a different syntax and concepts such as generators, iterators, metaclasses and list comprehensions were not new to me.fast forward a few years and while i admit that it probably has the best ecosystem a dynamic language has to offer, i still don't see the point of python language-wise (disclaimer: i don't factor in implementations and ecosystems when talking about programming languages).", "the biggest problem with php is that a lot of smart people don't want to work with it for whatever reason, and you really want these smart people. to succeed you have to have a good team. difficult to assemble one with php as your language.if you use php you are going to lose. the world has moved on. remember that you are not the only game in town. good engineers have opportunities in python, javascript, ruby, rust, golang, whatever. these languages have better reputation, better communities, they are more exciting. php might have become better but it's too late. it wasn't late for javascript, but it is late for php.", "worked in a team that developed it's own tools for nextstep (yeah, that long ago)...glen joined the team and holy shit glen was awesome. was scripting in perl with objectivec bindings until glen looked over my shoulder and told me that was crap. so he wrote python bindings for objectivec over the weekend and made me learn python.can't remember the version, but the only support came from guido van rossum via email so that was a magical time.", "php is very much like java, only being dynamically interpreted. symfony is a very high quality framework, with which can build serious applications.python is surely nice looking, but as it allows redefinition of everything at runtime, it is impossible to optimize at compile time. i think php, with the recent type strictness functionality, is better positioned for making way to new developments like meaningfull code analysis, (compile time) optimizations and performant jit compilers. python is too dynamic for that, unfortunately. the same holds for javascript.i don't know ruby well, but i know it is also slow.the biggest problem with php is that it standards library has inconsistent naming. the syntax is very much java-like. if php would shove the standard library into `legacyprelude` (and have it automatically loaded like haskell) it could improve on that as well. this problem is not as fundamental as the other problems in my opinion, although still requiring a major effort to slowly turn to a new `prelude`.", "hey, pext dev here:)it is indeed very specific to lists now, but theoretically more things could be added. the standard deal though: full-time job and no other contributors.windows users don't need to have python installed. python (with pip) ships with the installers, so you can also use any module in pip by specifying it in requirements.txt file, with standard pip syntax. check the pass module for example to see how that goes.", "i still sometimes write a few line tcl tools to guify a specific task. not sure if i would like to dig into a bigger python code base.for re-using existing modules i doubt pext would scratch my itch in the way i like it. i.e. the password module in the realms is not appealing.", "> because, it is installed on macs, as i said.apple also only supports very ancient set of gnu tools. that doesn't mean that in the majority of cases it is a wise choice to lock yourself to those same outdated tools - especially on a platform that hasn't guaranteed the presence of said tools.> i don't want to have to teach people how to install python 3, when they already have python 2, which is fine.i haven't directly addressed anything to do with your personal choices. however, this thread started with the quote:> no python 2 support means no python supportwhich frankly, isn't true. python 2 is about to be eol'd.> also, redhat had promised support for a while yeti wouldn't depend upon it being long-term however. rhel8 drops python 2, and rhel 7.5 deprecated it. [0][0]", "this is pretty cool. i've been looking for an example code base for creating a simple gui tool.this seems a bit too specific to lists though. would it be possible to write module for manipulating tree data, e.g. something like a filesystem? (drag and drop files to move them between folders)do windows users need to have python installed or does the installer come with python build-in?also, what does the installation story become if the modules require python libraries that are not in stdlib?", "because, it is installed on macs, as i said. i don't want to have to teach people how to install python 3, when they already have python 2, which is fine.also, redhat had promised support for a while yet, and i expect spoke to keep it working until they replace with python 3 (or remove python altogether?)", "yes, but the fact that running `swift` in command line while system python is active spews an unholy amount of error messages is an indication than even there it is going away.", ">>> and it can scale just fine, easily as well as python or ruby if you dig into how to do it right.the webserver is scaled by just adding more instances in parallel with a load balancer. it's the database that needs to scale and that's the challenge.", "i currently use conemu + zsh via msys2 as my preferred shell on windows. i need to run many interactive programs like `python`, `node` etc. via winpty, e.g.:``` alias node='winpty node.cmd' ```with the new conpty, will i be able to run native windows programs directly? if so, that would be huge, winpty (while i'm really thankful it exists) is a pita in practice, see e.g.", "python 2 is eol in just over a year. why would anyone write for a piece of software that will be completely unsupported in 18 months?", "obviously javascript, ruby, golang, python.javascript has become a nice language. you can't avoid it in webdev. rich ecosystem. languages like typescript, elm, purescript, etc. are close.ruby is very practical, has everything, very consistent and clean. php is not consistent and not clean.golang is fast and the simplest language that is actually used in the industry. it is dramatically simpler than c. it is the fastest.python is like ruby, but even more practical because it has ml and math, whereas ruby does not.php has no place in this world.", "python 2 is still the only python provided out of the box on mac.", "python 2's end of life is in 1 year, 4 months.", "sounds familiar. i work as an ase mechanic in a small chain of shops in the midwest. we had a deal with a company that provides code readers for long haul trucks. mostly its just to license their expensive software as the readers themselves are off the shelf. we still have to buy pricy laptops to run them on windows 10.recently i found out we could read codes using a raspberry pi and an adapter. i rigged up a cool little doctors cart with a monitor, a small keyboard, and the pi. i spent a few nights writing a simple python program that takes codes that are output from the reader and compares them against a database i laboriously typed up for techs to read.this went all the way to the owner of the company before i found out, but the owner loved it. later on, after we started culling our software licenses and stopped buying hardware from the vendor, they demanded an audit. the final verdict? we owed then just short of half a million in back licenses and damages for \"copying\" their patent on reading obd type codes from an engine computer. our formal response to their threat of a lawsuit was something along the lines of \"try it.\" i was pretty terrified the whole legal thing was happening, but the owner himself came down to my shop, promoted me to a shop captain, and personally told me to keep making the little readers if people wanted them.we never did hear back from our old software vendor, but every time someone rolls over a pi accidentally or drops one out of an engine bay, i wonder what theyre up to.", "we shouldn't neglect to mention if carpenters were hired like programmers as at similarly we should not miss the paragraph in the op:> we always tried to be creative about probing people and their r\u00e9sum\u00e9s. bethany once decided to analyze the r\u00e9sum\u00e9s of our best data-science people for common features. she found that those people shared an avid interest in music. from then on she and her team looked for that quality. she recalls, \u201cwe\u2019d get really excited and call out, \u2018hey, i found a guy who plays piano!\u2019\u201d she concluded that such people can easily toggle between their left and right brains\u2014a great skill for data analysis.\"music\"!!wow! on violin i got through the d-major section of the bach chaconne. guess i should have applied to netflix!!!then there is the:> say you need a software engineer. do you want a senior programmer fluent in the best new techniques in search engine development?hmm.... my work \"in search engine development\" is based on some original applied math i derived in an infinite dimensional hilbert space. so, can netflix advise me if my work is \"in the best new techniques\"? hmm. who in netflix can (a) give an example of an infinite dimensional hilbert space, (b) say what the connection with search might be, and (c) review the \"best new techniques\"? hmm....but (a) i've written some c code, but regarded it as like digging a ditch with a teaspoon, (b) never wrote any c++, (c) never touched php, (d) never wrote any javascript, (e) never used linux, (f) used only a little, simple css, (g) never used an ide (integrated development environment), (h) never used python, r, matlab, etc.but i've done a lot of scientific, engineering software with lots of algorithms, data structures, etc. i've done a lot in applied statistics and published peer-reviewed original research in mathematical statistics. i hold a ph.d. in applied math from a world class research university.but, no doubt, i couldn't be hired at netflix!!!!my one, short, telephone google interview stopped when i gave the wrong answer to their question \"what is your favorite programming language?\"; apparently the only correct answer was c++; i said pl/i. i'd still say pl/i. how many recruiters at google understood the pros and cons of pl/i versus other languages?with all this nonsense, as in many posts in this thread, in hiring for computer programming, i gave up and am doing my own startup.i picked windows over linux. on windows i picked visual basic.net (apparently essentially equivalent to c# but with a different flavor of syntactic sugar), type into my favorite text editor, kedit with a few hundred macros instead of an ide, used asp.net for the web pages, used ado.net for the sql server access, etc. works fine.if i need python, etc., then i'll learn it and use it. so far, i have all the software development tools i need.for spss, sas, r, matlab, etc., my work in statistics has always been too advanced for those packages -- so i wrote my own code.yup, i wouldn't get hired!!!flip side: i should be able to compete well with the people netflix, etc. do hire!!!!if i need to hire for my startup, then i will emphasize first -- technical writing.", "backend: c#/go/python, docker, kubernetesfrontend: asp.net/javascript/ext.net/css", "if a company already has a large php codebase, then it's probably the right choice to stick with it. and it can scale just fine, easily as well as python or ruby if you dig into how to do it right.but to manage scalable php (or any language, for that matter) you still need smart programmers, a lot of whom will have been through most of the serious academic works in the field. they won't love working with php (i never did), and it might mean greater flight risk (yup!), but they might join the job anyways (at least half a dozen people that i know with those creds that joined up with the last project i was on where we had no choice but to deal with php) because of other reasons, like liking the company, people, or goals.i've worked with a dozen different languages over the last 20 years. php is not one that i'd pick for a new project, but tbh, it's not the worst i've had to deal with. my experiences with the visual basic, perl, and r code that i've had to interact with were actually far worse (typically because the people that code them are not as solid in software engineering), but i wouldn't judge people that use them just based on having done so. unless we're spinning up new prototypes, very few of us actually get to choose the technologies that we use.", "that is a very interesting prospect actually, any plans on making python integration simple? some parts of my pipelines rely on very large python modules i don't see being rebuilt in anything else in the next few years.", "no python 2 support means no python support", "using z3 with python is like having superpowers. i second the recommendation.", "cool! can we use this from javascript? the example shows python, but this would be helpful if i can use javascript.", "i first saw the word \"tuple\" when i started learning python about 4 years ago, and i initially pronounced it \"tuh-ple\" in my head.when i got a job doing python a month in, my co-worker pronounced it \"too-ple\" and i quickly adopted that pronunciation.", "it looks like there is no support for writing data to this formatting. my inner hope was that there was a format i could parse, modify, and write again preserving as much comments and formatting as is reasonable.i looked at the python implementation but did not see that type of functionality. am i wrong?", "all i can do is recommend people to try out the z3 solver, especially through the python api. there's no better resource than", "i work in the aerospace industry as an embedded software engineer. there are a great deal of engineers in the department that use ruby over python (long story so please don't ask). however, i find that python shines in it's community of contributors and users. there are tons of free online documentation, resources, forums, tutorials and even free courses.other than the above, my mentors have stressed the advantages of automating my tasks now rather than later. this was the main drive in getting on board with learning a scripting language quite quickly. i initially tried ruby and it didn't seem like a language an embedded software engineer should be using \"daily\" over the alternative (the language i wanted to learn in first place) being python. i'm sure someone has an alternative opinion on that. another main contributor at the time was the amount of time i spent scp'n files back and forth between two hosts. there's an awesome python library called paramiko that solved all the annoyances with doing this manually. cheers~d", "in high school i wanted to make a twitter bot that tweeted about the weather everyday. i had an easy time with python's libraries / ecosystem.", "i wanted to get sms notifications from a constantly updated google spreadsheet.simplest way to work it and an sms api was using python.", "the pixelblaze guy wants 30$ for his esp8266 board (i get mine for 2,7\u20ac/piece). a lot of that money probably doesn't go into the development of the software, but manufacturing and shipping.i just installed an open pixel control [0] server [1] (receiver) on a wemos d1 mini and can now stream pixel values to it using python [2].this comes with all the downsides, of course, but does add a lot of flexibility. pixelblaze is probably very well written and has some great features (and infrastructure, i'm considering writing a parser for the patterns). however i don't want to pay a gigantic markup for a weird esp8266 board and have to wait for it to ship to me, supporting (while sinking a large part of the money into stupid overhead) the development of software that isn't even open source.[0]:", "well, explaining interface-fails in short text is always a bit hard, but lets see.the key f1 opens the help-widget, which is an area with text, located besides the editor. this area is just there, has now controls, no way to move it or close it. there is some text in it that explains this area can be closed with some shortcut, except it does not work. i don't even know whether it's a problem with my setup, because leo,has no way to check this. just broken by design. notable is that this hacked-in area seems to be used by several other functionalitys and plugins. all with the same fails.another strange quirk of leo is the minibuffer. it's something people knowing emacs will understand, for the rest, it,s just a strange line at the bottom of the window not explaing what it does, what purpose it has or how they can use it. but ok, that's how it is, just following some fade of unrelated culture. really interessing is the usage. you enter some text, then hit tab to invoke autocompletion and surprise..in another part of the application, unconnected to the textwidget of the minibuffer switches a tab and some new widget appears presenting a list of possible candidates. not that qt hasen't a way to present autocomplete-lists, but thats just how it is. quirky hackish, ugly. and of only working as ling as the tablist is not hidden.another funny part is the menu. just look up what is weitten in them, what name, what order they have, and how this compares to the established names and positions in pretty much every other application.or let's take the configuration. nodes with some directives in the outline. no actual documentation or discoverability in the app itself. no support to prevent failures. not even instant-loading of changed settings. restart the program, good enough 20 years ago, good enough today.or let's take the api: single letter variable names. objects with hundreds of unrelated methods and no documentation. plugins which are just python-modules imported from the python-path, and not real plugins with actual structure and api.oh, and the worst, plugins which just don't work and functions which have no obvious result because someone forgot to refresh the outline. though, this is probably just one of the endless number of obvious bugs one encounters at some point.", "i was working on java at the time. while browsing tech sites, i came across python. i think there was a good description of it as a vhll, including the usual term \"executable pseudocode\"; also, there was a good online tutorial about internet programming with python, and i was new to both. got hooked, and never looked back.", "for me, figuring out how to implement a* on a world was one of those programming milestones along the way of me grokking how software is put together. in fact, i see there's a stack overflow question from me still online with an implementation i wrote in python eight years ago ( and i have sometimes wondered how many people have then used that old snippet as a reference.amit's articles were online then, and definitely helped me think through these problems, and their main strength is just how visual the explanations all are. i'm pleased to see they're still online, being updated, and no doubt benefitting others, i know that personally amit's effort was one of a number of things that has allowed me to be a full-time indie game developer all these years later.", "i had been comfortably programming in c and python for years and found 0-based indexing to be perfectly sensible. for the last couple of years i've been programming a lot in julia. initially i was horrified by the idea of indexing from 1 but i got the hang of it remarkably fast. now i feel it's totally natural either way, depending what language i'm in.i think it's only ever been a matter of preference as you say. and julia was otherwise such a pleasure to write code in that it was easy for me to pay the small price to learn that.", "2006, but my highschool was really early in germany with this (and e.g. we weren't allowed to use python in our exit exams years later, since it hadn't been approved yet)", "some of this stuff is bananas, and you're totally within your rights to say, \"please read this pr; i am using this library / this is python.\". but if someone's asking me for a refactor, i say \"let's make a separate ticket for that\", and if multiple people ask me for conflicting things, it's totally fine to say \"please figure this out and update this pr, or i'm ignoring both of you\". i think you made a good call switching jobs, but sometimes a great job is worth dealing with some troublesome processes.", "i teach an intro to python class for people with no programming experience whatsoever that definitely goes through for-loops within the first few hours.if someone has basic proficiency in any other programming language at all, then learning how to do a for-loop in python is a trivial matter of minutes to maybe half an hour, in my experience.", "and the original sc version was written by james gosling, the guy who went on and invented java. i happened to attended a talk by him recently. to my surprise, the guy mentioned very little about java, which i thought to be his greatest achievement. for most of the talk, james just discussed his work in space-communication technology and the development of an autonomous marine-time vehicle in his recent employment. he went on and talked about the iot devices and felt annoyed that people think that they could shove nodejs or python code to a embedded device. it's amazing that i feel like he's still pretty much interested in doing the daily developing/debugging: his eyes were lit up when he talked about debugging the autonomous vehicle underwater in hawaii.", "this is arguably a limitation of c-style for loops.for example, python:code_removed rust:code_removed and many other languages have similar things.proper ranges also mean that the compiler never needs to do complicated reasoning that may depend on signed integer overflow having undefined behavior in order to prove that the loop has a finite number of iterations or that `i` doesn't wrap around.", "i agree. it's a spectrum and my python example is showing one extreme of obviously unimpressive. i was just trying to answer the comment i was responding to.", "> our idea was that if each extensible application supported scheme, you could write an implementation of tcl or python or perl in scheme that translates that program into scheme.a perl-to-scheme transpiler would be quite an accomplishment!", "someone, who can learn enough python in an hour, won't have a lot of problems in life.", "i noticed python in the very early 2000s and my reaction was pretty much like that of esr [0]:1. significant whitespace!? eeeew!2. pause, measured in a month or so.3. hm, code gets written quickly and usually works...4. finished problem solver that eluded me in c.5. sold![0]", "\"the flipside of easy-to-learn is there's no payoff for getting better with the language. your code will always be exactly as tedious as novices' code because they'd rather conserve compiler cycles than spend them to amplify programmers' work.\"in my years of experience with the language, this is, bluntly, untrue. go, used properly, is slightly more verbose than most comparable code in python or perl. if someone is writing code that is shot through with boilerplate in go, then i would say that they may be using \"oh, go just needs lots of boilerplate\" as an excuse.the problem isn't that go lacks abstraction mechanisms; the problem is that you need to learn how to use the ones that are there and not sit there pining for the ones that are not. i find this to be almost exactly like learning haskell; you need to learn to use what is there, not sit there pining for what you don't have. also like haskell, there are some particular points that it all comes together at once and hurts you, but, then again, there's some places in go where i've had big wins using the language features too. it does cut both ways. (i've done some fun things with interfaces, and the pervasive io.reader/writer support, while not necessarily a feature of the language, can make certain things amazingly easy to do while still retaining incredible flexibility.)as one example i went through personally, while by the time i learned go i had a lot of non-oo experience, so i wasn't as stuck on inheritance as someone who only did oo-languages for the last 10 years would be, i still had to adjust to using a generally-oo language (by my standard of the term) that did not support inheritance. it has now been literally plural years since i missed inheritance in go. (in fact, quite the opposite; i miss easy composition in my other oo languages! yes, virginia, it is possible to miss features go has when using other languages, despite what it may seem like if you only read the criticisms.) but my first couple of months were a bit rougher before i internalized how the composition works and affects the design of your code.complaining that go code is all boilerplate is like someone who tried haskell but complains that it's just an especially inconvenient imperative language and you end up doing everything in io anyhow. nope... you have not yet gotten past your \"writing x in y\" phase. that's fine; there's a ton of languages and platforms and libraries in the world. if you didn't get a short-term payoff from using it, go ahead and move on. but you haven't attained enough mastery to go around slagging on the language/platform/library yet.(and, again, let me say that, yes, it is somewhat more verbose that python or something. if you've shrunk your go down to that level, you probably went too far and are doing something ill-advised. but i find that in practice, for most tasks, it is not that much more verbose. there are exceptions, like heavy duty gui code or (imho) scientific code; the solution is not to use go for those.)", "yeah if she learned python and for-loops that day, and was able to ship the desired project to the satisfaction of the judges, that's pretty impressive, right? even if she used some existing code for facial recognition, isn't gluing together libraries the bulk of most software engineers' work?", "the point is that this app is 500 lines of c but it'd be two lines of python:code_removed in this case, wlroots is literally a library for writing a compositor.(\"a small demonstration of the wlroots api\" would have been a fine title for this post. pretending it's a small amount of code is silly.)\"i made a tiny webserver using only the linux kernel\" is impressive. \"i made a tiny webserver using only the golang webserver api\" is not as impressive. \"i made a tiny social networking website using the golang webserver api\" is heading back towards impressive.", ">you had to ask me for help on what for loops and import statements are. i had to give her a crash course on running python code and using git. this girl was fast-tracked to an offer on the watson team. none of the ibm employees understood what she was doing because there were literally zero technical people in the loop - it just sounded/looked cool so her plagiarism went unnoticed.it's going to be brutal when she has moved out there, spent a ton of time and money to relocate herself, and then gets fired for being unable to do the job she was hired for.and it'll be all her own fault.", "my first (real world) exposure to exceptions was python. i was a bit above novice. i would write some code, run it, and it would die of some horrible exception. i would catch that exception, try again, and die again. lather, rinse, repeat. eventually i would wrap everything in try-except blocks. talk about verbose; tab indents everywhere. you can often end up handling exceptions somewhere not immediately close to the call that failed.i liked when i started with go (mostly due to concurrency primatives, but errors were nice too). in my day job, many of the errors have a need for custom handling and i get that for \"free\" in go and i am never surprised by a program crash because i failed to read the docs on a function and what exceptions it may throw (or undocumented exceptions it may throw due to one of its dependencies). i can see right in the signature that i have an error to potentially handle.", "i completely agree it is ok to hear strange jokes and not to be offensive. but to hear 'if you can not handle a monty python reference maybe you shouldn't contribute' is too much for me.", ">why do people keep tiptoeing around this man?because they can't write a rule-abiding patch set and yet feel the pressure to deliver off of their half-baked code. under the pressure from their customers/bosses/whomever, they try to weasel their way in with nice words rather than with merit.patches that go by the rules and merge cleanly are graciously welcome. no tiptoeing ever needed or expected.also, they could \"just fork linux\", but nobody would take such project seriously without a well-established leader always on guard to reject any code smell. you don't get repeatable success spanning decades, and \u201cworld domination. fast.\u201d [1] without a focused and stern leader like torvalds, jobs, bezos, or musk. polite committees give us c++11 and xhtml 2.0 and posix and other such footguns, because nobody stood up and called out bullshiters on their bullshit. on that note, watch python, now that guido the bdfl is no more.[1]", "can you imagine that there are people who did not watch monty python at all? i know it is not always easy for a western person to accept a fact that there are different cultures around the world.", "there's an event loop implementation for python based on libuv: compatible with the native asyncio event loop and can be used as a drop-in replacement.", "linus' reasoning echos his past criticisms of hardening patches:>if people run things on real machines, then bug() is absolutely the last thing you ever want to do for \"debugging\".>this is why i scanned your pull request for bug() and similar. because i simply will not take \"hardening\" that kills the machine. that's a hard requirement. no excuses, and absolutely zero exceptions.>after a year or two, when the hardening has actually been in place, and you can say \"hey, look, none of the warnings happened\", i may be ok with turning them into bug() calls.the monty python drama is a distraction, this is an issue of principle that linus has had to explain multiple times.", "from his reply:>i absolutely refuse to take any hardening patches at all that have bug() or panic() or similar machine-killing in it. i care not one whit about the reason for them. in fact, if the reason is stated as \"it makes debugging easiler\", then i fart in your general direction and call your mother a hamster.yes \"this is typical torvalds\", but to me the tone suggests his personality is still stuck in the mid/early 90ies. it was perfectly ok to use insults dressed as humor in order to win arguments with the \"unwashed masses\" on usenet. it was funny back then but seeing this in a middle-aged man in 2018 tells me the guy still hasn't grown up.also nobody expects the spanish inquisition, but i'd expect at least some fresh jokes, because the monty python references were already old back then.", "if here are some leo people reading here:the \"screen shots\" link is broken.it should be: currently it points to nowhere.", "> python is the most practical language for doing interview questions, because interviews are heavily time-constrained and python allows the developer to go from idea to implementation in the shortest amount of time.i'm not sure if python is the best, but it's a very strong choice. python also has a lot of built in data structures that are relatively non-broken and highly useful for the archetypical algorithm-prototyping interview questionse.g. if you are using a language that only lets you use strings or ints as keys for dicts/hashmaps - instead of say letting you use immutable compound values as keys, or a language that doesnt give you set data-structures which support basic set operations (contains, equal, subset, superset, intersect, union, etc), or a language that requires you to write a paragraph of oop-boilerplate to encode e.g. a list of pairs of values, you're making life harder for yourself than necessary.also, i claim without evidence that prolonged and continuous exposure to working with languages that dont offer sensible data structures, or only offer broken data structures, may over time hamper your ability to think and problem solve clearly.", "ah yeah, leo editor. the lotus notes of the outliner-world. that thing is as powerful as horrible. so full of good ideas, and such a garbage in implementation.on the good side stands the ability to have an programmable outline, paired with a good enough editor to handle the content of nodes. then there also directives, which are basically special commands for inline-usage. things like @wrap to wrap lines, or @language <value> to define activate language-specific handling for the following code. @language is especially useful because you can combine several in one node. finally there is also the relativ open ability to customize things with python. all of this combined a regular workflow i still use with leo is to create a node with several @language-blocks on a node. one for code, one for data, one for documentation. then call a selfmade-function which extracts the code and data and executes it and save the result in a new node in the outline. very useful to transform or generate data on the fly. there are also directives to load/save external files, with support for some fileformats to automagically handle the outline.but then again, on the bad side, as a dev i must say the whole projects has a horrible stench of foul code and ugly design. there is a huge amount of features which either work bad, don't work at all or even work in harmful ways (dataloss). the codebase is just a ragtag-compilation of random ideas from people which seems to have no clue about proper software development, and who are even proud to have come up with some shit they consider as smart. for anyone who wanna try out this software: don't except to much, and prepare for many hateful corners and a steep unnesseccary learning-curve. and save your data! often.", "when you speak about executing code, then yes, out of the box it's limited to python. but literate programming is not executing code, but templating text and generating output-files. leo can do this with any language, and has syntax-support for the most important ones out of the box. additionally it's realtive simple to extend the executing-support for any language you want. leo is quite extendable in python.", "have you used it? it can lag like hell on the simplest things. just because it's written in python and qt instead of electron and javascript doesn't mean it performs well.", "just installed it tonight.high sierrapython 3 installed via homebrewpip3 install leoran fine right out of the box.", "i haven't tried it, but i believe it's easy/moderate to modify leo to support other languages. it just supports python out of the box.someone just has to come along and have a need for it.apparently one can use leo like the jupyter notebook, and indeed leo can interface with jupyter notebooks. i never tried that aspect of it, though.", "mostly the clones feature. the most common use case: if you get a bug report, you can put the bug report, portions of the code related to the bug report (which could be spread over several source code files), any tests and test collateral related to it, all in one tree. when you make changes to the code in that tree, the corresponding source file will get touch.also, things like \"find all functions that contain some string and put them in a tree\" is trivial, and used a lot.these are out of the box. you can then script more advanced stuff in python.even though it's all python, it seems to handle heavy loads well. i once opened all the source code of my project at work (0.5m lines of code). it took a long time to load them the first time (initial conversion into nodes). but once i had them loaded, i could save it as a project and reopen easily. then searching the whole code base for a string was pretty fast as well. i don't know how well emacs can handle thousands of files...the literate capability is really trivial as well. if you're coding with leo, you likely will start using basic literate constructs. it's just natural.but don't read too much into the comparison with org mode. i use org mode and leo for very different things.", "afaik leo is limited to python. orgmode supports, i believe, 50+ different languages:", "i have been using it forever (over 10 years) to organize financial data and do taxes. while i should credit python because these are python programs after all, it is really leo that enables me to easily keep data organized. queries can be ran by easily running python scripts inside of leo with results directed to the log panel, turning leo into a light-weight ide.for example the top node for accounting is like this:code_removed", "i think the behaviour of cmd.exe is part of the problem here. when an interactive cmd.exe launches a console-subsystem app, it waits for the process to finish before showing the prompt again, but when it launches a gui-subsystem app, cmd.exe writes the prompt again immediately, so even if the new process calls attachconsole(attach_parent_process) before it tries to write to the console, it will write over cmd.exe's prompt, which makes a poor user experience.so, if someone wants to make a \"dual-mode\" app that works as a win32-subsystem app when launched from explorer and a console-subsystem app when launched from a console, they have to choose between two bad options. they can make their app a console-subsystem app, which means a console will always briefly appear on screen when the app is started (no matter how quickly the app calls freeconsole(),) or they can make their app a gui-subsystem app (that opportunistically calls attachconsole(),) which behaves sub-optimally in cmd.exe.maybe the solution is to add a flag (in the.manifest file?) that makes the console initially hidden for a console-subsystem app. that would prevent the brief appearance of a console window when launching a console-subsystem app from explorer. then there would be no need for pythonw.exe and python.exe could show the console window only after a message is printed.", "no humor whatsoever. thank you for the explanation! i'm an ops person who knows python/golang to a dangerous extent and have never gone out of my way to understand the utf reasonings. your post intrigued me and made me want to ask why you felt that way. this will make me sound horrendously ignorant to someone of the likes of someone such as yourself but i'm here to learn.", "> imagine keeping code in emacs's org mode and being able to compile it.org mode already does that using org babel. i use it to take \"code notes\" (code blocks with their evaluated/compiled outputs with notes) for python [0], nim[0,1], awk [2], plantuml[3], etc.[0]:", "provides a simple, dependency-free, and efficient python interface for generating z-order curves from two and three-dimensional data.", "hopefully they will have something more advanced in the future, possibly calling it masters of science in cyber engineering with course work similar to the following:code_removed updated - [*] core courses.if the individual could make it through the above, they would be very knowledgable, experienced and ready for many of the hard problems in the realm of cyber that employers are wanting in extremely high demand.", "technically, any executable that's compiled as a commandline application is going to get a console allocated for it, no matter what on windows. i don't believe that's something we can fix retroactively unfortunately, that's just a part of how things have to be.now, i believe that python could have python.exe compiled as a win32 application, then call allocateconsole as soon as the script called print() or something. if the app was already running in a console, i believe (don't quote me) that allocateconsole won't allocate a new console for it, but if it doesn't yet have a console it'll spawn one.", "here's the link: for those on small screens:--leo is:--* a fully-featured ide, with many features inspired by emacs.* an outliner. everything in leo is an outline.* a data manager, data manager and personal information manager.* a powerful scripting environment.* a tool for organizing and studying computer code.* extensible via a simple plugin architecture.* a tool that plays well with ipython, vim and emacs.* written in 100% pure python--leo\u2019s unique features--leo completely integrates python scripting and outlines. simulating the following features in vim, emacs or eclipse is possible, just as it is possible to simulate python in assembly language\u2026* all commands and scripts have easy access to outline structure via a simple python api.for example, p.b is the body text of the selected outline node.scripts have full access to all of leo\u2019s sources.* clones create multiple views of an outline.leo\u2019s underlying data is a directed acyclic graphs.as a result, leo organizes data in completely new ways.* scripts and programs can be composed from outlines using outline-oriented directives.* importers convert flat text into outlines.* @test and @suite scripts create unit tests automatically.* @button scripts apply scripts to outline data.", "i've used leo extensively for one large project, several smaller ones and all the dd work for 8 years until 1/1/2017.it's a very good and well thought out program, it's a bit like a cross between emacs org mode and python programming environment (rather than lisp) geared towards literate programming but very useful for other kinds of applications.i'd love to see a 'google docs' style multi-user version of it, that would be an awesome tool, especially if it would be self hosted.", "have you given thought on how to solve the \"unwanted console\" problem? for example if you run a.py file (python) under windows then you get a console. that is fine for command line stuff, but beyond annoying if the file displays as a gui. so there are now two python binaries - python.exe and pythonw.exe. the only difference is the latter ensures no console appears. also good luck if the script printed a help message since often the console disappears before you even know that happened.i presume many tools deal with this issue, and do it in different ways. perhaps it is as simple as making the console itself only appear once there is any output, or a blocking read of input.", "first, something which amused me:> \u201cword outlines are very useful. but leo makes word look like a clunky toy.\u201d\u2014joe orrwell, yes. any text editor would make word look like a clunky toy, because word is a word processor. they don't do the same things, they aren't aimed at the same audiences, and they evolved different ui conventions because of that.that nitpick aside: it being written in python is nice, and the fact outlines give your document something like a dom which is accessible in a python object tree is nice. it might be possible to massage this into something almost as nice as macros for languages like c# or java.", "clicking on \"a brief summary of leo\" yields the following:code_removed", "grab the stdout of a python script that calls a webservice?there's no ruby equivalent of googleapiclient?", "sorry, are you saying knowing python/c++ makes op a niche developer? i was under the impression these were very common languages at the big-n? or is it something else about their background?not trying to correct you, i'm really just curious since i have a similar background and an in a similar situation to the op.", "charles provided really great documentation on how to build this here: if you're feeling lazy (like i was), there's a fork of his library at which compiles the 3.25.0 amalgamation by default. this worked for me:code_removed", "for python folks interested in using these features, you might be interested in this post [0] which describes how to compile the latest sqlite and the python sqlite3 driver. i've got a fork of the standard lib sqlite3 driver that includes support for user-defined window functions in python as well which may interest you.[0]", "i use travis ci for the os x build of my open source project. i build for os x 10.7 but it looks like you get a good set of other stuff, ios included: think you may have to pay them to use it for closed source stuff. not sure about setting it up to run on-site or whatever.setup was sort-of quite easy, because i just ignored their ci system, however it works, and made the build run the same python script i already used on my own mac to make.dmgs - but it was still a bit painful in places, as their deployment options had some limitations, and the documentation wasn't always clear. but the end result does what i'd hoped for: master gets built, packaged, and uploaded to web site; build branch gets built, packaged, and pushed as a github release.(for windows i use appveyor. mostly similar experience, but one advantage: they'll host your build artefacts for you, though presumably up to some limit. good for the non-release day-to-day builds, as i like to have a decent set of these but don't really care about keeping them indefinitely.)", "my experience is that having a really great design eliminates a lot of bugs (60% is perhaps not unreasonable, although pretty hard to measure), but having an awesome design is not even close to being the same thing as doing regular code reviews.i'm often thrown in to a project with bad to mediocre design and i am not able to just change the design to become \"good\" just by reviewing prs. i can only attempt to push the design into a slightly better direction over a long period of time.similarly, just because it's reviewed doesn't mean that the design is great.i am probably able to hit something approaching 45% defect detection rate with integration testing on most projects though.with code reviews i'd say it's about 1 or 2%. bug-wise, i usually only spot fairly obvious \"language\" gotchas (e.g. initializing an empty list in a method signature in python).", "i think it depends on what the dependency actually is. here it is expected that you would include this header.when someone does a python one liner that just calls a library or something that relies on boost, the point is lost for anyone hoping to have a minimal example.", "i just graduated this past december. while most of my college buddies were spending their time sleeping or 'enjoying school', i was asking myself the same question you are right now.i concluded that if i wanted to be one step ahead of other college students, i had to be somehow proficient in a popular programming language. to execute that i did two things during my free time: i chose to develop small projects in python and django. and, also looked for internships to gain some real life experience because i knew college wouldn't prepare me very well. thinking that way opened several doors for me after school (i graduated with several job offers).also, college time is a crucial period of your life that will structure and define most of your thinking process. so make sure you spend time working on that as well. what do i mean by that? make sure you are spending time working on your becoming a better version of yourself not only career wise but also personal. spend time exploring yourself finding out what you like and what you want as a person. i hope it helps, you are in a great path already.", "is this going to be an issue?or just a benefit?ive had python libraries break with updates.", "my counter to this would be - if i'm doing a thing, and i need to look back through my shell history a few days or weeks later to figure out how i did it (especially if something went wrong), seeing a more readable version is going to help me figure it out faster.granted, anything really nontrivial i'll usually just write a python script for, but the point stands.", "> > scheme\u2019s a pretty little language, but it\u2019s not meant for building large applications.> weren't people saying the same about python and java long ago?i spent roughly a decade as a professional python developer, and in retrospect i\u2019d probably agree that it\u2019s not well-suited to large applications. why, exactly, is for another discussion.as for java \u2014 it\u2019s certainly not meant for building small ones!> how do you arrive at that conclusion?the same way that the scheme committee did, when they came up with r6rs: by trying to use scheme for large systems.it lacks features which aid building large systems (e.g. namespaces); it lacks features which enable building industrial-strength systems (e.g. the lisp condition system or clos); it has features which make code more complex and tend to hinder performance (e.g. call/cc); and it even has a broken feature (dynamic-wind).then there are things like: conflating functions, variables & all other names; breaking nil into nil, () & #f. those are partly a matter of taste, but i think also an indication of lisp\u2019s pragmatic nature: in practice, doing things the lisp way is better, even though in theory doing them the scheme way is.> racket at least wasnt flat iirc, is guile scheme?racket isn\u2019t scheme anymore. that\u2019s not a bad thing, and indeed racket is pretty amazing. i wish that the same amount of effort had been expended making lisp better, but everyone\u2019s gotta scratch his own itch.", "that's right. my first reaction after learning node was that its tcl/tk done poorly but with a faster runtime. tcl/tk had first class event loops from the beginning (of time), later it acquired coroutines as well.the best part for me, however, was that in tcl you can have multiple interpreters in the same process, each in its own native thread if you want. no gil, no need to pickle/unpickle to talk the other interpreter. had python adopted this model, using multiple cores would have been so much nicer.that said, the mantle has passed on to lua, but i still harbor a soft corner for tcl.", "i just left a job with a truly toxic code review culture.- managers would swoop in and leave incorrect criticisms. asserting plainly false things and in one case reviewing python code as though it was javascript.- devs would make conflicting requirements for improvement. dev a demands change a. dev b wants change b. a and b are incompatible and neither dev will back down. management of course refused to help settle the argument.- change requests completely unrelated to the pr being reviewed would get crammed into the pr. so every small code change could balloon into weeks on refactoring work.- often times code review comments were made without ever reading the code being reviewed. such as a comment of \u201cwhy aren\u2019t you using library x?\u201d sandwiched in between invocations of library x.the culture was clearly rewarding people for shitting on others work instead of doing any work themselves. getting even the simplest code changes past required endless meetings explaining the most basic facts about computers.so glad to be gone from there.", "maybe \"pure\" python can't do fast sound processing, i can accept that. you have to at least use scipy/numpy. but his claim of \"python can't keep up with real-time requirement of rtp, bursting out packets every few milliseconds\" seems bogus to me.", "sqlite has had fdw since forever. has had user-defined functions since forever. underappreciated feature (while we're at it) would be wal instead of undo-journaling ( which enables concurrent reading and writing of sqlite databases. has been available for some ten years or so, but is off by default.functions are available through the python bindings (which are not maintained by the sqlite project), virtual tables i think, aren't. alternate bindings ( claim to achieve \"everything you can do from the sqlite c api\" interop, which would include virtual tables.", "this is different than doing pure python implementation. python acting as glue is most suitable solution when heavy lifting is done by c code. i have used pjsip/pjsua lib with python bindings which performs beautifully.", "is python virtualenv not an option?", "ruby or python imho.", "i might be missing something, but it sounds to me like you're describing this:", "as far as i know you already can extend sqlite with custom scalar and aggregate functions and virtual tables (fdw) at least in python (with apsw). am i missing something?", "this is super cool.does anyone know how to upgrade python3's sqlite module to the latest version?", "i\u2019d argue what they would need for that is tighter integration with pandas (python) and data.table (r) but that would be niceclever support for multilevel indexes would be top of my wishlist", "sqlite guys, please add fdw support ala postgres and easy foreign function support for python and r, and you\u2019ll corner most of analytics and data science.", "oh boy, i just wrote a 100 line client and another 100 line server that is able to do real time voice communication, comparable to the likes of telegram. (in python)albeit it uses portaudio (python-soundevice) for the audio io which is not written in python, and opus which isn't written in python either. but all the glue and udp communication is pure python.and p.s. - pulseaudio uses more cpu than my thingy.talking about production, i suspect the guys at onbeep are making use of python a lot, just looking at their github repo python opus bindings are created by them)i'm just tired of people underestimating the speed of python. its often a matter of not using it correctly...", "the most common implementation of python can't cut it.but other implementations might, such as pypy. cython almost certainly could, but it's a super-set of the python language so maybe it shouldn't count.i am thinking of", "the sip stack implementation in python is good. but rtp part - python can't keep up with real time requirement of rtp, bursting out packets every few milliseconds. even after depending on c code for encoding/decoding. there once was an attempt to code softphone (shtoom - ) in pure python by anthony baxter and was later abandoned. there are some jobs that python can't cut it.", "i don't see how this is different from lambdas in general, in js/python/etc... wouldn't it only be held as long as the variables are alive? if you use the callback only once then they should be eligible to be freed after being used once."]